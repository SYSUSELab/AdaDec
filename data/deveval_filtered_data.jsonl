{"namespace": "trailscraper.record_sources.local_directory_record_source.LocalDirectoryRecordSource.load_from_dir", "prompt": "# Please complete the load_from_dir function based on the contexts above the function.\n\n# The contexts above the function are:\n\"\"\"Module for LocalDirectoryRecordSource\"\"\"\nimport logging\nimport os\n\nfrom toolz import pipe\nfrom toolz.curried import filter as filterz\nfrom toolz.curried import last as lastz\nfrom toolz.curried import map as mapz\nfrom toolz.curried import mapcat as mapcatz\nfrom toolz.curried import sorted as sortedz\n\nfrom trailscraper.cloudtrail import LogFile\n\n\nclass LocalDirectoryRecordSource():\n    \"\"\"Class to represent cloudtrail records stored on disk\"\"\"\n    def __init__(self, log_dir):\n        self._log_dir = log_dir\n\n    def _valid_log_files(self):\n        def _valid_or_warn(log_file):\n            if log_file.has_valid_filename():\n                return True\n\n            logging.warning(\"Invalid filename: %s\", log_file.filename())\n            return False\n\n        def _to_paths(triple):\n            root, _, files_in_dir = triple\n            return [os.path.join(root, file_in_dir) for file_in_dir in files_in_dir]\n\n        return pipe(os.walk(self._log_dir),\n                    mapcatz(_to_paths),\n                    mapz(LogFile),\n                    filterz(_valid_or_warn))\n\n\n\n# The code to be completed is:\n    def load_from_dir(self, from_date, to_date):\n\n        \"\"\"\n        Load all CloudTrail records from a directory within a specified date range. It iterates through all valid log files in the directory and checks if each file contains events within the specified date range. If a file meets the criteria, it retrieves the records from that file and adds them to the list of records.\n        Input-Output Arguments\n        :param self: LocalDirectoryRecordSource. An instance of the LocalDirectoryRecordSource class.\n        :param from_date: The starting date of the desired records.\n        :param to_date: The ending date of the desired records.\n        :return: List of CloudTrail records. The records that fall within the specified date range.\n        \"\"\""}
{"namespace": "exodus_bundler.templating.render_template", "prompt": "# Please complete the render_template function based on the contexts above the function.\n\n# The contexts above the function are:\n# -*- coding: utf-8 -*-\n\"\"\"Contains a couple of fairly trivial templating methods used for constructing\nthe loaders and the output filename. Any instances of {{variable_name}} will be\nreplaced by the corresponding values.\"\"\"\n\nimport os\n\n\nparent_directory = os.path.dirname(os.path.realpath(__file__))\ntemplate_directory = os.path.join(parent_directory, 'templates')\n\n\n\n\n# The code to be completed is:\ndef render_template(string, **context):\n\n    \"\"\"\n    This function replaces the placeholders in the input string with the corresponding values from the context dictionary.\n    Input-Output Arguments\n    :param string: String. The input string containing placeholders.\n    :param context: Dictionary. The key-value pairs to replace the placeholders in the input string.\n    :return: String. The modified string after replacing the placeholders.\n    \"\"\""}
{"namespace": "falcon.inspect.register_router", "prompt": "# Please complete the register_router function based on the contexts above the function.\n\n# The contexts above the function are:\n# Copyright 2020 by Federico Caselli\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Inspect utilities for falcon applications.\"\"\"\nfrom functools import partial\nimport inspect\nfrom typing import Callable\nfrom typing import Dict\nfrom typing import List\nfrom typing import Optional\nfrom typing import Type\n\n\nfrom falcon.app import App\nfrom falcon.routing import CompiledRouter\n\n\ndef inspect_app(app: App) -> 'AppInfo':\n    \"\"\"Inspects an application.\n\n    Args:\n        app (falcon.App): The application to inspect. Works with both\n            :class:`falcon.App` and :class:`falcon.asgi.App`.\n\n    Returns:\n        AppInfo: The information regarding the application. Call\n        :meth:`~.AppInfo.to_string` on the result to obtain a human-friendly\n        representation.\n    \"\"\"\n    routes = inspect_routes(app)\n    static = inspect_static_routes(app)\n    sinks = inspect_sinks(app)\n    error_handlers = inspect_error_handlers(app)\n    middleware = inspect_middleware(app)\n    return AppInfo(routes, middleware, static, sinks, error_handlers, app._ASGI)\n\n\ndef inspect_routes(app: App) -> 'List[RouteInfo]':\n    \"\"\"Inspects the routes of an application.\n\n    Args:\n        app (falcon.App): The application to inspect. Works with both\n            :class:`falcon.App` and :class:`falcon.asgi.App`.\n\n    Returns:\n        List[RouteInfo]: A list of route descriptions for the application.\n    \"\"\"\n    router = app._router\n\n    inspect_function = _supported_routers.get(type(router))\n    if inspect_function is None:\n        raise TypeError(\n            'Unsupported router class {}. Use \"register_router\" '\n            'to register a function that can inspect the router '\n            'used by the provided application'.format(type(router))\n        )\n    return inspect_function(router)\n\n\n\n\n# The code to be completed is:\ndef register_router(router_class):\n\n    \"\"\"\n    This function is a decorator that registers a new function for a custom router class. It takes the router class as input and returns a new function that can be used to inspect the router. If the router class is already registered, raise a ValueError.\n    Input-Output Arguments\n    :param router_class: Type. The router class to register. If already registered, an error will be raised.\n    :return: The new function that inspects the router.\n    \"\"\""}
{"namespace": "gunicorn.config.Config.logger_class", "prompt": "# Please complete the logger_class function based on the contexts above the function.\n\n# The contexts above the function are:\n# -*- coding: utf-8 -\n#\n# This file is part of gunicorn released under the MIT license.\n# See the NOTICE for more information.\n\n# Please remember to run \"make -C docs html\" after update \"desc\" attributes.\n\nimport argparse\nimport copy\nimport grp\nimport inspect\nimport os\nimport pwd\nimport re\nimport shlex\nimport ssl\nimport sys\nimport textwrap\n\nfrom gunicorn import __version__, util\nfrom gunicorn.errors import ConfigError\nfrom gunicorn.reloader import reloader_engines\n\nKNOWN_SETTINGS = []\nPLATFORM = sys.platform\n\n\ndef make_settings(ignore=None):\n    settings = {}\n    ignore = ignore or ()\n    for s in KNOWN_SETTINGS:\n        setting = s()\n        if setting.name in ignore:\n            continue\n        settings[setting.name] = setting.copy()\n    return settings\n\n\ndef auto_int(_, x):\n    # for compatible with octal numbers in python3\n    if re.match(r'0(\\d)', x, re.IGNORECASE):\n        x = x.replace('0', '0o', 1)\n    return int(x, 0)\n\n\nclass Config(object):\n\n    def __init__(self, usage=None, prog=None):\n        self.settings = make_settings()\n        self.usage = usage\n        self.prog = prog or os.path.basename(sys.argv[0])\n        self.env_orig = os.environ.copy()\n\n    def __str__(self):\n        lines = []\n        kmax = max(len(k) for k in self.settings)\n        for k in sorted(self.settings):\n            v = self.settings[k].value\n            if callable(v):\n                v = \"<{}()>\".format(v.__qualname__)\n            lines.append(\"{k:{kmax}} = {v}\".format(k=k, v=v, kmax=kmax))\n        return \"\\n\".join(lines)\n\n    def __getattr__(self, name):\n        if name not in self.settings:\n            raise AttributeError(\"No configuration setting for: %s\" % name)\n        return self.settings[name].get()\n\n    def __setattr__(self, name, value):\n        if name != \"settings\" and name in self.settings:\n            raise AttributeError(\"Invalid access!\")\n        super().__setattr__(name, value)\n\n    def set(self, name, value):\n        if name not in self.settings:\n            raise AttributeError(\"No configuration setting for: %s\" % name)\n        self.settings[name].set(value)\n\n    def get_cmd_args_from_env(self):\n        if 'GUNICORN_CMD_ARGS' in self.env_orig:\n            return shlex.split(self.env_orig['GUNICORN_CMD_ARGS'])\n        return []\n\n    def parser(self):\n        kwargs = {\n            \"usage\": self.usage,\n            \"prog\": self.prog\n        }\n        parser = argparse.ArgumentParser(**kwargs)\n        parser.add_argument(\"-v\", \"--version\",\n                            action=\"version\", default=argparse.SUPPRESS,\n                            version=\"%(prog)s (version \" + __version__ + \")\\n\",\n                            help=\"show program's version number and exit\")\n        parser.add_argument(\"args\", nargs=\"*\", help=argparse.SUPPRESS)\n\n        keys = sorted(self.settings, key=self.settings.__getitem__)\n        for k in keys:\n            self.settings[k].add_option(parser)\n\n        return parser\n\n    @property\n    def worker_class_str(self):\n        uri = self.settings['worker_class'].get()\n\n        # are we using a threaded worker?\n        is_sync = uri.endswith('SyncWorker') or uri == 'sync'\n        if is_sync and self.threads > 1:\n            return \"gthread\"\n        return uri\n\n    @property\n    def worker_class(self):\n        uri = self.settings['worker_class'].get()\n\n        # are we using a threaded worker?\n        is_sync = uri.endswith('SyncWorker') or uri == 'sync'\n        if is_sync and self.threads > 1:\n            uri = \"gunicorn.workers.gthread.ThreadWorker\"\n\n        worker_class = util.load_class(uri)\n        if hasattr(worker_class, \"setup\"):\n            worker_class.setup()\n        return worker_class\n\n    @property\n    def address(self):\n        s = self.settings['bind'].get()\n        return [util.parse_address(util.bytes_to_str(bind)) for bind in s]\n\n    @property\n    def uid(self):\n        return self.settings['user'].get()\n\n    @property\n    def gid(self):\n        return self.settings['group'].get()\n\n    @property\n    def proc_name(self):\n        pn = self.settings['proc_name'].get()\n        if pn is not None:\n            return pn\n        else:\n            return self.settings['default_proc_name'].get()\n\n    @property\n\n\n# The code to be completed is:\n    def logger_class(self):\n\n        \"\"\"\n        This function retrieves the logger class based on the configuration settings. It first checks the 'logger_class' setting and if it is \"simple\", it uses the default logger class. If the default logger class is being used andstatsd is on, it automatically switches to the gunicorn.instrument.statsd.Statsd class. Then, it loads the logger class (with default: \"gunicorn.glogging.Logger\" and section: \"gunicorn.loggers\") and install it if can, finally returns it.\n        Input-Output Arguments\n        :param self: Config. An instance of the Config class.\n        :return: The logger class based on the configuration settings.\n        \"\"\""}
{"namespace": "aioxmpp.entitycaps.caps390.Key.node", "prompt": "# Please complete the node function based on the contexts above the function.\n\n# The contexts above the function are:\n########################################################################\n# File name: caps390.py\n# This file is part of: aioxmpp\n#\n# LICENSE\n#\n# This program is free software: you can redistribute it and/or modify\n# it under the terms of the GNU Lesser General Public License as\n# published by the Free Software Foundation, either version 3 of the\n# License, or (at your option) any later version.\n#\n# This program is distributed in the hope that it will be useful, but\n# WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\n# Lesser General Public License for more details.\n#\n# You should have received a copy of the GNU Lesser General Public\n# License along with this program.  If not, see\n# <http://www.gnu.org/licenses/>.\n#\n########################################################################\nimport base64\nimport pathlib\nimport collections\nimport urllib.parse\n\nimport aioxmpp.hashes\n\nfrom .common import AbstractKey\nfrom . import xso as caps_xso\n\n\ndef _process_features(features):\n    \"\"\"\n    Generate the `Features String` from an iterable of features.\n\n    :param features: The features to generate the features string from.\n    :type features: :class:`~collections.abc.Iterable` of :class:`str`\n    :return: The `Features String`\n    :rtype: :class:`bytes`\n\n    Generate the `Features String` from the given `features` as specified in\n    :xep:`390`.\n    \"\"\"\n    parts = [\n        feature.encode(\"utf-8\")+b\"\\x1f\"\n        for feature in features\n    ]\n    parts.sort()\n    return b\"\".join(parts)+b\"\\x1c\"\n\n\ndef _process_identity(identity):\n    category = (identity.category or \"\").encode(\"utf-8\")+b\"\\x1f\"\n    type_ = (identity.type_ or \"\").encode(\"utf-8\")+b\"\\x1f\"\n    lang = str(identity.lang or \"\").encode(\"utf-8\")+b\"\\x1f\"\n    name = (identity.name or \"\").encode(\"utf-8\")+b\"\\x1f\"\n\n    return b\"\".join([category, type_, lang, name]) + b\"\\x1e\"\n\n\ndef _process_identities(identities):\n    \"\"\"\n    Generate the `Identities String` from an iterable of identities.\n\n    :param identities: The identities to generate the features string from.\n    :type identities: :class:`~collections.abc.Iterable` of\n        :class:`~.disco.xso.Identity`\n    :return: The `Identities String`\n    :rtype: :class:`bytes`\n\n    Generate the `Identities String` from the given `identities` as specified\n    in :xep:`390`.\n    \"\"\"\n    parts = [\n        _process_identity(identity)\n        for identity in identities\n    ]\n    parts.sort()\n    return b\"\".join(parts)+b\"\\x1c\"\n\n\ndef _process_field(field):\n    parts = [\n        (value or \"\").encode(\"utf-8\") + b\"\\x1f\"\n        for value in field.values\n    ]\n\n    parts.insert(0, field.var.encode(\"utf-8\")+b\"\\x1f\")\n    return b\"\".join(parts)+b\"\\x1e\"\n\n\ndef _process_form(form):\n    parts = [\n        _process_field(form)\n        for form in form.fields\n    ]\n\n    parts.sort()\n    return b\"\".join(parts)+b\"\\x1d\"\n\n\ndef _process_extensions(exts):\n    \"\"\"\n    Generate the `Extensions String` from an iterable of data forms.\n\n    :param exts: The data forms to generate the extensions string from.\n    :type exts: :class:`~collections.abc.Iterable` of\n        :class:`~.forms.xso.Data`\n    :return: The `Extensions String`\n    :rtype: :class:`bytes`\n\n    Generate the `Extensions String` from the given `exts` as specified\n    in :xep:`390`.\n    \"\"\"\n    parts = [\n        _process_form(form)\n        for form in exts\n    ]\n    parts.sort()\n    return b\"\".join(parts)+b\"\\x1c\"\n\n\ndef _get_hash_input(info):\n    return b\"\".join([\n        _process_features(info.features),\n        _process_identities(info.identities),\n        _process_extensions(info.exts)\n    ])\n\n\ndef _calculate_hash(algo, hash_input):\n    impl = aioxmpp.hashes.hash_from_algo(algo)\n    impl.update(hash_input)\n    return impl.digest()\n\n\nKey = collections.namedtuple(\"Key\", [\"algo\", \"digest\"])\n\n\nclass Key(Key, AbstractKey):\n    @property\n\n\n# The code to be completed is:\n    def node(self):\n\n        \"\"\"\n        This function returns a node string based on the algorithm and digest of the Key instance.\n        Input-Output Arguments\n        :param self: Key. An instance of the Key class.\n        :return: String. The node string based on the algorithm and digest of the Key instance.\n        \"\"\""}
{"namespace": "hbmqtt.codecs.bytes_to_int", "prompt": "# Please complete the bytes_to_int function based on the contexts above the function.\n\n# The contexts above the function are:\n# Copyright (c) 2015 Nicolas JOUANIN\n#\n# See the file license.txt for copying permission.\nimport asyncio\nfrom struct import pack, unpack\nfrom hbmqtt.errors import NoDataException\n\n\ndef bytes_to_hex_str(data):\n    \"\"\"\n    converts a sequence of bytes into its displayable hex representation, ie: 0x??????\n    :param data: byte sequence\n    :return: Hexadecimal displayable representation\n    \"\"\"\n    return '0x' + ''.join(format(b, '02x') for b in data)\n\n\n\n\n# The code to be completed is:\ndef bytes_to_int(data):\n\n    \"\"\"\n    Convert a sequence of bytes to an integer using big endian byte ordering. It first converts the byte sequence to an integer using big endian byte ordering.\n    Input-Output Arguments\n    :param data: byte sequence. The sequence of bytes to be converted to an integer.\n    :return: integer value. The integer value converted from the byte sequence.\n    \"\"\""}
{"namespace": "sacred.config.custom_containers.DogmaticDict.update", "prompt": "# Please complete the update function based on the contexts above the function.\n\n# The contexts above the function are:\n#!/usr/bin/env python\n# coding=utf-8\nimport copy\n\nimport sacred.optional as opt\nfrom sacred.utils import join_paths, SacredError\n\n\ndef fallback_dict(fallback, **kwargs):\n    fallback_copy = fallback.copy()\n    fallback_copy.update(kwargs)\n    return fallback_copy\n\n\nclass DogmaticDict(dict):\n    def __init__(self, fixed=None, fallback=None):\n        super().__init__()\n        self.typechanges = {}\n        self.fallback_writes = []\n        self.modified = set()\n        self.fixed = fixed or {}\n        self._fallback = {}\n        if fallback:\n            self.fallback = fallback\n\n    @property\n    def fallback(self):\n        return self._fallback\n\n    @fallback.setter\n    def fallback(self, newval):\n        ffkeys = set(self.fixed.keys()).intersection(set(newval.keys()))\n        for k in ffkeys:\n            if isinstance(self.fixed[k], DogmaticDict):\n                self.fixed[k].fallback = newval[k]\n            elif isinstance(self.fixed[k], dict):\n                self.fixed[k] = DogmaticDict(self.fixed[k])\n                self.fixed[k].fallback = newval[k]\n\n        self._fallback = newval\n\n    def _log_blocked_setitem(self, key, value, fixed_value):\n        if type_changed(value, fixed_value):\n            self.typechanges[key] = (type(value), type(fixed_value))\n\n        if is_different(value, fixed_value):\n            self.modified.add(key)\n\n        # if both are dicts recursively collect modified and typechanges\n        if isinstance(fixed_value, DogmaticDict) and isinstance(value, dict):\n            for k, val in fixed_value.typechanges.items():\n                self.typechanges[join_paths(key, k)] = val\n\n            self.modified |= {join_paths(key, m) for m in fixed_value.modified}\n\n    def __setitem__(self, key, value):\n        if key not in self.fixed:\n            if key in self.fallback:\n                self.fallback_writes.append(key)\n            return dict.__setitem__(self, key, value)\n\n        fixed_value = self.fixed[key]\n        dict.__setitem__(self, key, fixed_value)\n        # if both are dicts do a recursive update\n        if isinstance(fixed_value, DogmaticDict) and isinstance(value, dict):\n            for k, val in value.items():\n                fixed_value[k] = val\n\n        self._log_blocked_setitem(key, value, fixed_value)\n\n    def __getitem__(self, item):\n        if dict.__contains__(self, item):\n            return dict.__getitem__(self, item)\n        elif item in self.fallback:\n            if item in self.fixed:\n                return self.fixed[item]\n            else:\n                return self.fallback[item]\n        raise KeyError(item)\n\n    def __contains__(self, item):\n        return dict.__contains__(self, item) or (item in self.fallback)\n\n    def get(self, k, d=None):\n        if dict.__contains__(self, k):\n            return dict.__getitem__(self, k)\n        else:\n            return self.fallback.get(k, d)\n\n    def has_key(self, item):\n        return self.__contains__(item)\n\n    def __delitem__(self, key):\n        if key not in self.fixed:\n            dict.__delitem__(self, key)\n\n\n\n# The code to be completed is:\n    def update(self, iterable=None, **kwargs):\n\n        \"\"\"\n        Update the DogmaticDict instance with the given iterable or keyword arguments. If the iterable is not None, it iterates through the keys and values of the iterable and updates the instance. If the iterable does not have keys, it iterates through the items of the iterable and updates the instance. Then, it updates the instance with the keyword arguments.\n        Input-Output Arguments\n        :param self: DogmaticDict. An instance of the DogmaticDict class.\n        :param iterable: Iterable. An iterable object to update the instance. Defaults to None.\n        :param kwargs: Keyword arguments. Key-value pairs to update the instance.\n        :return: No return values.\n        \"\"\""}
{"namespace": "pycoin.satoshi.stackops.do_OP_2OVER", "prompt": "# Please complete the do_OP_2OVER function based on the contexts above the function.\n\n# The contexts above the function are:\nimport hashlib\n\nfrom . import errno\nfrom pycoin.coins.SolutionChecker import ScriptError\n\n\n\n\ndef do_OP_NOP(s):\n    pass\n\n\nfor i in range(1, 11):\n    exec(\"def do_OP_NOP%d(s): pass\" % i)\n\n\ndef do_OP_VER(stack):\n    raise ScriptError(\"OP_VER encountered\", errno.BAD_OPCODE)\n\n\ndef do_OP_RESERVED1(stack):\n    raise ScriptError(\"OP_RESERVED1 encountered\", errno.BAD_OPCODE)\n\n\ndef do_OP_RESERVED2(stack):\n    raise ScriptError(\"OP_RESERVED2 encountered\", errno.BAD_OPCODE)\n\n\ndef do_OP_RETURN(stack):\n    raise ScriptError(\"OP_RETURN encountered\", errno.OP_RETURN)\n\n\ndef do_OP_2DROP(stack):\n    stack.pop()\n    stack.pop()\n\n\ndef do_OP_2DUP(stack):\n    #  (x1 x2 -- x1 x2 x1 x2)\n    stack.append(stack[-2])\n    stack.append(stack[-2])\n\n\ndef do_OP_3DUP(stack):\n    #  (x1 x2 x3 -- x1 x2 x3 x1 x2 x3)\n    stack.append(stack[-3])\n    stack.append(stack[-3])\n    stack.append(stack[-3])\n\n\n\n\n# The code to be completed is:\ndef do_OP_2OVER(stack):\n    #  (x1 x2 x3 x4 -- x1 x2 x3 x4 x1 x2)\n\n    \"\"\"\n    This function duplicates the -3rd and -4th element to the top of the stack, like this: (x1 x2 x3 x4 \"top\" -- x1 x2 x3 x4 x1 x2 \"top\")\n    Input-Output Arguments\n    :param stack: List. The stack containing the items to be duplicated.\n    :return: No return values.\n    \"\"\""}
{"namespace": "boltons.formatutils.infer_positional_format_args", "prompt": "# Please complete the infer_positional_format_args function based on the contexts above the function.\n\n# The contexts above the function are:\n# -*- coding: utf-8 -*-\n\n# Copyright (c) 2013, Mahmoud Hashemi\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are\n# met:\n#\n#    * Redistributions of source code must retain the above copyright\n#      notice, this list of conditions and the following disclaimer.\n#\n#    * Redistributions in binary form must reproduce the above\n#      copyright notice, this list of conditions and the following\n#      disclaimer in the documentation and/or other materials provided\n#      with the distribution.\n#\n#    * The names of the contributors may not be used to endorse or\n#      promote products derived from this software without specific\n#      prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\n# \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\n# LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR\n# A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT\n# OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,\n# SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT\n# LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,\n# DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY\n# THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n# (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\n\"\"\"`PEP 3101`_ introduced the :meth:`str.format` method, and what\nwould later be called \"new-style\" string formatting. For the sake of\nexplicit correctness, it is probably best to refer to Python's dual\nstring formatting capabilities as *bracket-style* and\n*percent-style*. There is overlap, but one does not replace the\nother.\n\n  * Bracket-style is more pluggable, slower, and uses a method.\n  * Percent-style is simpler, faster, and uses an operator.\n\nBracket-style formatting brought with it a much more powerful toolbox,\nbut it was far from a full one. :meth:`str.format` uses `more powerful\nsyntax`_, but `the tools and idioms`_ for working with\nthat syntax are not well-developed nor well-advertised.\n\n``formatutils`` adds several functions for working with bracket-style\nformat strings:\n\n  * :class:`DeferredValue`: Defer fetching or calculating a value\n    until format time.\n  * :func:`get_format_args`: Parse the positional and keyword\n    arguments out of a format string.\n  * :func:`tokenize_format_str`: Tokenize a format string into\n    literals and :class:`BaseFormatField` objects.\n  * :func:`construct_format_field_str`: Assists in programmatic\n    construction of format strings.\n  * :func:`infer_positional_format_args`: Converts anonymous\n    references in 2.7+ format strings to explicit positional arguments\n    suitable for usage with Python 2.6.\n\n.. _more powerful syntax: https://docs.python.org/2/library/string.html#format-string-syntax\n.. _the tools and idioms: https://docs.python.org/2/library/string.html#string-formatting\n.. _PEP 3101: https://www.python.org/dev/peps/pep-3101/\n\"\"\"\n# TODO: also include percent-formatting utils?\n# TODO: include lithoxyl.formatters.Formatter (or some adaptation)?\n\nfrom __future__ import print_function\n\nimport re\nfrom string import Formatter\n\ntry:\n    unicode        # Python 2\nexcept NameError:\n    unicode = str  # Python 3\n\n__all__ = ['DeferredValue', 'get_format_args', 'tokenize_format_str',\n           'construct_format_field_str', 'infer_positional_format_args',\n           'BaseFormatField']\n\n\n_pos_farg_re = re.compile('({{)|'         # escaped open-brace\n                          '(}})|'         # escaped close-brace\n                          r'({[:!.\\[}])')  # anon positional format arg\n\n\ndef construct_format_field_str(fname, fspec, conv):\n    \"\"\"\n    Constructs a format field string from the field name, spec, and\n    conversion character (``fname``, ``fspec``, ``conv``). See Python\n    String Formatting for more info.\n    \"\"\"\n    if fname is None:\n        return ''\n    ret = '{' + fname\n    if conv:\n        ret += '!' + conv\n    if fspec:\n        ret += ':' + fspec\n    ret += '}'\n    return ret\n\n\ndef split_format_str(fstr):\n    \"\"\"Does very basic splitting of a format string, returns a list of\n    strings. For full tokenization, see :func:`tokenize_format_str`.\n\n    \"\"\"\n    ret = []\n\n    for lit, fname, fspec, conv in Formatter().parse(fstr):\n        if fname is None:\n            ret.append((lit, None))\n            continue\n        field_str = construct_format_field_str(fname, fspec, conv)\n        ret.append((lit, field_str))\n    return ret\n\n\n\n\n# The code to be completed is:\ndef infer_positional_format_args(fstr):\n\n    \"\"\"\n    This function takes format strings with anonymous positional arguments (e.g., \"{}\" and {:d}) and converts them into numbered ones for explicitness and compatibility with Python 2.6. It replaces the anonymous positional arguments with numbered ones and returns the modified string.\n    Input-Output Arguments\n    :param fstr: String. The format string with anonymous positional arguments.\n    :return: String. The modified format string with numbered positional arguments.\n    \"\"\""}
{"namespace": "gif_for_cli.utils._get_default_display_mode", "prompt": "# Please complete the _get_default_display_mode function based on the contexts above the function.\n\n# The contexts above the function are:\n\"\"\"\nCopyright 2018 Google LLC\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    https://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n\"\"\"\nimport argparse\nfrom multiprocessing import Pool\nimport itertools\nimport os\n\n\ndef memoize(f):\n    \"\"\"\n    Caveat: Presumes each arg is hashable, and therefore a valid dict key.\n    \"\"\"\n    d = {}\n\n    def wrapper(*args):\n        if args in d:\n            return d[args]\n\n        res = f(*args)\n        d[args] = res\n        return res\n\n    return wrapper\n\n\n\n\n# The code to be completed is:\ndef _get_default_display_mode(environ):\n\n    \"\"\"\n    This function returns the default display mode based on the environment variables. It checks the TERM and COLORTERM environment variables and returns the display mode based on the values of these variables. If \"truecolor\" in TERM or COLORTERM, it will return \"truecolor\". If \"256\" in TERM or COLORTERM, it will return \"256fgbg\". In other conditions, it returns \"nocolor\".\n    Input-Output Arguments\n    :param environ: Dictionary. The environment variables.\n    :return: String. The default display mode based on the environment variables.\n    \"\"\""}
{"namespace": "barf.core.smt.smtfunction.ite", "prompt": "# Please complete the ite function based on the contexts above the function.\n\n# The contexts above the function are:\n# Copyright (c) 2017, Fundacion Dr. Manuel Sadosky\n# All rights reserved.\n\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n\n# 1. Redistributions of source code must retain the above copyright notice, this\n# list of conditions and the following disclaimer.\n\n# 2. Redistributions in binary form must reproduce the above copyright notice,\n# this list of conditions and the following disclaimer in the documentation\n# and/or other materials provided with the distribution.\n\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n# DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\n# FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\n# DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\n# SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\n# CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\n# OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\nfrom __future__ import absolute_import\n\nfrom barf.core.smt.smtsymbol import BitVec\n\nfrom barf.core.smt.smtsymbol import Constant\n\n\ndef zero_extend(s, size):\n    assert type(s) in (Constant, BitVec) and size - s.size >= 0\n\n    if size == s.size:\n        return s\n\n    return BitVec(size, \"(_ zero_extend {})\".format(size - s.size), s)\n\n\ndef sign_extend(s, size):\n    assert type(s) in (Constant, BitVec) and size - s.size >= 0\n\n    if size == s.size:\n        return s\n\n    return BitVec(size, \"(_ sign_extend {})\".format(size - s.size), s)\n\n\ndef extract(s, offset, size):\n    assert type(s) in (Constant, BitVec)\n\n    if offset == 0 and size == s.size:\n        return s\n\n    return BitVec(size, \"(_ extract {} {})\".format(offset + size - 1, offset), s)\n\n\n\n\n# The code to be completed is:\ndef ite(size, cond, true, false):\n\n    \"\"\"\n    This function creates an if-then-else expression. It takes in a size, a condition, a true value, and a false value, validate the condition type, and returns a class expression representing the if-then-else expression.\n    Input-Output Arguments\n    :param size: Integer. The size of the BitVec expression to be created.\n    :param cond: Bool. The condition for the if-then-else expression.\n    :param true: BitVec. The value to be returned if the condition is true.\n    :param false: BitVec. The value to be returned if the condition is false.\n    :return: BitVec. The if-then-else expression.\n    \"\"\""}
{"namespace": "gunicorn.config.Config.address", "prompt": "# Please complete the address function based on the contexts above the function.\n\n# The contexts above the function are:\n# -*- coding: utf-8 -\n#\n# This file is part of gunicorn released under the MIT license.\n# See the NOTICE for more information.\n\n# Please remember to run \"make -C docs html\" after update \"desc\" attributes.\n\nimport argparse\nimport copy\nimport grp\nimport inspect\nimport os\nimport pwd\nimport re\nimport shlex\nimport ssl\nimport sys\nimport textwrap\n\nfrom gunicorn import __version__, util\nfrom gunicorn.errors import ConfigError\nfrom gunicorn.reloader import reloader_engines\n\nKNOWN_SETTINGS = []\nPLATFORM = sys.platform\n\n\ndef make_settings(ignore=None):\n    settings = {}\n    ignore = ignore or ()\n    for s in KNOWN_SETTINGS:\n        setting = s()\n        if setting.name in ignore:\n            continue\n        settings[setting.name] = setting.copy()\n    return settings\n\n\ndef auto_int(_, x):\n    # for compatible with octal numbers in python3\n    if re.match(r'0(\\d)', x, re.IGNORECASE):\n        x = x.replace('0', '0o', 1)\n    return int(x, 0)\n\n\nclass Config(object):\n\n    def __init__(self, usage=None, prog=None):\n        self.settings = make_settings()\n        self.usage = usage\n        self.prog = prog or os.path.basename(sys.argv[0])\n        self.env_orig = os.environ.copy()\n\n    def __str__(self):\n        lines = []\n        kmax = max(len(k) for k in self.settings)\n        for k in sorted(self.settings):\n            v = self.settings[k].value\n            if callable(v):\n                v = \"<{}()>\".format(v.__qualname__)\n            lines.append(\"{k:{kmax}} = {v}\".format(k=k, v=v, kmax=kmax))\n        return \"\\n\".join(lines)\n\n    def __getattr__(self, name):\n        if name not in self.settings:\n            raise AttributeError(\"No configuration setting for: %s\" % name)\n        return self.settings[name].get()\n\n    def __setattr__(self, name, value):\n        if name != \"settings\" and name in self.settings:\n            raise AttributeError(\"Invalid access!\")\n        super().__setattr__(name, value)\n\n    def set(self, name, value):\n        if name not in self.settings:\n            raise AttributeError(\"No configuration setting for: %s\" % name)\n        self.settings[name].set(value)\n\n    def get_cmd_args_from_env(self):\n        if 'GUNICORN_CMD_ARGS' in self.env_orig:\n            return shlex.split(self.env_orig['GUNICORN_CMD_ARGS'])\n        return []\n\n    def parser(self):\n        kwargs = {\n            \"usage\": self.usage,\n            \"prog\": self.prog\n        }\n        parser = argparse.ArgumentParser(**kwargs)\n        parser.add_argument(\"-v\", \"--version\",\n                            action=\"version\", default=argparse.SUPPRESS,\n                            version=\"%(prog)s (version \" + __version__ + \")\\n\",\n                            help=\"show program's version number and exit\")\n        parser.add_argument(\"args\", nargs=\"*\", help=argparse.SUPPRESS)\n\n        keys = sorted(self.settings, key=self.settings.__getitem__)\n        for k in keys:\n            self.settings[k].add_option(parser)\n\n        return parser\n\n    @property\n    def worker_class_str(self):\n        uri = self.settings['worker_class'].get()\n\n        # are we using a threaded worker?\n        is_sync = uri.endswith('SyncWorker') or uri == 'sync'\n        if is_sync and self.threads > 1:\n            return \"gthread\"\n        return uri\n\n    @property\n    def worker_class(self):\n        uri = self.settings['worker_class'].get()\n\n        # are we using a threaded worker?\n        is_sync = uri.endswith('SyncWorker') or uri == 'sync'\n        if is_sync and self.threads > 1:\n            uri = \"gunicorn.workers.gthread.ThreadWorker\"\n\n        worker_class = util.load_class(uri)\n        if hasattr(worker_class, \"setup\"):\n            worker_class.setup()\n        return worker_class\n\n    @property\n\n\n# The code to be completed is:\n    def address(self):\n\n        \"\"\"\n        This function retrieves the bind address from settings and returns a list of parsed addresses.\n        Input-Output Arguments\n        :param self: Config. An instance of the Config class.\n        :return: List of addresses. The parsed addresses from settings.\n        \"\"\""}
{"namespace": "bentoml._internal.resource.get_resource", "prompt": "# Please complete the get_resource function based on the contexts above the function.\n\n# The contexts above the function are:\nfrom __future__ import annotations\n\nimport functools\nimport logging\nimport math\nimport os\nimport re\nimport typing as t\nfrom abc import ABC\nfrom abc import abstractmethod\n\nimport psutil\n\nfrom ..exceptions import BentoMLConfigException\n\nlogger = logging.getLogger(__name__)\n\n_RESOURCE_REGISTRY: dict[str, t.Type[Resource[t.Any]]] = {}\n\nT = t.TypeVar(\"T\")\n\n\n\n\n# The code to be completed is:\ndef get_resource(\n    resources: dict[str, t.Any], resource_kind: str, validate: bool = True\n) -> t.Any:\n\n    \"\"\"\n    This function retrieves a resource from a dictionary of resources based on the specified resource kind. It first checks if the resource kind is registered in the resource registry. If it is, it retrieves the corresponding resource class. Then, it checks if the resource kind exists in the resources dictionary. If it does, it checks the value associated with the resource kind. If the value is \"system\", it creates a resource instance from the system. Otherwise, it creates a resource instance from the specified resource specification. If the validate parameter is True, it validates the created resource instance. If the resource kind does not exist in the resources dictionary, it returns None.\n    Input-Output Arguments\n    :param resources: Dict[str, Any]. A dictionary of resources where the keys are resource kinds and the values are resource specifications.\n    :param resource_kind: str. The kind of resource to retrieve.\n    :param validate: bool. Whether to validate the created resource instance. Defaults to True.\n    :return: Any. The retrieved resource instance or None if the resource kind does not exist in the resources dictionary.\n    \"\"\""}
{"namespace": "mingus.containers.instrument.Guitar.can_play_notes", "prompt": "# Please complete the can_play_notes function based on the contexts above the function.\n\n# The contexts above the function are:\n# -*- coding: utf-8 -*-\n\nfrom __future__ import absolute_import\n\n#    mingus - Music theory Python package, instrument module.\n#    Copyright (C) 2008-2009, Bart Spaans\n#\n#    This program is free software: you can redistribute it and/or modify\n#    it under the terms of the GNU General Public License as published by\n#    the Free Software Foundation, either version 3 of the License, or\n#    (at your option) any later version.\n#\n#    This program is distributed in the hope that it will be useful,\n#    but WITHOUT ANY WARRANTY; without even the implied warranty of\n#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#    GNU General Public License for more details.\n#\n#    You should have received a copy of the GNU General Public License\n#    along with this program.  If not, see <http://www.gnu.org/licenses/>.\n\nfrom mingus.containers.note import Note\nfrom mingus.containers.mt_exceptions import UnexpectedObjectError\nimport six\n\n\nclass Instrument(object):\n\n    \"\"\"An instrument object.\n\n    The Instrument class is pretty self explanatory. Instruments can be used\n    with Tracks to define which instrument plays what, with the added bonus\n    of checking whether the entered notes are in the range of the\n    instrument.\n\n    It's probably easiest to subclass your own Instruments (see Piano and\n    Guitar for examples).\n    \"\"\"\n\n    name = \"Instrument\"\n    range = (Note(\"C\", 0), Note(\"C\", 8))\n    clef = \"bass and treble\"\n    tuning = None  # optional StringTuning object\n\n    def __init__(self):\n        pass\n\n    def set_range(self, range):\n        \"\"\"Set the range of the instrument.\n\n        A range is a tuple of two Notes or note strings.\n        \"\"\"\n        if isinstance(range[0], six.string_types):\n            range[0] = Note(range[0])\n            range[1] = Note(range[1])\n        if not hasattr(range[0], \"name\"):\n            raise UnexpectedObjectError(\n                \"Unexpected object '%s'. \"\n                \"Expecting a mingus.containers.Note object\" % range[0]\n            )\n        self.range = range\n\n    def note_in_range(self, note):\n        \"\"\"Test whether note is in the range of this Instrument.\n\n        Return True if so, False otherwise.\n        \"\"\"\n        if isinstance(note, six.string_types):\n            note = Note(note)\n        if not hasattr(note, \"name\"):\n            raise UnexpectedObjectError(\n                \"Unexpected object '%s'. \"\n                \"Expecting a mingus.containers.Note object\" % note\n            )\n        if note >= self.range[0] and note <= self.range[1]:\n            return True\n        return False\n\n    def notes_in_range(self, notes):\n        \"\"\"An alias for can_play_notes.\"\"\"\n        return self.can_play_notes(notes)\n\n    def can_play_notes(self, notes):\n        \"\"\"Test if the notes lie within the range of the instrument.\n\n        Return True if so, False otherwise.\n        \"\"\"\n        if hasattr(notes, \"notes\"):\n            notes = notes.notes\n        if not isinstance(notes, list):\n            notes = [notes]\n        for n in notes:\n            if not self.note_in_range(n):\n                return False\n        return True\n\n    def __repr__(self):\n        \"\"\"Return a string representing the object.\"\"\"\n        return \"%s [%s - %s]\" % (self.name, self.range[0], self.range[1])\n\n\nclass Piano(Instrument):\n\n    name = \"Piano\"\n    range = (Note(\"F\", 0), Note(\"B\", 8))\n\n    def __init__(self):\n        Instrument.__init__(self)\n\n\nclass Guitar(Instrument):\n\n    name = \"Guitar\"\n    range = (Note(\"E\", 3), Note(\"E\", 7))\n    clef = \"Treble\"\n\n    def __init__(self):\n        Instrument.__init__(self)\n\n\n\n# The code to be completed is:\n    def can_play_notes(self, notes):\n\n        \"\"\"\n        Check if the guitar can play the given notes. It checks if the number of notes is greater than 6, and if so, returns False. Otherwise, it just returns the parent method.\n        Input-Output Arguments\n        :param self: Guitar. An instance of the Guitar class.\n        :param notes: List of strings. The notes to be played.\n        :return: Bool. True if the guitar can play the notes, False otherwise.\n        \"\"\""}
{"namespace": "mopidy.config.validators.validate_choice", "prompt": "# Please complete the validate_choice function based on the contexts above the function.\n\n# The contexts above the function are:\n# TODO: add validate regexp?\n\n\ndef validate_required(value, required):\n    \"\"\"Validate that ``value`` is set if ``required``\n\n    Normally called in :meth:`~mopidy.config.types.ConfigValue.deserialize` on\n    the raw string, _not_ the converted value.\n    \"\"\"\n    if required and not value:\n        raise ValueError(\"must be set.\")\n\n\n\n\n# The code to be completed is:\ndef validate_choice(value, choices):\n\n    \"\"\"\n    This function validates whether the given value is one of the choices provided. If the value is not in the choices, it raises a ValueError in the format \"must be one of {names}, not {value}.\".\n    Input-Output Arguments\n    :param value: The value to be validated.\n    :param choices: List. The list of choices to validate the value against.\n    :return: No return values.\n    \"\"\""}
{"namespace": "wal_e.blobstore.file.calling_format.Bucket.delete_keys", "prompt": "# Please complete the delete_keys function based on the contexts above the function.\n\n# The contexts above the function are:\nimport os\nimport shutil\nfrom datetime import datetime\n\n\ndef remove_empty_dirs(path):\n    \"\"\" removes empty dirs under a given path \"\"\"\n    for root, dirs, files in os.walk(path):\n        for d in dirs:\n            dir_path = os.path.join(root, d)\n            if not os.listdir(dir_path):\n                os.rmdir(dir_path)\n\n\ndef ensure_dir_exists(path):\n    \"\"\" create a directory if required \"\"\"\n    dir_path = os.path.dirname(path)\n    if not os.path.exists(dir_path):\n        os.makedirs(dir_path)\n\n\ndef common_dir_path(args, sep='/'):\n    \"\"\" return the highest common directory given a list of files \"\"\"\n    return os.path.commonprefix(args).rpartition(sep)[0]\n\n\ndef epoch_to_iso8601(timestamp):\n    return datetime.utcfromtimestamp(timestamp).isoformat()\n\n\nclass FileKey(object):\n    def __init__(self, bucket, name):\n        self.bucket = bucket\n        self.name = name\n        self.path = os.path.join(\"/\", name.strip(\"/\"))\n        if os.path.isfile(self.path):\n            stat = os.stat(self.path)\n            self.last_modified = epoch_to_iso8601(stat.st_mtime)\n            self.size = stat.st_size\n\n    def get_contents_as_string(self):\n        with open(self.path, 'rb') as fp:\n            contents = fp.read()\n        return contents\n\n    def set_contents_from_file(self, fp):\n        ensure_dir_exists(self.path)\n        with open(self.path, 'wb') as f:\n            shutil.copyfileobj(fp, f)\n        setattr(self, 'size', os.path.getsize(self.path))\n\n    def get_contents_to_file(self, fp):\n        with open(self.path, 'rb') as f:\n            shutil.copyfileobj(f, fp)\n\n\nclass Bucket(object):\n    def __init__(self, name):\n        self.name = name\n\n    def get_key(self, name):\n        return FileKey(bucket=self, name=name)\n\n\n\n# The code to be completed is:\n    def delete_keys(self, keys):\n\n        \"\"\"\n        Delete the specified keys in the Bucket instance. It iterates over the keys and removes the corresponding files from the file system. It also trims any empty directories that may be left after deleting the files.\n        Input-Output Arguments\n        :param self: Bucket. An instance of the Bucket class.\n        :param keys: List of strings. The keys to be deleted.\n        :return: No return values.\n        \"\"\""}
{"namespace": "boto.machinelearning.connect_to_region", "prompt": "# Please complete the connect_to_region function based on the contexts above the function.\n\n# The contexts above the function are:\n# Copyright (c) 2015 Amazon.com, Inc. or its affiliates.\n# All Rights Reserved\n#\n# Permission is hereby granted, free of charge, to any person obtaining a\n# copy of this software and associated documentation files (the\n# \"Software\"), to deal in the Software without restriction, including\n# without limitation the rights to use, copy, modify, merge, publish, dis-\n# tribute, sublicense, and/or sell copies of the Software, and to permit\n# persons to whom the Software is furnished to do so, subject to the fol-\n# lowing conditions:\n#\n# The above copyright notice and this permission notice shall be included\n# in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS\n# OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABIL-\n# ITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT\n# SHALL THE AUTHOR BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY,\n# WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS\n# IN THE SOFTWARE.\n#\nfrom boto.regioninfo import RegionInfo, get_regions\n\n\n\ndef regions():\n    \"\"\"\n    Get all available regions for the Amazon Machine Learning.\n\n    :rtype: list\n    :return: A list of :class:`boto.regioninfo.RegionInfo`\n    \"\"\"\n    from boto.machinelearning.layer1 import MachineLearningConnection\n    return get_regions('machinelearning',\n                       connection_cls=MachineLearningConnection)\n\n\n\n\n# The code to be completed is:\ndef connect_to_region(region_name, **kw_params):\n\n    \"\"\"\n    Connect to a specific region using the MachineLearningConnection class from the boto library. It creates a connection to the specified region using the provided parameters.\n    Input-Output Arguments\n    :param region_name: String. The name of the region to connect to.\n    :param **kw_params: Additional keyword arguments that can be passed to the connection.\n    :return: MachineLearningConnection. The connection object to the specified region.\n    \"\"\""}
{"namespace": "boto.cloudfront.invalidation.InvalidationBatch.escape", "prompt": "# Please complete the escape function based on the contexts above the function.\n\n# The contexts above the function are:\n# Copyright (c) 2006-2010 Chris Moyer http://coredumped.org/\n#\n# Permission is hereby granted, free of charge, to any person obtaining a\n# copy of this software and associated documentation files (the\n# \"Software\"), to deal in the Software without restriction, including\n# without limitation the rights to use, copy, modify, merge, publish, dis-\n# tribute, sublicense, and/or sell copies of the Software, and to permit\n# persons to whom the Software is furnished to do so, subject to the fol-\n# lowing conditions:\n#\n# The above copyright notice and this permission notice shall be included\n# in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS\n# OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABIL-\n# ITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT\n# SHALL THE AUTHOR BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, \n# WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS\n# IN THE SOFTWARE.\n\nimport uuid\n\nfrom boto.compat import urllib\nfrom boto.resultset import ResultSet\n\n\nclass InvalidationBatch(object):\n    \"\"\"A simple invalidation request.\n        :see: http://docs.amazonwebservices.com/AmazonCloudFront/2010-08-01/APIReference/index.html?InvalidationBatchDatatype.html\n    \"\"\"\n\n    def __init__(self, paths=None, connection=None, distribution=None, caller_reference=''):\n        \"\"\"Create a new invalidation request:\n            :paths: An array of paths to invalidate\n        \"\"\"\n        self.paths = paths or []\n        self.distribution = distribution\n        self.caller_reference = caller_reference\n        if not self.caller_reference:\n            self.caller_reference = str(uuid.uuid4())\n\n        # If we passed in a distribution,\n        # then we use that as the connection object\n        if distribution:\n            self.connection = distribution\n        else:\n            self.connection = connection\n\n    def __repr__(self):\n        return '<InvalidationBatch: %s>' % self.id\n\n    def add(self, path):\n        \"\"\"Add another path to this invalidation request\"\"\"\n        return self.paths.append(path)\n\n    def remove(self, path):\n        \"\"\"Remove a path from this invalidation request\"\"\"\n        return self.paths.remove(path)\n\n    def __iter__(self):\n        return iter(self.paths)\n\n    def __getitem__(self, i):\n        return self.paths[i]\n\n    def __setitem__(self, k, v):\n        self.paths[k] = v\n\n\n\n# The code to be completed is:\n    def escape(self, p):\n\n        \"\"\"\n        This function escapes a path. It prepends a slash if it does not start with one and then escapes the path but retaining '/' and '*'.\n        Input-Output Arguments\n        :param self: InvalidationBatch. An instance of the InvalidationBatch class.\n        :param p: str. The path to escape.\n        :return: str. The escaped path.\n        \"\"\""}
{"namespace": "boto.sdb.db.sequence.fib", "prompt": "# Please complete the fib function based on the contexts above the function.\n\n# The contexts above the function are:\n# Copyright (c) 2010 Chris Moyer http://coredumped.org/\n#\n# Permission is hereby granted, free of charge, to any person obtaining a\n# copy of this software and associated documentation files (the\n# \"Software\"), to deal in the Software without restriction, including\n# without limitation the rights to use, copy, modify, merge, publish, dis-\n# tribute, sublicense, and/or sell copies of the Software, and to permit\n# persons to whom the Software is furnished to do so, subject to the fol-\n# lowing conditions:\n#\n# The above copyright notice and this permission notice shall be included\n# in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS\n# OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABIL-\n# ITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT\n# SHALL THE AUTHOR BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, \n# WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS\n# IN THE SOFTWARE.\n\nfrom boto.exception import SDBResponseError\nfrom boto.compat import six\n\nclass SequenceGenerator(object):\n    \"\"\"Generic Sequence Generator object, this takes a single\n    string as the \"sequence\" and uses that to figure out\n    what the next value in a string is. For example\n    if you give \"ABC\" and pass in \"A\" it will give you \"B\",\n    and if you give it \"C\" it will give you \"AA\".\n\n    If you set \"rollover\" to True in the above example, passing\n    in \"C\" would give you \"A\" again.\n\n    The Sequence string can be a string or any iterable\n    that has the \"index\" function and is indexable.\n    \"\"\"\n    __name__ = \"SequenceGenerator\"\n\n    def __init__(self, sequence_string, rollover=False):\n        \"\"\"Create a new SequenceGenerator using the sequence_string\n        as how to generate the next item.\n\n        :param sequence_string: The string or list that explains\n        how to generate the next item in the sequence\n        :type sequence_string: str,iterable\n\n        :param rollover: Rollover instead of incrementing when\n        we hit the end of the sequence\n        :type rollover: bool\n        \"\"\"\n        self.sequence_string = sequence_string\n        self.sequence_length = len(sequence_string[0])\n        self.rollover = rollover\n        self.last_item = sequence_string[-1]\n        self.__name__ = \"%s('%s')\" % (self.__class__.__name__, sequence_string)\n\n    def __call__(self, val, last=None):\n        \"\"\"Get the next value in the sequence\"\"\"\n        # If they pass us in a string that's not at least\n        # the lenght of our sequence, then return the\n        # first element in our sequence\n        if val is None or len(val) < self.sequence_length:\n            return self.sequence_string[0]\n        last_value = val[-self.sequence_length:]\n        if (not self.rollover) and (last_value == self.last_item):\n            val = \"%s%s\" % (self(val[:-self.sequence_length]), self._inc(last_value))\n        else:\n            val = \"%s%s\" % (val[:-self.sequence_length], self._inc(last_value))\n        return val\n\n    def _inc(self, val):\n        \"\"\"Increment a single value\"\"\"\n        assert(len(val) == self.sequence_length)\n        return self.sequence_string[(self.sequence_string.index(val) + 1) % len(self.sequence_string)]\n\n\n#\n# Simple Sequence Functions\n#\ndef increment_by_one(cv=None, lv=None):\n    if cv is None:\n        return 0\n    return cv + 1\n\ndef double(cv=None, lv=None):\n    if cv is None:\n        return 1\n    return cv * 2\n\n\n\n# The code to be completed is:\ndef fib(cv=1, lv=0):\n\n    \"\"\"\n    This function calculates the next number in the Fibonacci sequence based on the last two numbers.\n    Input-Output Arguments\n    :param cv: int. The current value in the sequence. Defaults to 1.\n    :param lv: int. The last value in the sequence. Defaults to 0.\n    :return: int. The next number in the Fibonacci sequence.\n    \"\"\""}
{"namespace": "mopidy.config.types.ConfigValue.serialize", "prompt": "# Please complete the serialize function based on the contexts above the function.\n\n# The contexts above the function are:\nimport logging\nimport re\nimport socket\n\nfrom mopidy.config import validators\nfrom mopidy.internal import log, path\n\n\ndef decode(value):\n    if isinstance(value, bytes):\n        value = value.decode(errors=\"surrogateescape\")\n\n    for char in (\"\\\\\", \"\\n\", \"\\t\"):\n        value = value.replace(\n            char.encode(encoding=\"unicode-escape\").decode(), char\n        )\n\n    return value\n\n\ndef encode(value):\n    if isinstance(value, bytes):\n        value = value.decode(errors=\"surrogateescape\")\n\n    for char in (\"\\\\\", \"\\n\", \"\\t\"):\n        value = value.replace(\n            char, char.encode(encoding=\"unicode-escape\").decode()\n        )\n\n    return value\n\n\nclass DeprecatedValue:\n    pass\n\n\nclass _TransformedValue(str):\n    def __new__(cls, original, transformed):\n        return super().__new__(cls, transformed)\n\n    def __init__(self, original, transformed):\n        self.original = original\n\n\nclass ConfigValue:\n    \"\"\"Represents a config key's value and how to handle it.\n\n    Normally you will only be interacting with sub-classes for config values\n    that encode either deserialization behavior and/or validation.\n\n    Each config value should be used for the following actions:\n\n    1. Deserializing from a raw string and validating, raising ValueError on\n       failure.\n    2. Serializing a value back to a string that can be stored in a config.\n    3. Formatting a value to a printable form (useful for masking secrets).\n\n    :class:`None` values should not be deserialized, serialized or formatted,\n    the code interacting with the config should simply skip None config values.\n    \"\"\"\n\n    def deserialize(self, value):\n        \"\"\"Cast raw string to appropriate type.\"\"\"\n        return decode(value)\n\n\n\n# The code to be completed is:\n    def serialize(self, value, display=False):\n\n        \"\"\"\n        This function converts the input value to a string for saving purposes. If the input value is None, it returns an empty string.\n        Input-Output Arguments\n        :param self: ConfigValue. An instance of the ConfigValue class.\n        :param value: Any. The value to be converted to a string.\n        :param display: Bool. Whether to display the value. Defaults to False.\n        :return: String. The converted string value.\n        \"\"\""}
{"namespace": "pymc.gp.util.kmeans_inducing_points", "prompt": "# Please complete the kmeans_inducing_points function based on the contexts above the function.\n\n# The contexts above the function are:\n#   Copyright 2023 The PyMC Developers\n#\n#   Licensed under the Apache License, Version 2.0 (the \"License\");\n#   you may not use this file except in compliance with the License.\n#   You may obtain a copy of the License at\n#\n#       http://www.apache.org/licenses/LICENSE-2.0\n#\n#   Unless required by applicable law or agreed to in writing, software\n#   distributed under the License is distributed on an \"AS IS\" BASIS,\n#   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#   See the License for the specific language governing permissions and\n#   limitations under the License.\n\nimport warnings\n\nimport numpy as np\nimport pytensor.tensor as pt\n\nfrom pytensor.compile import SharedVariable\nfrom pytensor.tensor.variable import TensorConstant\nfrom scipy.cluster.vq import kmeans\n\n# Avoid circular dependency when importing modelcontext\nfrom pymc.distributions.distribution import Distribution\nfrom pymc.model import modelcontext\nfrom pymc.pytensorf import compile_pymc, walk_model\n\n_ = Distribution  # keep both pylint and black happy\n\nJITTER_DEFAULT = 1e-6\n\n\ndef replace_with_values(vars_needed, replacements=None, model=None):\n    R\"\"\"\n    Replace random variable nodes in the graph with values given by the replacements dict.\n    Uses untransformed versions of the inputs, performs some basic input validation.\n\n    Parameters\n    ----------\n    vars_needed: list of TensorVariables\n        A list of variable outputs\n    replacements: dict with string keys, numeric values\n        The variable name and values to be replaced in the model graph.\n    model: Model\n        A PyMC model object\n    \"\"\"\n    model = modelcontext(model)\n\n    inputs, input_names = [], []\n    for rv in walk_model(vars_needed):\n        if rv in model.named_vars.values() and not isinstance(rv, SharedVariable):\n            inputs.append(rv)\n            input_names.append(rv.name)\n\n    # Then it's deterministic, no inputs are required, can eval and return\n    if len(inputs) == 0:\n        return tuple(v.eval() for v in vars_needed)\n\n    fn = compile_pymc(\n        inputs,\n        vars_needed,\n        allow_input_downcast=True,\n        accept_inplace=True,\n        on_unused_input=\"ignore\",\n    )\n\n    # Remove unneeded inputs\n    replacements = {name: val for name, val in replacements.items() if name in input_names}\n    missing = set(input_names) - set(replacements.keys())\n\n    # Error if more inputs are needed\n    if len(missing) > 0:\n        missing_str = \", \".join(missing)\n        raise ValueError(f\"Values for {missing_str} must be included in `replacements`.\")\n\n    return fn(**replacements)\n\n\ndef stabilize(K, jitter=JITTER_DEFAULT):\n    R\"\"\"\n    Adds small diagonal to a covariance matrix.\n\n    Often the matrices calculated from covariance functions, `K = cov_func(X)`\n    do not appear numerically to be positive semi-definite.  Adding a small\n    correction, `jitter`, to the diagonal is usually enough to fix this.\n\n    Parameters\n    ----------\n    K: array-like\n        A square covariance or kernel matrix.\n    jitter: float\n        A small constant.\n    \"\"\"\n    return K + jitter * pt.identity_like(K)\n\n\n\n\n# The code to be completed is:\ndef kmeans_inducing_points(n_inducing, X, **kmeans_kwargs):\n\n    \"\"\"\n    This function uses the K-means algorithm to initialize the locations of the inducing points `fu` based on the input parameters.\n    Input-Output Arguments\n    :param n_inducing: int. The number of inducing points (or k, the number of clusters).\n    :param X: array-like. Gaussian process input matrix.\n    :param **kmeans_kwargs: Extra keyword arguments that are passed to `scipy.cluster.vq.kmeans`.\n    :return: array-like. The initialized locations of the inducing points `fu` multiplied by the scaling factor.\n    \"\"\""}
{"namespace": "praw.models.listing.listing.ModNoteListing.after", "prompt": "# Please complete the after function based on the contexts above the function.\n\n# The contexts above the function are:\n\"\"\"Provide the Listing class.\"\"\"\nfrom typing import Any, Optional\n\nfrom ..base import PRAWBase\n\n\nclass Listing(PRAWBase):\n    \"\"\"A listing is a collection of :class:`.RedditBase` instances.\"\"\"\n\n    AFTER_PARAM = \"after\"\n    CHILD_ATTRIBUTE = \"children\"\n\n    def __getitem__(self, index: int) -> Any:\n        \"\"\"Return the item at position index in the list.\"\"\"\n        return getattr(self, self.CHILD_ATTRIBUTE)[index]\n\n    def __len__(self) -> int:\n        \"\"\"Return the number of items in the Listing.\"\"\"\n        return len(getattr(self, self.CHILD_ATTRIBUTE))\n\n    def __setattr__(self, attribute: str, value: Any):\n        \"\"\"Objectify the ``CHILD_ATTRIBUTE`` attribute.\"\"\"\n        if attribute == self.CHILD_ATTRIBUTE:\n            value = self._reddit._objector.objectify(value)\n        super().__setattr__(attribute, value)\n\n\nclass FlairListing(Listing):\n    \"\"\"Special Listing for handling flair lists.\"\"\"\n\n    CHILD_ATTRIBUTE = \"users\"\n\n    @property\n    def after(self) -> Optional[Any]:\n        \"\"\"Return the next attribute or ``None``.\"\"\"\n        return getattr(self, \"next\", None)\n\n\nclass ModNoteListing(Listing):\n    \"\"\"Special Listing for handling :class:`.ModNote` lists.\"\"\"\n\n    AFTER_PARAM = \"before\"\n    CHILD_ATTRIBUTE = \"mod_notes\"\n\n    @property\n\n\n# The code to be completed is:\n    def after(self) -> Optional[Any]:\n\n        \"\"\"\n        This method returns the next attribute or None based on the condition. If the \"has_next_page\" attribute is False, it returns None. Otherwise, it returns the \"end_cursor\" attribute.\n        Input-Output Arguments\n        :param self: ModNoteListing. An instance of the ModNoteListing class.\n        :return: Optional[Any]. The next attribute or None.\n        \"\"\""}
{"namespace": "alembic.config.Config.print_stdout", "prompt": "# Please complete the print_stdout function based on the contexts above the function.\n\n# The contexts above the function are:\nfrom __future__ import annotations\n\nfrom argparse import ArgumentParser\nfrom argparse import Namespace\nfrom configparser import ConfigParser\nimport inspect\nimport os\nimport sys\nfrom typing import Any\nfrom typing import cast\nfrom typing import Dict\nfrom typing import Mapping\nfrom typing import Optional\nfrom typing import overload\nfrom typing import TextIO\nfrom typing import Union\n\nfrom typing_extensions import TypedDict\n\nfrom . import __version__\nfrom . import command\nfrom . import util\nfrom .util import compat\n\n\nclass Config:\n    r\"\"\"Represent an Alembic configuration.\n\n    Within an ``env.py`` script, this is available\n    via the :attr:`.EnvironmentContext.config` attribute,\n    which in turn is available at ``alembic.context``::\n\n        from alembic import context\n\n        some_param = context.config.get_main_option(\"my option\")\n\n    When invoking Alembic programmatically, a new\n    :class:`.Config` can be created by passing\n    the name of an .ini file to the constructor::\n\n        from alembic.config import Config\n        alembic_cfg = Config(\"/path/to/yourapp/alembic.ini\")\n\n    With a :class:`.Config` object, you can then\n    run Alembic commands programmatically using the directives\n    in :mod:`alembic.command`.\n\n    The :class:`.Config` object can also be constructed without\n    a filename.   Values can be set programmatically, and\n    new sections will be created as needed::\n\n        from alembic.config import Config\n        alembic_cfg = Config()\n        alembic_cfg.set_main_option(\"script_location\", \"myapp:migrations\")\n        alembic_cfg.set_main_option(\"sqlalchemy.url\", \"postgresql://foo/bar\")\n        alembic_cfg.set_section_option(\"mysection\", \"foo\", \"bar\")\n\n    .. warning::\n\n       When using programmatic configuration, make sure the\n       ``env.py`` file in use is compatible with the target configuration;\n       including that the call to Python ``logging.fileConfig()`` is\n       omitted if the programmatic configuration doesn't actually include\n       logging directives.\n\n    For passing non-string values to environments, such as connections and\n    engines, use the :attr:`.Config.attributes` dictionary::\n\n        with engine.begin() as connection:\n            alembic_cfg.attributes['connection'] = connection\n            command.upgrade(alembic_cfg, \"head\")\n\n    :param file\\_: name of the .ini file to open.\n    :param ini_section: name of the main Alembic section within the\n     .ini file\n    :param output_buffer: optional file-like input buffer which\n     will be passed to the :class:`.MigrationContext` - used to redirect\n     the output of \"offline generation\" when using Alembic programmatically.\n    :param stdout: buffer where the \"print\" output of commands will be sent.\n     Defaults to ``sys.stdout``.\n\n    :param config_args: A dictionary of keys and values that will be used\n     for substitution in the alembic config file.  The dictionary as given\n     is **copied** to a new one, stored locally as the attribute\n     ``.config_args``. When the :attr:`.Config.file_config` attribute is\n     first invoked, the replacement variable ``here`` will be added to this\n     dictionary before the dictionary is passed to ``ConfigParser()``\n     to parse the .ini file.\n\n    :param attributes: optional dictionary of arbitrary Python keys/values,\n     which will be populated into the :attr:`.Config.attributes` dictionary.\n\n     .. seealso::\n\n        :ref:`connection_sharing`\n\n    \"\"\"\n\n    def __init__(\n        self,\n        file_: Union[str, os.PathLike[str], None] = None,\n        ini_section: str = \"alembic\",\n        output_buffer: Optional[TextIO] = None,\n        stdout: TextIO = sys.stdout,\n        cmd_opts: Optional[Namespace] = None,\n        config_args: Mapping[str, Any] = util.immutabledict(),\n        attributes: Optional[dict] = None,\n    ) -> None:\n        \"\"\"Construct a new :class:`.Config`\"\"\"\n        self.config_file_name = file_\n        self.config_ini_section = ini_section\n        self.output_buffer = output_buffer\n        self.stdout = stdout\n        self.cmd_opts = cmd_opts\n        self.config_args = dict(config_args)\n        if attributes:\n            self.attributes.update(attributes)\n\n    cmd_opts: Optional[Namespace] = None\n    \"\"\"The command-line options passed to the ``alembic`` script.\n\n    Within an ``env.py`` script this can be accessed via the\n    :attr:`.EnvironmentContext.config` attribute.\n\n    .. seealso::\n\n        :meth:`.EnvironmentContext.get_x_argument`\n\n    \"\"\"\n\n    config_file_name: Union[str, os.PathLike[str], None] = None\n    \"\"\"Filesystem path to the .ini file in use.\"\"\"\n\n    config_ini_section: str = None  # type:ignore[assignment]\n    \"\"\"Name of the config file section to read basic configuration\n    from.  Defaults to ``alembic``, that is the ``[alembic]`` section\n    of the .ini file.  This value is modified using the ``-n/--name``\n    option to the Alembic runner.\n\n    \"\"\"\n\n    @util.memoized_property\n    def attributes(self):\n        \"\"\"A Python dictionary for storage of additional state.\n\n\n        This is a utility dictionary which can include not just strings but\n        engines, connections, schema objects, or anything else.\n        Use this to pass objects into an env.py script, such as passing\n        a :class:`sqlalchemy.engine.base.Connection` when calling\n        commands from :mod:`alembic.command` programmatically.\n\n        .. seealso::\n\n            :ref:`connection_sharing`\n\n            :paramref:`.Config.attributes`\n\n        \"\"\"\n        return {}\n\n\n\n# The code to be completed is:\n    def print_stdout(self, text: str, *arg) -> None:\n\n        \"\"\"\n        This function is used to render a message to standard output. It takes a text string as input and formats it with additional arguments if provided. The formatted message is then output to the standard output. If no additional arguments are provided, the text is output verbatim. This function does nothing if the \"quiet\" messaging option is enabled.\n        Input-Output Arguments\n        :param self: Config. An instance of the Config class.\n        :param text: String. The text to be rendered to standard output.\n        :param *arg: Additional arguments to be formatted against the provided text.\n        :return: None.\n        \"\"\""}
{"namespace": "mrjob.util.safeeval", "prompt": "# Please complete the safeeval function based on the contexts above the function.\n\n# The contexts above the function are:\n# Copyright 2009-2016 Yelp and Contributors\n# Copyright 2017-2018 Yelp\n# Copyright 2019 Yelp\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Utility functions for MRJob\n\"\"\"\n# don't add imports here that aren't part of the standard Python library,\n# since MRJobs need to run in Amazon's generic EMR environment\nimport logging\nimport os\nimport os.path\nimport pipes\nimport random\nimport shlex\nimport shutil\nimport sys\nimport tarfile\nfrom contextlib import contextmanager\nfrom datetime import timedelta\nfrom distutils.spawn import find_executable\nfrom logging import getLogger\nfrom zipfile import ZIP_DEFLATED\nfrom zipfile import ZIP_STORED\nfrom zipfile import ZipFile\nfrom zipfile import is_zipfile\n\n\n\n\nlog = getLogger(__name__)\n\n\nclass NullHandler(logging.Handler):\n    def emit(self, record):\n        pass\n\n\ndef cmd_line(args):\n    \"\"\"build a command line that works in a shell.\n    \"\"\"\n    args = [str(x) for x in args]\n    return ' '.join(pipes.quote(x) for x in args)\n\n\ndef expand_path(path):\n    \"\"\"Resolve ``~`` (home dir) and environment variables in *path*.\n\n    If *path* is ``None``, return ``None``.\n    \"\"\"\n    if path is None:\n        return None\n    else:\n        return os.path.expanduser(os.path.expandvars(path))\n\n\ndef file_ext(filename):\n    \"\"\"return the file extension, including the ``.``\n\n    >>> file_ext('foo.tar.gz')\n    '.tar.gz'\n\n    >>> file_ext('.emacs')\n    ''\n\n    >>> file_ext('.mrjob.conf')\n    '.conf'\n    \"\"\"\n    stripped_name = filename.lstrip('.')\n    dot_index = stripped_name.find('.')\n\n    if dot_index == -1:\n        return ''\n    return stripped_name[dot_index:]\n\n\ndef log_to_null(name=None):\n    \"\"\"Set up a null handler for the given stream, to suppress\n    \"no handlers could be found\" warnings.\"\"\"\n    logger = logging.getLogger(name)\n    logger.addHandler(NullHandler())\n\n\ndef log_to_stream(name=None, stream=None, format=None, level=None,\n                  debug=False):\n    \"\"\"Set up logging.\n\n    :type name: str\n    :param name: name of the logger, or ``None`` for the root logger\n    :type stream: file object\n    :param stream:  stream to log to (default is ``sys.stderr``)\n    :type format: str\n    :param format: log message format (default is '%(message)s')\n    :param level: log level to use\n    :type debug: bool\n    :param debug: quick way of setting the log level: if true, use\n                  ``logging.DEBUG``, otherwise use ``logging.INFO``\n    \"\"\"\n    if level is None:\n        level = logging.DEBUG if debug else logging.INFO\n\n    if format is None:\n        format = '%(message)s'\n\n    if stream is None:\n        stream = sys.stderr\n\n    handler = logging.StreamHandler(stream)\n    handler.setLevel(level)\n    handler.setFormatter(logging.Formatter(format))\n\n    logger = logging.getLogger(name)\n    logger.setLevel(level)\n    logger.addHandler(handler)\n\n\ndef random_identifier():\n    \"\"\"A random 16-digit hex string.\"\"\"\n    return '%016x' % random.randint(0, 2 ** 64 - 1)\n\n\n# Thanks to http://lybniz2.sourceforge.net/safeeval.html for\n# explaining how to do this!\n\n\n# The code to be completed is:\ndef safeeval(expr, globals=None, locals=None):\n\n    \"\"\"\n    This function evaluates an expression in a safe environment. It creates a dictionary of safe global variables, including True, False, None, set and range or xrange. `open` is specially handled to raise a NameError with the message \"name 'open' is not defined\".\n    Input-Output Arguments\n    :param expr: The expression to be evaluated.\n    :param globals: dict. Optional dictionary mapping names to values for global variables. Defaults to None.\n    :param locals: dict. Optional dictionary mapping names to values for local variables. Defaults to None.\n    :return: The result of evaluating the expression.\n    \"\"\""}
{"namespace": "falcon.cmd.inspect_app.make_parser", "prompt": "# Please complete the make_parser function based on the contexts above the function.\n\n# The contexts above the function are:\n#!/usr/bin/env python\n# Copyright 2013 by Rackspace Hosting, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an 'AS IS' BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"\nScript that prints out the routes of an App instance.\n\"\"\"\nimport argparse\nimport importlib\nimport os\nimport sys\n\nimport falcon\nfrom falcon.inspect import inspect_app\nfrom falcon.inspect import inspect_routes\nfrom falcon.inspect import StringVisitor\n\nsys.path.append(os.getcwd())\n\n\n\n\n# The code to be completed is:\ndef make_parser():\n\n    \"\"\"\n    Create a parser for the application with the specified arguments and options. The arguments contains \"-r (--router)\", \"-v (--verbose)\", \"-i (--internal)\", and \"app_module\".\n    Input-Output Arguments\n    :param: No input parameters.\n    :return: ArgumentParser. The created parser instance.\n    \"\"\""}
{"namespace": "mingus.core.keys.get_key_signature_accidentals", "prompt": "# Please complete the get_key_signature_accidentals function based on the contexts above the function.\n\n# The contexts above the function are:\n# -*- coding: utf-8 -*-\n\n#    mingus - Music theory Python package, keys module.\n#    Copyright (C) 2010-2011, Carlo Stemberger\n#\n#    This program is free software: you can redistribute it and/or modify\n#    it under the terms of the GNU General Public License as published by\n#    the Free Software Foundation, either version 3 of the License, or\n#    (at your option) any later version.\n#\n#    This program is distributed in the hope that it will be useful,\n#    but WITHOUT ANY WARRANTY; without even the implied warranty of\n#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#    GNU General Public License for more details.\n#\n#    You should have received a copy of the GNU General Public License\n#    along with this program.  If not, see <http://www.gnu.org/licenses/>.\n\n\"\"\"Module for dealing with keys.\n\nThis module provides a simple interface for dealing with keys.\n\"\"\"\nfrom __future__ import absolute_import\n\nfrom itertools import cycle, islice\n\nfrom six.moves import range\n\n\nfrom mingus.core.mt_exceptions import NoteFormatError\n\nkeys = [\n    (\"Cb\", \"ab\"),  #  7 b\n    (\"Gb\", \"eb\"),  #  6 b\n    (\"Db\", \"bb\"),  #  5 b\n    (\"Ab\", \"f\"),  #  4 b\n    (\"Eb\", \"c\"),  #  3 b\n    (\"Bb\", \"g\"),  #  2 b\n    (\"F\", \"d\"),  #  1 b\n    (\"C\", \"a\"),  #  nothing\n    (\"G\", \"e\"),  #  1 #\n    (\"D\", \"b\"),  #  2 #\n    (\"A\", \"f#\"),  #  3 #\n    (\"E\", \"c#\"),  #  4 #\n    (\"B\", \"g#\"),  #  5 #\n    (\"F#\", \"d#\"),  #  6 #\n    (\"C#\", \"a#\"),  #  7 #\n]\n\nmajor_keys = [couple[0] for couple in keys]\nminor_keys = [couple[1] for couple in keys]\n\nbase_scale = [\"C\", \"D\", \"E\", \"F\", \"G\", \"A\", \"B\"]\n\n_key_cache = {}\n\n\ndef is_valid_key(key):\n    \"\"\"Return True if key is in a recognized format. False if not.\"\"\"\n    for couple in keys:\n        if key in couple:\n            return True\n    return False\n\n\ndef get_key(accidentals=0):\n    \"\"\"Return the key corrisponding to accidentals.\n\n    Return the tuple containing the major key corrensponding to the\n    accidentals put as input, and his relative minor; negative numbers for\n    flats, positive numbers for sharps.\n    \"\"\"\n    from mingus.core.mt_exceptions import RangeError\n    if accidentals not in range(-7, 8):\n        raise RangeError(\"integer not in range (-7)-(+7).\")\n    return keys[accidentals + 7]\n\n\ndef get_key_signature(key=\"C\"):\n    \"\"\"Return the key signature.\n\n    0 for C or a, negative numbers for flat key signatures, positive numbers\n    for sharp key signatures.\n    \"\"\"\n    if not is_valid_key(key):\n        raise NoteFormatError(\"unrecognized format for key '%s'\" % key)\n\n    for couple in keys:\n        if key in couple:\n            accidentals = keys.index(couple) - 7\n            return accidentals\n\n\n\n\n# The code to be completed is:\ndef get_key_signature_accidentals(key=\"C\"):\n\n    \"\"\"\n    This function returns a list of accidentals present in the key signature of a given key. It first determines the number of accidentals in the key signature. Then, it creates a list of accidentals based on the number of accidentals and the key.\n    Input-Output Arguments\n    :param key: String. The key for which the key signature accidentals are to be determined. It defaults to \"C\" if not specified.\n    :return: List of strings. The list of accidentals present in the key signature.\n    \"\"\""}
{"namespace": "albumentations.core.transforms_interface.to_tuple", "prompt": "# Please complete the to_tuple function based on the contexts above the function.\n\n# The contexts above the function are:\nfrom __future__ import absolute_import\n\nimport random\nfrom copy import deepcopy\nfrom typing import Any, Callable, Dict, List, Optional, Sequence, Tuple, Union, cast\nfrom warnings import warn\n\nimport cv2\nimport numpy as np\n\nfrom .serialization import Serializable, get_shortest_class_fullname\nfrom .utils import format_args\n\n__all__ = [\n    \"to_tuple\",\n    \"BasicTransform\",\n    \"DualTransform\",\n    \"ImageOnlyTransform\",\n    \"NoOp\",\n    \"BoxType\",\n    \"KeypointType\",\n    \"ImageColorType\",\n    \"ScaleFloatType\",\n    \"ScaleIntType\",\n    \"ImageColorType\",\n]\n\nNumType = Union[int, float, np.ndarray]\nBoxInternalType = Tuple[float, float, float, float]\nBoxType = Union[BoxInternalType, Tuple[float, float, float, float, Any]]\nKeypointInternalType = Tuple[float, float, float, float]\nKeypointType = Union[KeypointInternalType, Tuple[float, float, float, float, Any]]\nImageColorType = Union[float, Sequence[float]]\n\nScaleFloatType = Union[float, Tuple[float, float]]\nScaleIntType = Union[int, Tuple[int, int]]\n\nFillValueType = Optional[Union[int, float, Sequence[int], Sequence[float]]]\n\n\n\n\n# The code to be completed is:\ndef to_tuple(param, low=None, bias=None):\n\n    \"\"\"\n    Convert the input argument to a min-max tuple. If the input is a scalar, it returns a tuple with the first element as the offset - value and the second element as the offset + value. If the input is a tuple, it returns the value + offset (broadcasted).\n    Input-Output Arguments\n    :param param: scalar, tuple or list of 2+ elements. Input value.\n    If value is scalar, return value would be (offset - value, offset + value).\n    If value is tuple, return value would be value + offset (broadcasted).\n    :param low: Second element of tuple can be passed as an optional argument.\n    :param bias: An offset factor added to each element.\n    :return: tuple. The min-max tuple of the input argument.\n    \"\"\""}
{"namespace": "lux.action.default.register_default_actions", "prompt": "# Please complete the register_default_actions function based on the contexts above the function.\n\n# The contexts above the function are:\n\n\n# The code to be completed is:\ndef register_default_actions():\n\n    \"\"\"\n    This function registers default actions for the Lux library. It imports various action modules and defines display conditions for each action. Then, it globally registers each action with its corresponding display condition.\n    Input-Output Arguments\n    :param: No input parameters.\n    :return: No return values.\n    \"\"\""}
{"namespace": "threatingestor.Ingestor.run", "prompt": "# Please complete the run function based on the contexts above the function.\n\n# The contexts above the function are:\nimport sys\nimport time\nimport collections\n\nfrom loguru import logger\nimport statsd\n\ntry:\n    import notifiers\n    from notifiers.logging import NotificationHandler\nexcept ImportError:\n    logger.info(\"Notifiers is not installed.\")\n    notifiers = None\n\nimport threatingestor.config\nimport threatingestor.state\nimport threatingestor.exceptions\nimport threatingestor.whitelist\n\nclass Ingestor:\n    \"\"\"ThreatIngestor main work logic.\n\n    Handles reading the config file, calling sources, maintaining state, and\n    sending artifacts to operators.\n    \"\"\"\n    def __init__(self, config_file):\n        # Load config.\n        try:\n            logger.debug(f\"Reading config from '{config_file}'\")\n            self.config = config.Config(config_file)\n        except (OSError, threatingestor.exceptions.IngestorError):\n            # Error loading config.\n            logger.exception(\"Couldn't read config\")\n            sys.exit(1)\n\n        # Configure logging with optional notifiers.\n        logger.configure(**self.config.logging())\n        try:\n            logger.level(\"NOTIFY\", no=35, color=\"<yellow>\", icon=\"\\U0001F514\")\n        except TypeError:\n            # logger raises TypeError if NOTIFY is already defined\n            pass\n\n        if notifiers:\n            notifier_config = self.config.notifiers()\n            notifier = notifiers.get_notifier(notifier_config.get('provider'))\n\n            if notifier:\n                logger.debug(f\"Adding notification handler '{notifier_config.get('provider')}'\")\n                # Notifier 'provider_name' is set and valid.\n                handler = NotificationHandler(**notifier_config)\n                logger.add(handler, level=\"NOTIFY\")\n\n        logger.debug(\"Log handler reconfigured\")\n\n        # Configure statsd.\n        try:\n            self.statsd = statsd.StatsClient(**self.config.statsd())\n            self.statsd.incr('start')\n        except TypeError:\n            logger.exception(\"Couldn't initialize statsd client; bad config?\")\n            sys.exit(1)\n\n        # Load state DB.\n        try:\n            logger.debug(f\"Opening state database '{self.config.state_path()}'\")\n            self.statedb = threatingestor.state.State(self.config.state_path())\n        except (OSError, IOError, threatingestor.exceptions.IngestorError):\n            # Error loading state DB.\n            logger.exception(\"Error reading state database\")\n            sys.exit(1)\n\n        # Instantiate plugins.\n        try:\n            logger.debug(\"Initializing sources\")\n            self.sources = {name: source(**kwargs)\n                            for name, source, kwargs in self.config.sources()}\n\n            logger.debug(\"Initializing operators\")\n            self.operators = {name: operator(**kwargs)\n                              for name, operator, kwargs in self.config.operators()}\n\n            logger.debug(\"Initializing whitelists\")\n            self.whitelist = threatingestor.whitelist.Whitelist(self.config.whitelists())\n\n        except (TypeError, ConnectionError, threatingestor.exceptions.PluginError):\n            logger.warning(\"Twitter config format has recently changed. See https://github.com/InQuest/ThreatIngestor/releases/tag/v1.0.0b5\")\n            logger.exception(\"Error initializing plugins\")\n            sys.exit(1)\n\n    def _is_whitelisted(self, artifact) -> bool:\n        if self.whitelist.contains(str(artifact)):\n            logger.debug(\n                f\"Reject {str(artifact)} from further processing because it is whitelisted.\"\n            )\n            return True\n        return False\n\n\n\n# The code to be completed is:\n    def run(self):\n\n        \"\"\"\n        This function runs the Ingestor instance either once or forever, depending on the configuration. If the configuration specifies to run as a daemon, it runs the instance in a loop. Otherwise, it runs the instance once to completion.\n        Input-Output Arguments\n        :param self: Ingestor. An instance of the Ingestor class.\n        :return: No return values.\n        \"\"\""}
{"namespace": "playhouse.signals.Signal.connect", "prompt": "# Please complete the connect function based on the contexts above the function.\n\n# The contexts above the function are:\n\"\"\"\nProvide django-style hooks for model events.\n\"\"\"\nfrom peewee import Model as _Model\n\n\nclass Signal(object):\n    def __init__(self):\n        self._flush()\n\n    def _flush(self):\n        self._receivers = set()\n        self._receiver_list = []\n\n\n\n# The code to be completed is:\n    def connect(self, receiver, name=None, sender=None):\n\n        \"\"\"\n        Connect a receiver function to a signal. It adds the receiver function to the list of receivers for the signal, along with the name and sender (if specified). If a receiver with the same name and sender already exists, it raises a ValueError.\n        Input-Output Arguments\n        :param self: Signal. An instance of the Signal class.\n        :param receiver: The function to be connected as a receiver.\n        :param name: String. The name of the receiver. If not specified, it defaults to the name of the receiver function. Defaults to None.\n        :param sender: Any. The sender object. If specified, the receiver will only be called when the signal is emitted by this sender. Defaults to None.\n        :return: No return values.\n        \"\"\""}
{"namespace": "bentoml._internal.utils.analytics.usage_stats.get_serve_info", "prompt": "# Please complete the get_serve_info function based on the contexts above the function.\n\n# The contexts above the function are:\nfrom __future__ import annotations\n\nimport contextlib\nimport logging\nimport os\nimport secrets\nimport threading\nimport typing as t\nfrom datetime import datetime\nfrom datetime import timezone\nfrom functools import lru_cache\nfrom functools import wraps\nfrom typing import TYPE_CHECKING\n\nimport attr\nimport httpx\nfrom simple_di import Provide\nfrom simple_di import inject\n\nfrom ...configuration import get_debug_mode\nfrom ...configuration.containers import BentoMLContainer\nfrom ...utils import compose\nfrom .schemas import CommonProperties\nfrom .schemas import EventMeta\nfrom .schemas import ServeInitEvent\nfrom .schemas import ServeUpdateEvent\nfrom .schemas import TrackingPayload\n\nif TYPE_CHECKING:\n    P = t.ParamSpec(\"P\")\n    T = t.TypeVar(\"T\")\n    AsyncFunc = t.Callable[P, t.Coroutine[t.Any, t.Any, t.Any]]\n\n    from prometheus_client.samples import Sample\n\n    from bentoml import Service\n\n    from ...server.metrics.prometheus import PrometheusClient\n\nlogger = logging.getLogger(__name__)\n\nBENTOML_DO_NOT_TRACK = \"BENTOML_DO_NOT_TRACK\"\nBENTOML_SERVE_FROM_SERVER_API = \"__BENTOML_SERVE_FROM_SERVER_API\"\nUSAGE_TRACKING_URL = \"https://t.bentoml.com\"\nSERVE_USAGE_TRACKING_INTERVAL_SECONDS = int(12 * 60 * 60)  # every 12 hours\nUSAGE_REQUEST_TIMEOUT_SECONDS = 1\n\n\n@lru_cache(maxsize=None)\ndef _bentoml_serve_from_server_api() -> bool:\n    return os.environ.get(BENTOML_SERVE_FROM_SERVER_API, str(False)).lower() == \"true\"\n\n\n@lru_cache(maxsize=1)\ndef do_not_track() -> bool:  # pragma: no cover\n    # Returns True if and only if the environment variable is defined and has value True.\n    # The function is cached for better performance.\n    return os.environ.get(BENTOML_DO_NOT_TRACK, str(False)).lower() == \"true\"\n\n\n@lru_cache(maxsize=1)\ndef _usage_event_debugging() -> bool:\n    # For BentoML developers only - debug and print event payload if turned on\n    return os.environ.get(\"__BENTOML_DEBUG_USAGE\", str(False)).lower() == \"true\"\n\n\ndef silent(func: t.Callable[P, T]) -> t.Callable[P, T]:  # pragma: no cover\n    # Silent errors when tracking\n    @wraps(func)\n    def wrapper(*args: P.args, **kwargs: P.kwargs) -> t.Any:\n        try:\n            return func(*args, **kwargs)\n        except Exception as err:  # pylint: disable=broad-except\n            if _usage_event_debugging():\n                if get_debug_mode():\n                    logger.error(\n                        \"Tracking Error: %s\", err, stack_info=True, stacklevel=3\n                    )\n                else:\n                    logger.info(\"Tracking Error: %s\", err)\n            else:\n                logger.debug(\"Tracking Error: %s\", err)\n\n    return wrapper\n\n\n@attr.define\nclass ServeInfo:\n    serve_id: str\n    serve_started_timestamp: datetime\n\n\n\n\n# The code to be completed is:\ndef get_serve_info() -> ServeInfo:  # pragma: no cover\n    # Returns a safe token for serve as well as timestamp of creating this token\n\n    \"\"\"\n    This function generates a safe token for serving and returns the serve information, including the serve ID and the timestamp when the token was created.\n    Input-Output Arguments\n    :param: No input parameters.\n    :return: ServeInfo. An instance of the ServeInfo class, containing the serve ID and the timestamp of token creation.\n    \"\"\""}
{"namespace": "boto.codedeploy.connect_to_region", "prompt": "# Please complete the connect_to_region function based on the contexts above the function.\n\n# The contexts above the function are:\n# Copyright (c) 2015 Amazon.com, Inc. or its affiliates.  All Rights Reserved\n#\n# Permission is hereby granted, free of charge, to any person obtaining a\n# copy of this software and associated documentation files (the\n# \"Software\"), to deal in the Software without restriction, including\n# without limitation the rights to use, copy, modify, merge, publish, dis-\n# tribute, sublicense, and/or sell copies of the Software, and to permit\n# persons to whom the Software is furnished to do so, subject to the fol-\n# lowing conditions:\n#\n# The above copyright notice and this permission notice shall be included\n# in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS\n# OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABIL-\n# ITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT\n# SHALL THE AUTHOR BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY,\n# WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS\n# IN THE SOFTWARE.\n#\nfrom boto.regioninfo import RegionInfo, get_regions\n\n\n\ndef regions():\n    \"\"\"\n    Get all available regions for the AWS CodeDeploy service.\n\n    :rtype: list\n    :return: A list of :class:`boto.regioninfo.RegionInfo`\n    \"\"\"\n    from boto.codedeploy.layer1 import CodeDeployConnection\n    return get_regions('codedeploy', connection_cls=CodeDeployConnection)\n\n\n\n\n# The code to be completed is:\ndef connect_to_region(region_name, **kw_params):\n\n    \"\"\"\n    Connect to a specific region using the CodeDeployConnection class from the boto library. It creates a connection to the specified region using the provided parameters.\n    Input-Output Arguments\n    :param region_name: String. The name of the region to connect to.\n    :param **kw_params: Additional keyword arguments that can be passed to the connection.\n    :return: CodeDeployConnection. The connection object to the specified region.\n    \"\"\""}
{"namespace": "sacred.metrics_logger.linearize_metrics", "prompt": "# Please complete the linearize_metrics function based on the contexts above the function.\n\n# The contexts above the function are:\n#!/usr/bin/env python\n# coding=utf-8\nimport datetime\nimport sacred.optional as opt\n\nfrom queue import Queue, Empty\n\n\nclass MetricsLogger:\n    \"\"\"MetricsLogger collects metrics measured during experiments.\n\n    MetricsLogger is the (only) part of the Metrics API.\n    An instance of the class should be created for the Run class, such that the\n    log_scalar_metric method is accessible from running experiments using\n    _run.metrics.log_scalar_metric.\n    \"\"\"\n\n    def __init__(self):\n        # Create a message queue that remembers\n        # calls of the log_scalar_metric\n        self._logged_metrics = Queue()\n        self._metric_step_counter = {}\n        \"\"\"Remembers the last number of each metric.\"\"\"\n\n    def log_scalar_metric(self, metric_name, value, step=None):\n        \"\"\"\n        Add a new measurement.\n\n        The measurement will be processed by the MongoDB observer\n        during a heartbeat event.\n        Other observers are not yet supported.\n\n        :param metric_name: The name of the metric, e.g. training.loss.\n        :param value: The measured value.\n        :param step: The step number (integer), e.g. the iteration number\n                    If not specified, an internal counter for each metric\n                    is used, incremented by one.\n        \"\"\"\n        if opt.has_numpy:\n            np = opt.np\n            if isinstance(value, np.generic):\n                value = value.item()\n            if isinstance(step, np.generic):\n                step = step.item()\n        if step is None:\n            step = self._metric_step_counter.get(metric_name, -1) + 1\n        self._logged_metrics.put(\n            ScalarMetricLogEntry(metric_name, step, datetime.datetime.utcnow(), value)\n        )\n        self._metric_step_counter[metric_name] = step\n\n    def get_last_metrics(self):\n        \"\"\"Read all measurement events since last call of the method.\n\n        :return List[ScalarMetricLogEntry]\n        \"\"\"\n        read_up_to = self._logged_metrics.qsize()\n        messages = []\n        for i in range(read_up_to):\n            try:\n                messages.append(self._logged_metrics.get_nowait())\n            except Empty:\n                pass\n        return messages\n\n\nclass ScalarMetricLogEntry:\n    \"\"\"Container for measurements of scalar metrics.\n\n    There is exactly one ScalarMetricLogEntry per logged scalar metric value.\n    \"\"\"\n\n    def __init__(self, name, step, timestamp, value):\n        self.name = name\n        self.step = step\n        self.timestamp = timestamp\n        self.value = value\n\n\n\n\n# The code to be completed is:\ndef linearize_metrics(logged_metrics):\n\n    \"\"\"\n    Group metrics by name. It takes a list of individual measurements, possibly belonging to different metrics and groups them by name.\n    Input-Output Arguments\n    :param logged_metrics: A list of ScalarMetricLogEntries\n    :return: Measured values grouped by the metric name:\n    {\"metric_name1\": {\"steps\": [0,1,2], \"values\": [4, 5, 6],\n    \"timestamps\": [datetime, datetime, datetime]},\n    \"metric_name2\": {...}}\n    \"\"\""}
{"namespace": "pyramid.request.CallbackMethodsMixin._process_finished_callbacks", "prompt": "# Please complete the _process_finished_callbacks function based on the contexts above the function.\n\n# The contexts above the function are:\nfrom collections import deque\nimport functools\nimport weakref\nfrom webob import BaseRequest\nfrom zope.interface import implementer\nfrom zope.interface.interface import InterfaceClass\n\nfrom pyramid.decorator import reify\nfrom pyramid.i18n import LocalizerRequestMixin\nfrom pyramid.interfaces import IRequest, IRequestExtensions, IResponse\nfrom pyramid.response import Response, _get_response_factory\nfrom pyramid.security import AuthenticationAPIMixin, SecurityAPIMixin\nfrom pyramid.url import URLMethodsMixin\nfrom pyramid.util import (\n    InstancePropertyHelper,\n    InstancePropertyMixin,\n    Sentinel,\n    bytes_,\n    text_,\n)\nfrom pyramid.view import ViewMethodsMixin\n\n\nclass TemplateContext:\n    pass\n\n\nclass CallbackMethodsMixin:\n    @reify\n    def finished_callbacks(self):\n        return deque()\n\n    @reify\n    def response_callbacks(self):\n        return deque()\n\n    def add_response_callback(self, callback):\n        \"\"\"\n        Add a callback to the set of callbacks to be called by the\n        :term:`router` at a point after a :term:`response` object is\n        successfully created.  :app:`Pyramid` does not have a\n        global response object: this functionality allows an\n        application to register an action to be performed against the\n        response once one is created.\n\n        A 'callback' is a callable which accepts two positional\n        parameters: ``request`` and ``response``.  For example:\n\n        .. code-block:: python\n           :linenos:\n\n           def cache_callback(request, response):\n               'Set the cache_control max_age for the response'\n               response.cache_control.max_age = 360\n           request.add_response_callback(cache_callback)\n\n        Response callbacks are called in the order they're added\n        (first-to-most-recently-added).  No response callback is\n        called if an exception happens in application code, or if the\n        response object returned by :term:`view` code is invalid.\n\n        All response callbacks are called *after* the tweens and\n        *before* the :class:`pyramid.events.NewResponse` event is sent.\n\n        Errors raised by callbacks are not handled specially.  They\n        will be propagated to the caller of the :app:`Pyramid`\n        router application.\n\n        .. seealso::\n\n            See also :ref:`using_response_callbacks`.\n        \"\"\"\n\n        self.response_callbacks.append(callback)\n\n    def _process_response_callbacks(self, response):\n        callbacks = self.response_callbacks\n        while callbacks:\n            callback = callbacks.popleft()\n            callback(self, response)\n\n    def add_finished_callback(self, callback):\n        \"\"\"\n        Add a callback to the set of callbacks to be called\n        unconditionally by the :term:`router` at the very end of\n        request processing.\n\n        ``callback`` is a callable which accepts a single positional\n        parameter: ``request``.  For example:\n\n        .. code-block:: python\n           :linenos:\n\n           import transaction\n\n           def commit_callback(request):\n               '''commit or abort the transaction associated with request'''\n               if request.exception is not None:\n                   transaction.abort()\n               else:\n                   transaction.commit()\n           request.add_finished_callback(commit_callback)\n\n        Finished callbacks are called in the order they're added (\n        first- to most-recently- added).  Finished callbacks (unlike\n        response callbacks) are *always* called, even if an exception\n        happens in application code that prevents a response from\n        being generated.\n\n        The set of finished callbacks associated with a request are\n        called *very late* in the processing of that request; they are\n        essentially the last thing called by the :term:`router`. They\n        are called after response processing has already occurred in a\n        top-level ``finally:`` block within the router request\n        processing code.  As a result, mutations performed to the\n        ``request`` provided to a finished callback will have no\n        meaningful effect, because response processing will have\n        already occurred, and the request's scope will expire almost\n        immediately after all finished callbacks have been processed.\n\n        Errors raised by finished callbacks are not handled specially.\n        They will be propagated to the caller of the :app:`Pyramid`\n        router application.\n\n        .. seealso::\n\n            See also :ref:`using_finished_callbacks`.\n        \"\"\"\n        self.finished_callbacks.append(callback)\n\n\n\n# The code to be completed is:\n    def _process_finished_callbacks(self):\n\n        \"\"\"\n        This function processes the finished callbacks in a CallbackMethodsMixin instance. It retrieves the finished callbacks and executes them one by one, passing the input instance as an argument to each callback.\n        Input-Output Arguments\n        :param self: CallbackMethodsMixin. An instance of the CallbackMethodsMixin class.\n        :return: No return values.\n        \"\"\""}
{"namespace": "wal_e.log_help.WalELogger.fmt_logline", "prompt": "# Please complete the fmt_logline function based on the contexts above the function.\n\n# The contexts above the function are:\n\"\"\"\nA module to assist with using the Python logging module\n\n\"\"\"\nimport datetime\nimport errno\nimport logging\nimport os\n\nfrom logging import handlers\nfrom os import path\n\n\n# Global logging handlers created by configure.\nHANDLERS = []\n\n\nclass IndentFormatter(logging.Formatter):\n\n    def format(self, record, *args, **kwargs):\n        \"\"\"\n        Format a message in the log\n\n        Act like the normal format, but indent anything that is a\n        newline within the message.\n\n        \"\"\"\n        return logging.Formatter.format(\n            self, record, *args, **kwargs).replace('\\n', '\\n' + ' ' * 8)\n\n\ndef configure(*args, **kwargs):\n    \"\"\"\n    Configure logging.\n\n    Borrowed from logging.basicConfig\n\n    Uses the IndentFormatter instead of the regular Formatter\n\n    Also, opts the caller into Syslog output, unless syslog could not\n    be opened for some reason or another, in which case a warning will\n    be printed to the other log handlers.\n\n    \"\"\"\n    # Configuration must only happen once: no mechanism for avoiding\n    # duplication of handlers exists.\n    assert len(HANDLERS) == 0\n\n    log_destinations = get_log_destinations()\n\n    if 'stderr' in log_destinations:\n        # Add stderr output.\n        HANDLERS.append(logging.StreamHandler())\n\n    def terrible_log_output(s):\n        import sys\n\n        print(s, file=sys.stderr)\n\n    places = [\n        # Linux\n        '/dev/log',\n\n        # FreeBSD\n        '/var/run/log',\n\n        # Macintosh\n        '/var/run/syslog',\n    ]\n\n    default_syslog_address = places[0]\n    for p in places:\n        if path.exists(p):\n            default_syslog_address = p\n            break\n\n    syslog_address = kwargs.setdefault('syslog_address',\n                                       default_syslog_address)\n\n    valid_facility = False\n    if 'syslog' in log_destinations:\n        facility, valid_facility = get_syslog_facility()\n\n        if not valid_facility:\n            terrible_log_output('invalid syslog facility level specified')\n\n        try:\n            # Add syslog output.\n            HANDLERS.append(handlers.SysLogHandler(syslog_address,\n                                                           facility=facility))\n        except EnvironmentError as e:\n            if e.errno in [errno.EACCES, errno.ECONNREFUSED]:\n                message = ('wal-e: Could not set up syslog, '\n                           'continuing anyway.  '\n                           'Reason: {0}').format(errno.errorcode[e.errno])\n\n                terrible_log_output(message)\n\n    fs = kwargs.get(\"format\", logging.BASIC_FORMAT)\n    dfs = kwargs.get(\"datefmt\", None)\n    fmt = IndentFormatter(fs, dfs)\n\n    for handler in HANDLERS:\n        handler.setFormatter(fmt)\n        logging.root.addHandler(handler)\n\n    # Default to INFO level logging.\n    set_level(kwargs.get('level', logging.INFO))\n\n\ndef get_log_destinations():\n    \"\"\"Parse env string\"\"\"\n    # if env var is not set default to stderr + syslog\n    env = os.getenv('WALE_LOG_DESTINATION', 'stderr,syslog')\n    return env.split(\",\")\n\n\ndef get_syslog_facility():\n    \"\"\"Get syslog facility from ENV var\"\"\"\n    facil = os.getenv('WALE_SYSLOG_FACILITY', 'user')\n\n    valid_facility = True\n    try:\n        facility = handlers.SysLogHandler.facility_names[facil.lower()]\n    except KeyError:\n        valid_facility = False\n        facility = handlers.SysLogHandler.LOG_USER\n\n    return facility, valid_facility\n\n\ndef set_level(level):\n    \"\"\"Adjust the logging level of WAL-E\"\"\"\n    for handler in HANDLERS:\n        handler.setLevel(level)\n\n    logging.root.setLevel(level)\n\n\nclass WalELogger(object):\n    def __init__(self, *args, **kwargs):\n        self._logger = logging.getLogger(*args, **kwargs)\n\n    @staticmethod\n    def _fmt_structured(d):\n        \"\"\"Formats '{k1:v1, k2:v2}' => 'time=... pid=... k1=v1 k2=v2'\n\n        Output is lexically sorted, *except* the time and pid always\n        come first, to assist with human scanning of the data.\n        \"\"\"\n        timeEntry = datetime.datetime.utcnow().strftime(\n            \"time=%Y-%m-%dT%H:%M:%S.%f-00\")\n        pidEntry = \"pid=\" + str(os.getpid())\n\n        rest = sorted('='.join([str(k), str(v)])\n                      for (k, v) in list(d.items()))\n\n        return ' '.join([timeEntry, pidEntry] + rest)\n\n    @staticmethod\n\n\n# The code to be completed is:\n    def fmt_logline(msg, detail=None, hint=None, structured=None):\n\n        \"\"\"\n        Format a log line with the given message, detail, hint, and structured data. It creates a list to format these information, and the forst is \"['MSG: {message}', 'DETAIL: {detail}', 'HINT: {hint}', 'STRUCTURED: {structured data}']\". Then, it joins them with a newline character to obtain the log line.\n        Input-Output Arguments\n        :param msg: String. The main message to be included in the log line.\n        :param detail: String [optional]. Additional details to be included in the log line.\n        :param hint: String [optional]. A hint or suggestion related to the log message.\n        :param structured: Dictionary [optional]. Additional structured data to be included in the log line.\n        :return: String. The formatted log line.\n        \"\"\""}
{"namespace": "cupy_builder._command.filter_files_by_extension", "prompt": "# Please complete the filter_files_by_extension function based on the contexts above the function.\n\n# The contexts above the function are:\nimport json\nimport os\nimport os.path\nimport subprocess\nimport sys\nfrom typing import Any, Dict, List, Tuple\n\nimport setuptools\nimport setuptools.command.build_ext\n\nimport cupy_builder\nimport cupy_builder.install_build as build\nfrom cupy_builder._context import Context\nfrom cupy_builder._compiler import DeviceCompilerUnix, DeviceCompilerWin32\n\n\n\n\n# The code to be completed is:\ndef filter_files_by_extension(\n        sources: List[str],\n        extension: str,\n) -> Tuple[List[str], List[str]]:\n\n    \"\"\"\n    Filter the files in the given list based on the file extension. It separates the files with the given extension from the rest of the files.\n    Input-Output Arguments\n    :param sources: List of strings. The list of file paths to be filtered.\n    :param extension: String. The file extension to be used for filtering.\n    :return: Two lists of strings. The first list contains the files with the given extension, and the second list contains the rest of the files.\n    \"\"\""}
{"namespace": "twilio.twiml.voice_response.VoiceResponse.enqueue", "prompt": "# Please complete the enqueue function based on the contexts above the function.\n\n# The contexts above the function are:\n# coding=utf-8\nr\"\"\"\nThis code was generated by\n\\ / _    _  _|   _  _\n | (_)\\/(_)(_|\\/| |(/_  v1.0.0\n      /       /\n\"\"\"\n\nfrom twilio.twiml import (\n    TwiML,\n)\n\n\nclass VoiceResponse(TwiML):\n    \"\"\"<Response> TwiML for Voice\"\"\"\n\n    def __init__(self, **kwargs):\n        super(VoiceResponse, self).__init__(**kwargs)\n        self.name = \"Response\"\n\n    def connect(self, action=None, method=None, **kwargs):\n        \"\"\"\n        Create a <Connect> element\n\n        :param action: Action URL\n        :param method: Action URL method\n        :param kwargs: additional attributes\n\n        :returns: <Connect> element\n        \"\"\"\n        return self.nest(Connect(action=action, method=method, **kwargs))\n\n    def dial(\n        self,\n        number=None,\n        action=None,\n        method=None,\n        timeout=None,\n        hangup_on_star=None,\n        time_limit=None,\n        caller_id=None,\n        record=None,\n        trim=None,\n        recording_status_callback=None,\n        recording_status_callback_method=None,\n        recording_status_callback_event=None,\n        answer_on_bridge=None,\n        ring_tone=None,\n        recording_track=None,\n        sequential=None,\n        refer_url=None,\n        refer_method=None,\n        **kwargs\n    ):\n        \"\"\"\n        Create a <Dial> element\n\n        :param number: Phone number to dial\n        :param action: Action URL\n        :param method: Action URL method\n        :param timeout: Time to wait for answer\n        :param hangup_on_star: Hangup call on star press\n        :param time_limit: Max time length\n        :param caller_id: Caller ID to display\n        :param record: Record the call\n        :param trim: Trim the recording\n        :param recording_status_callback: Recording status callback URL\n        :param recording_status_callback_method: Recording status callback URL method\n        :param recording_status_callback_event: Recording status callback events\n        :param answer_on_bridge: Preserve the ringing behavior of the inbound call until the Dialed call picks up\n        :param ring_tone: Ringtone allows you to override the ringback tone that Twilio will play back to the caller while executing the Dial\n        :param recording_track: To indicate which audio track should be recorded\n        :param sequential: Used to determine if child TwiML nouns should be dialed in order, one after the other (sequential) or dial all at once (parallel). Default is false, parallel\n        :param refer_url: Webhook that will receive future SIP REFER requests\n        :param refer_method: The HTTP method to use for the refer Webhook\n        :param kwargs: additional attributes\n\n        :returns: <Dial> element\n        \"\"\"\n        return self.nest(\n            Dial(\n                number=number,\n                action=action,\n                method=method,\n                timeout=timeout,\n                hangup_on_star=hangup_on_star,\n                time_limit=time_limit,\n                caller_id=caller_id,\n                record=record,\n                trim=trim,\n                recording_status_callback=recording_status_callback,\n                recording_status_callback_method=recording_status_callback_method,\n                recording_status_callback_event=recording_status_callback_event,\n                answer_on_bridge=answer_on_bridge,\n                ring_tone=ring_tone,\n                recording_track=recording_track,\n                sequential=sequential,\n                refer_url=refer_url,\n                refer_method=refer_method,\n                **kwargs\n            )\n        )\n\n    def echo(self, **kwargs):\n        \"\"\"\n        Create a <Echo> element\n\n        :param kwargs: additional attributes\n\n        :returns: <Echo> element\n        \"\"\"\n        return self.nest(Echo(**kwargs))\n\n\n\n# The code to be completed is:\n    def enqueue(\n        self,\n        name=None,\n        action=None,\n        max_queue_size=None,\n        method=None,\n        wait_url=None,\n        wait_url_method=None,\n        workflow_sid=None,\n        **kwargs\n    ):\n\n        \"\"\"\n        This function creates an <Enqueue> element for a VoiceResponse object. It sets various attributes of the <Enqueue> element based on the input parameters.\n        Input-Output Arguments\n        :param self: VoiceResponse. An instance of the VoiceResponse class.\n        :param name: String. The friendly name of the <Enqueue> element.\n        :param action: String. The action URL of the <Enqueue> element.\n        :param max_queue_size: Integer. The maximum size of the queue for the <Enqueue> element.\n        :param method: String. The HTTP method to be used for the action URL.\n        :param wait_url: String. The wait URL for the <Enqueue> element.\n        :param wait_url_method: String. The HTTP method to be used for the wait URL.\n        :param workflow_sid: String. The TaskRouter Workflow SID for the <Enqueue> element.\n        :param kwargs: Additional attributes for the <Enqueue> element.\n        :return: <Enqueue> element. The created <Enqueue> element.\n        \"\"\""}
{"namespace": "csvkit.convert.fixed.fixed2csv", "prompt": "# Please complete the fixed2csv function based on the contexts above the function.\n\n# The contexts above the function are:\n#!/usr/bin/env python\n\nfrom codecs import iterdecode\nfrom collections import namedtuple\nfrom io import StringIO\n\nimport agate\n\n\n\n\n# The code to be completed is:\ndef fixed2csv(f, schema, output=None, skip_lines=0, **kwargs):\n\n    \"\"\"\n    This function converts a fixed-width file to a CSV file using a CSV-formatted schema description. It reads the fixed-width file, parses it based on the provided schema, and writes the parsed data to a CSV file. If an output file is not specified, the function returns the complete parsed data as a string.\n    Input-Output Arguments\n    :param f: File object. The fixed-width file to be converted to CSV.\n    :param schema: CSV-formatted schema description. A CSV file that specifies the column names, starting indices, and lengths of each column in the fixed-width file.\n    :param output: File object [optional]. The output CSV file where the parsed data will be written. If not specified, the parsed data will be returned as a string.\n    :param skip_lines: Integer [optional]. The number of lines to skip from the top of the fixed-width file.\n    :param kwargs: Additional keyword arguments [optional]. Additional arguments that can be passed to the function.\n    :return: String or None. If an output file is specified, the function returns None. If an output file is not specified, the function returns the complete parsed data as a string.\n    \"\"\""}
{"namespace": "parsel.utils.shorten", "prompt": "# Please complete the shorten function based on the contexts above the function.\n\n# The contexts above the function are:\nimport re\nfrom typing import Any, Iterable, Iterator, List, Match, Pattern, Union, cast\nfrom w3lib.html import replace_entities as w3lib_replace_entities\n\n\ndef flatten(x: Iterable[Any]) -> List[Any]:\n    \"\"\"flatten(sequence) -> list\n    Returns a single, flat list which contains all elements retrieved\n    from the sequence and all recursively contained sub-sequences\n    (iterables).\n    Examples:\n    >>> [1, 2, [3,4], (5,6)]\n    [1, 2, [3, 4], (5, 6)]\n    >>> flatten([[[1,2,3], (42,None)], [4,5], [6], 7, (8,9,10)])\n    [1, 2, 3, 42, None, 4, 5, 6, 7, 8, 9, 10]\n    >>> flatten([\"foo\", \"bar\"])\n    ['foo', 'bar']\n    >>> flatten([\"foo\", [\"baz\", 42], \"bar\"])\n    ['foo', 'baz', 42, 'bar']\n    \"\"\"\n    return list(iflatten(x))\n\n\ndef iflatten(x: Iterable[Any]) -> Iterator[Any]:\n    \"\"\"iflatten(sequence) -> Iterator\n    Similar to ``.flatten()``, but returns iterator instead\"\"\"\n    for el in x:\n        if _is_listlike(el):\n            yield from flatten(el)\n        else:\n            yield el\n\n\ndef _is_listlike(x: Any) -> bool:\n    \"\"\"\n    >>> _is_listlike(\"foo\")\n    False\n    >>> _is_listlike(5)\n    False\n    >>> _is_listlike(b\"foo\")\n    False\n    >>> _is_listlike([b\"foo\"])\n    True\n    >>> _is_listlike((b\"foo\",))\n    True\n    >>> _is_listlike({})\n    True\n    >>> _is_listlike(set())\n    True\n    >>> _is_listlike((x for x in range(3)))\n    True\n    >>> _is_listlike(range(5))\n    True\n    \"\"\"\n    return hasattr(x, \"__iter__\") and not isinstance(x, (str, bytes))\n\n\ndef extract_regex(\n    regex: Union[str, Pattern[str]], text: str, replace_entities: bool = True\n) -> List[str]:\n    \"\"\"Extract a list of strings from the given text/encoding using the following policies:\n    * if the regex contains a named group called \"extract\" that will be returned\n    * if the regex contains multiple numbered groups, all those will be returned (flattened)\n    * if the regex doesn't contain any group the entire regex matching is returned\n    \"\"\"\n    if isinstance(regex, str):\n        regex = re.compile(regex, re.UNICODE)\n\n    if \"extract\" in regex.groupindex:\n        # named group\n        try:\n            extracted = cast(Match[str], regex.search(text)).group(\"extract\")\n        except AttributeError:\n            strings = []\n        else:\n            strings = [extracted] if extracted is not None else []\n    else:\n        # full regex or numbered groups\n        strings = regex.findall(text)\n\n    strings = flatten(strings)\n    if not replace_entities:\n        return strings\n    return [w3lib_replace_entities(s, keep=[\"lt\", \"amp\"]) for s in strings]\n\n\n\n\n# The code to be completed is:\ndef shorten(text: str, width: int, suffix: str = \"...\") -> str:\n\n    \"\"\"\n    Shorten the given text to fit in the given width. If the length of the text is less than or equal to the width, the original text is returned. If the width is greater than the length of the suffix, the text is truncated to fit the width and the suffix is added. If the width is greater than or equal to 0, the suffix is returned based on the width. Otherwise, a ValueError(\"width must be equal or greater than 0\") is raised.\n    Input-Output Arguments\n    :param text: String. The input text to be shortened.\n    :param width: Integer. The width to which the text should be shortened.\n    :param suffix: String. The suffix to be added at the end of the shortened text. Defaults to \"...\".\n    :return: String. The shortened text.\n    \"\"\""}
{"namespace": "praw.util.token_manager.SQLiteTokenManager.pre_refresh_callback", "prompt": "# Please complete the pre_refresh_callback function based on the contexts above the function.\n\n# The contexts above the function are:\n\"\"\"Token Manager classes.\n\nThere should be a 1-to-1 mapping between an instance of a subclass of\n:class:`.BaseTokenManager` and a :class:`.Reddit` instance.\n\nA few proof of concept token manager classes are provided here, but it is expected that\nPRAW users will create their own token manager classes suitable for their needs.\n\n.. deprecated:: 7.4.0\n\n    Tokens managers have been deprecated and will be removed in the near future.\n\n\"\"\"\nfrom abc import ABC, abstractmethod\n\nfrom . import _deprecate_args\n\n\nclass BaseTokenManager(ABC):\n    \"\"\"An abstract class for all token managers.\"\"\"\n\n    @abstractmethod\n    def post_refresh_callback(self, authorizer):\n        \"\"\"Handle callback that is invoked after a refresh token is used.\n\n        :param authorizer: The ``prawcore.Authorizer`` instance used containing\n            ``access_token`` and ``refresh_token`` attributes.\n\n        This function will be called after refreshing the access and refresh tokens.\n        This callback can be used for saving the updated ``refresh_token``.\n\n        \"\"\"\n\n    @abstractmethod\n    def pre_refresh_callback(self, authorizer):\n        \"\"\"Handle callback that is invoked before refreshing PRAW's authorization.\n\n        :param authorizer: The ``prawcore.Authorizer`` instance used containing\n            ``access_token`` and ``refresh_token`` attributes.\n\n        This callback can be used to inspect and modify the attributes of the\n        ``prawcore.Authorizer`` instance, such as setting the ``refresh_token``.\n\n        \"\"\"\n\n    @property\n    def reddit(self):\n        \"\"\"Return the :class:`.Reddit` instance bound to the token manager.\"\"\"\n        return self._reddit\n\n    @reddit.setter\n    def reddit(self, value):\n        if self._reddit is not None:\n            raise RuntimeError(\n                \"'reddit' can only be set once and is done automatically\"\n            )\n        self._reddit = value\n\n    def __init__(self):\n        \"\"\"Initialize a :class:`.BaseTokenManager` instance.\"\"\"\n        self._reddit = None\n\n\nclass FileTokenManager(BaseTokenManager):\n    \"\"\"Provides a single-file based token manager.\n\n    It is expected that the file with the initial ``refresh_token`` is created prior to\n    use.\n\n    .. warning::\n\n        The same ``file`` should not be used by more than one instance of this class\n        concurrently. Doing so may result in data corruption. Consider using\n        :class:`.SQLiteTokenManager` if you want more than one instance of PRAW to\n        concurrently manage a specific ``refresh_token`` chain.\n\n    \"\"\"\n\n    def __init__(self, filename):\n        \"\"\"Initialize a :class:`.FileTokenManager` instance.\n\n        :param filename: The file the contains the refresh token.\n\n        \"\"\"\n        super().__init__()\n        self._filename = filename\n\n    def post_refresh_callback(self, authorizer):\n        \"\"\"Update the saved copy of the refresh token.\"\"\"\n        with open(self._filename, \"w\") as fp:\n            fp.write(authorizer.refresh_token)\n\n    def pre_refresh_callback(self, authorizer):\n        \"\"\"Load the refresh token from the file.\"\"\"\n        if authorizer.refresh_token is None:\n            with open(self._filename) as fp:\n                authorizer.refresh_token = fp.read().strip()\n\n\nclass SQLiteTokenManager(BaseTokenManager):\n    \"\"\"Provides a SQLite3 based token manager.\n\n    Unlike, :class:`.FileTokenManager`, the initial database need not be created ahead\n    of time, as it'll automatically be created on first use. However, initial refresh\n    tokens will need to be registered via :meth:`.register` prior to use.\n\n    .. warning::\n\n        This class is untested on Windows because we encountered file locking issues in\n        the test environment.\n\n    \"\"\"\n\n    @_deprecate_args(\"database\", \"key\")\n    def __init__(self, *, database, key):\n        \"\"\"Initialize a :class:`.SQLiteTokenManager` instance.\n\n        :param database: The path to the SQLite database.\n        :param key: The key used to locate the refresh token. This ``key`` can be\n            anything. You might use the ``client_id`` if you expect to have unique a\n            refresh token for each ``client_id``, or you might use a redditor's\n            ``username`` if you're managing multiple users' authentications.\n\n        \"\"\"\n        super().__init__()\n        import sqlite3\n\n        self._connection = sqlite3.connect(database)\n        self._connection.execute(\n            \"CREATE TABLE IF NOT EXISTS tokens (id, refresh_token, updated_at)\"\n        )\n        self._connection.execute(\n            \"CREATE UNIQUE INDEX IF NOT EXISTS ux_tokens_id on tokens(id)\"\n        )\n        self._connection.commit()\n        self.key = key\n\n    def _get(self):\n        cursor = self._connection.execute(\n            \"SELECT refresh_token FROM tokens WHERE id=?\", (self.key,)\n        )\n        result = cursor.fetchone()\n        if result is None:\n            raise KeyError\n        return result[0]\n\n    def _set(self, refresh_token):\n        \"\"\"Set the refresh token in the database.\n\n        This function will overwrite an existing value if the corresponding ``key``\n        already exists.\n\n        \"\"\"\n        self._connection.execute(\n            \"REPLACE INTO tokens VALUES (?, ?, datetime('now'))\",\n            (self.key, refresh_token),\n        )\n        self._connection.commit()\n\n    def is_registered(self):\n        \"\"\"Return whether or not ``key`` already has a ``refresh_token``.\"\"\"\n        cursor = self._connection.execute(\n            \"SELECT refresh_token FROM tokens WHERE id=?\", (self.key,)\n        )\n        return cursor.fetchone() is not None\n\n    def post_refresh_callback(self, authorizer):\n        \"\"\"Update the refresh token in the database.\"\"\"\n        self._set(authorizer.refresh_token)\n\n        # While the following line is not strictly necessary, it ensures that the\n        # refresh token is not used elsewhere. And also forces the pre_refresh_callback\n        # to always load the latest refresh_token from the database.\n        authorizer.refresh_token = None\n\n\n\n# The code to be completed is:\n    def pre_refresh_callback(self, authorizer):\n\n        \"\"\"\n        This function is a callback method that is called before refreshing the token. It loads the refresh token from the database.\n        Input-Output Arguments\n        :param self: SQLiteTokenManager. An instance of the SQLiteTokenManager class.\n        :param authorizer: The authorizer object that contains the refresh token attribute.\n        :return: None.\n        \"\"\""}
{"namespace": "twilio.base.serialize.iso8601_datetime", "prompt": "# Please complete the iso8601_datetime function based on the contexts above the function.\n\n# The contexts above the function are:\nimport datetime\nimport json\n\nfrom twilio.base import values\n\n\ndef iso8601_date(d):\n    \"\"\"\n    Return a string representation of a date that the Twilio API understands\n    Format is YYYY-MM-DD. Returns None if d is not a string, datetime, or date\n    \"\"\"\n    if d == values.unset:\n        return d\n    elif isinstance(d, datetime.datetime):\n        return str(d.date())\n    elif isinstance(d, datetime.date):\n        return str(d)\n    elif isinstance(d, str):\n        return d\n\n\n\n\n# The code to be completed is:\ndef iso8601_datetime(d):\n\n    \"\"\"\n    This function takes a date object and returns a string representation of the date in the format \"%Y-%m-%dT%H:%M:%SZ\" that is compatible with the Twilio API. If the input is not a string, datetime, or date object, it returns None.\n    Input-Output Arguments\n    :param d: The date object to be converted to a string representation.\n    :return: String. The string representation of the date in the format \"YYYY-MM-DD\" or None if the input is not a valid date object.\n    \"\"\""}
{"namespace": "faker.utils.decorators.slugify", "prompt": "# Please complete the slugify function based on the contexts above the function.\n\n# The contexts above the function are:\nfrom functools import wraps\nfrom typing import Callable, Dict, Tuple, TypeVar\n\nfrom ..utils import text\n\nT = TypeVar(\"T\")\n\n\n\n\n# The code to be completed is:\ndef slugify(fn: Callable) -> Callable:\n    @wraps(fn)\n\n    \"\"\"\n    This function is a decorator that takes a function and returns a new function. The new function calls the original function and then slugifies the result.\n    Input-Output Arguments\n    :param fn: Callable. The original function to be decorated.\n    :return: Callable. The decorated function.\n    \"\"\""}
{"namespace": "playhouse.db_url.parse", "prompt": "# Please complete the parse function based on the contexts above the function.\n\n# The contexts above the function are:\ntry:\n    from urlparse import parse_qsl, unquote, urlparse\nexcept ImportError:\n    from urllib.parse import parse_qsl, unquote, urlparse\n\nfrom peewee import *\nfrom playhouse.cockroachdb import CockroachDatabase\nfrom playhouse.cockroachdb import PooledCockroachDatabase\nfrom playhouse.pool import PooledMySQLDatabase\nfrom playhouse.pool import PooledPostgresqlDatabase\nfrom playhouse.pool import PooledSqliteDatabase\nfrom playhouse.pool import PooledSqliteExtDatabase\nfrom playhouse.sqlite_ext import SqliteExtDatabase\n\n\nschemes = {\n    'cockroachdb': CockroachDatabase,\n    'cockroachdb+pool': PooledCockroachDatabase,\n    'crdb': CockroachDatabase,\n    'crdb+pool': PooledCockroachDatabase,\n    'mysql': MySQLDatabase,\n    'mysql+pool': PooledMySQLDatabase,\n    'postgres': PostgresqlDatabase,\n    'postgresql': PostgresqlDatabase,\n    'postgres+pool': PooledPostgresqlDatabase,\n    'postgresql+pool': PooledPostgresqlDatabase,\n    'sqlite': SqliteDatabase,\n    'sqliteext': SqliteExtDatabase,\n    'sqlite+pool': PooledSqliteDatabase,\n    'sqliteext+pool': PooledSqliteExtDatabase,\n}\n\ndef register_database(db_class, *names):\n    global schemes\n    for name in names:\n        schemes[name] = db_class\n\ndef parseresult_to_dict(parsed, unquote_password=False):\n\n    # urlparse in python 2.6 is broken so query will be empty and instead\n    # appended to path complete with '?'\n    path_parts = parsed.path[1:].split('?')\n    try:\n        query = path_parts[1]\n    except IndexError:\n        query = parsed.query\n\n    connect_kwargs = {'database': path_parts[0]}\n    if parsed.username:\n        connect_kwargs['user'] = parsed.username\n    if parsed.password:\n        connect_kwargs['password'] = parsed.password\n        if unquote_password:\n            connect_kwargs['password'] = unquote(connect_kwargs['password'])\n    if parsed.hostname:\n        connect_kwargs['host'] = parsed.hostname\n    if parsed.port:\n        connect_kwargs['port'] = parsed.port\n\n    # Adjust parameters for MySQL.\n    if parsed.scheme == 'mysql' and 'password' in connect_kwargs:\n        connect_kwargs['passwd'] = connect_kwargs.pop('password')\n    elif 'sqlite' in parsed.scheme and not connect_kwargs['database']:\n        connect_kwargs['database'] = ':memory:'\n\n    # Get additional connection args from the query string\n    qs_args = parse_qsl(query, keep_blank_values=True)\n    for key, value in qs_args:\n        if value.lower() == 'false':\n            value = False\n        elif value.lower() == 'true':\n            value = True\n        elif value.isdigit():\n            value = int(value)\n        elif '.' in value and all(p.isdigit() for p in value.split('.', 1)):\n            try:\n                value = float(value)\n            except ValueError:\n                pass\n        elif value.lower() in ('null', 'none'):\n            value = None\n\n        connect_kwargs[key] = value\n\n    return connect_kwargs\n\n\n\n# The code to be completed is:\ndef parse(url, unquote_password=False):\n\n    \"\"\"\n    This function takes a URL as input and parses it. It then convert the parsed result into a dictionary using the parsed result and unquote password which determines whether the password in the URL should be unquoted or not.\n    Input-Output Arguments\n    :param url: String. The URL to be parsed.\n    :param unquote_password: Bool. Whether to unquote the password in the URL. Defaults to False.\n    :return: Dictionary. The parsed URL as a dictionary.\n    \"\"\""}
{"namespace": "bentoml._internal.types.LazyType.get_class", "prompt": "# Please complete the get_class function based on the contexts above the function.\n\n# The contexts above the function are:\nfrom __future__ import annotations\n\nimport io\nimport logging\nimport os\nimport sys\nimport typing as t\nfrom dataclasses import dataclass\nfrom datetime import date\nfrom datetime import datetime\nfrom datetime import time\nfrom datetime import timedelta\nfrom types import TracebackType\nfrom typing import get_args\nfrom typing import get_origin\n\n__all__ = [\n    \"MetadataType\",\n    \"MetadataDict\",\n    \"JSONSerializable\",\n    \"LazyType\",\n    \"is_compatible_type\",\n    \"FileLike\",\n]\n\nlogger = logging.getLogger(__name__)\n\nBATCH_HEADER = \"Bentoml-Is-Batch-Request\"\n\n# For non latin1 characters: https://tools.ietf.org/html/rfc8187\n# Also https://github.com/benoitc/gunicorn/issues/1778\nHEADER_CHARSET = \"latin1\"\n\nJSON_CHARSET = \"utf-8\"\n\nMetadataType: t.TypeAlias = t.Union[\n    str,\n    bytes,\n    bool,\n    int,\n    float,\n    complex,\n    datetime,\n    date,\n    time,\n    timedelta,\n    t.List[\"MetadataType\"],\n    t.Tuple[\"MetadataType\"],\n    t.Dict[str, \"MetadataType\"],\n]\n\n\nclass ModelSignatureDict(t.TypedDict, total=False):\n    batchable: bool\n    batch_dim: t.Union[t.Tuple[int, int], int]\n    input_spec: t.Optional[t.Union[t.Tuple[AnyType], AnyType]]\n    output_spec: t.Optional[AnyType]\n\n\nif t.TYPE_CHECKING:\n    PathType: t.TypeAlias = str | os.PathLike[str]\n    JSONSerializable: t.TypeAlias = (\n        str\n        | int\n        | float\n        | bool\n        | None\n        | list[\"JSONSerializable\"]\n        | dict[str, \"JSONSerializable\"]\n    )\n    MetadataDict = t.Dict[str, MetadataType]\nelse:\n    PathType = t.Union[str, os.PathLike]\n    JSONSerializable = t.NewType(\"JSONSerializable\", object)\n    # NOTE: remove this when registering hook for MetadataType\n    MetadataDict = dict\n\nLifecycleHook = t.Callable[[], t.Union[None, t.Coroutine[t.Any, t.Any, None]]]\n\nT = t.TypeVar(\"T\")\n\n\nclass LazyType(t.Generic[T]):\n    \"\"\"\n    LazyType provides solutions for several conflicts when applying lazy dependencies,\n        type annotations and runtime class checking.\n    It works both for runtime and type checking phases.\n\n    * conflicts 1\n\n    isinstance(obj, class) requires importing the class first, which breaks\n    lazy dependencies\n\n    solution:\n    >>> LazyType(\"numpy.ndarray\").isinstance(obj)\n\n    * conflicts 2\n\n    `isinstance(obj, str)` will narrow obj types down to str. But it only works for the\n    case that the class is the type at the same time. For numpy.ndarray which the type\n    is actually numpy.typing.NDArray, we had to hack the type checking.\n\n    solution:\n    >>> if TYPE_CHECKING:\n    >>>     from numpy.typing import NDArray\n    >>> LazyType[\"NDArray\"](\"numpy.ndarray\").isinstance(obj)`\n    >>> #  this will narrow the obj to NDArray with PEP-647\n\n    * conflicts 3\n\n    compare/refer/map classes before importing them.\n\n    >>> HANDLER_MAP = {\n    >>>     LazyType(\"numpy.ndarray\"): ndarray_handler,\n    >>>     LazyType(\"pandas.DataFrame\"): pdframe_handler,\n    >>> }\n    >>>\n    >>> HANDLER_MAP[LazyType(numpy.ndarray)]](array)\n    >>> LazyType(\"numpy.ndarray\") == numpy.ndarray\n    \"\"\"\n\n    @t.overload\n    def __init__(self, module_or_cls: str, qualname: str) -> None:\n        \"\"\"LazyType(\"numpy\", \"ndarray\")\"\"\"\n\n    @t.overload\n    def __init__(self, module_or_cls: t.Type[T]) -> None:\n        \"\"\"LazyType(numpy.ndarray)\"\"\"\n\n    @t.overload\n    def __init__(self, module_or_cls: str) -> None:\n        \"\"\"LazyType(\"numpy.ndarray\")\"\"\"\n\n    def __init__(\n        self,\n        module_or_cls: str | t.Type[T],\n        qualname: str | None = None,\n    ) -> None:\n        if isinstance(module_or_cls, str):\n            if qualname is None:  # LazyType(\"numpy.ndarray\")\n                parts = module_or_cls.rsplit(\".\", 1)\n                if len(parts) == 1:\n                    raise ValueError(\"LazyType only works with classes\")\n                self.module, self.qualname = parts\n            else:  # LazyType(\"numpy\", \"ndarray\")\n                self.module = module_or_cls\n                self.qualname = qualname\n            self._runtime_class = None\n        else:  # LazyType(numpy.ndarray)\n            self._runtime_class = module_or_cls\n            self.module = module_or_cls.__module__\n            if hasattr(module_or_cls, \"__qualname__\"):\n                self.qualname: str = getattr(module_or_cls, \"__qualname__\")\n            else:\n                self.qualname: str = getattr(module_or_cls, \"__name__\")\n\n    def __instancecheck__(self, obj: object) -> t.TypeGuard[T]:\n        return self.isinstance(obj)\n\n    @classmethod\n    def from_type(cls, typ_: t.Union[LazyType[T], t.Type[T]]) -> LazyType[T]:\n        if isinstance(typ_, LazyType):\n            return typ_\n        return cls(typ_)\n\n    def __eq__(self, o: object) -> bool:\n        \"\"\"\n        LazyType(\"numpy\", \"ndarray\") == np.ndarray\n        \"\"\"\n        if isinstance(o, type):\n            o = self.__class__(o)\n\n        if isinstance(o, LazyType):\n            return self.module == o.module and self.qualname == o.qualname\n\n        return False\n\n    def __hash__(self) -> int:\n        return hash(f\"{self.module}.{self.qualname}\")\n\n    def __repr__(self) -> str:\n        return f'LazyType(\"{self.module}\", \"{self.qualname}\")'\n\n\n\n# The code to be completed is:\n    def get_class(self, import_module: bool = True) -> t.Type[T]:\n\n        \"\"\"\n        This function returns the class object based on the given module and qualname. If the runtime class object is not available, it tries to import the module and retrieve the class object. It caches the runtime class object for future use.\n        Input-Output Arguments\n        :param self: LazyType. An instance of the LazyType class.\n        :param import_module: Bool. Whether to import the module if it is not already imported. Defaults to True.\n        :return: Type[T]. The class object corresponding to the module and qualname.\n        \"\"\""}
{"namespace": "falcon.inspect.inspect_static_routes", "prompt": "# Please complete the inspect_static_routes function based on the contexts above the function.\n\n# The contexts above the function are:\n# Copyright 2020 by Federico Caselli\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Inspect utilities for falcon applications.\"\"\"\nfrom functools import partial\nimport inspect\nfrom typing import Callable\nfrom typing import Dict\nfrom typing import List\nfrom typing import Optional\nfrom typing import Type\n\n\nfrom falcon.app import App\nfrom falcon.routing import CompiledRouter\n\n\ndef inspect_app(app: App) -> 'AppInfo':\n    \"\"\"Inspects an application.\n\n    Args:\n        app (falcon.App): The application to inspect. Works with both\n            :class:`falcon.App` and :class:`falcon.asgi.App`.\n\n    Returns:\n        AppInfo: The information regarding the application. Call\n        :meth:`~.AppInfo.to_string` on the result to obtain a human-friendly\n        representation.\n    \"\"\"\n    routes = inspect_routes(app)\n    static = inspect_static_routes(app)\n    sinks = inspect_sinks(app)\n    error_handlers = inspect_error_handlers(app)\n    middleware = inspect_middleware(app)\n    return AppInfo(routes, middleware, static, sinks, error_handlers, app._ASGI)\n\n\ndef inspect_routes(app: App) -> 'List[RouteInfo]':\n    \"\"\"Inspects the routes of an application.\n\n    Args:\n        app (falcon.App): The application to inspect. Works with both\n            :class:`falcon.App` and :class:`falcon.asgi.App`.\n\n    Returns:\n        List[RouteInfo]: A list of route descriptions for the application.\n    \"\"\"\n    router = app._router\n\n    inspect_function = _supported_routers.get(type(router))\n    if inspect_function is None:\n        raise TypeError(\n            'Unsupported router class {}. Use \"register_router\" '\n            'to register a function that can inspect the router '\n            'used by the provided application'.format(type(router))\n        )\n    return inspect_function(router)\n\n\ndef register_router(router_class):\n    \"\"\"Register a function to inspect a particular router.\n\n    This decorator registers a new function for a custom router\n    class, so that it can be inspected with the function\n    :func:`.inspect_routes`.\n    An inspection function takes the router instance used by the\n    application and returns a list of :class:`.RouteInfo`. Eg::\n\n        @register_router(MyRouterClass)\n        def inspect_my_router(router):\n            return [RouteInfo('foo', 'bar', '/path/to/foo.py:42', [])]\n\n    Args:\n        router_class (Type): The router class to register. If\n            already registered an error will be raised.\n    \"\"\"\n\n    def wraps(fn):\n        if router_class in _supported_routers:\n            raise ValueError(\n                'Another function is already registered'\n                ' for the router {}'.format(router_class)\n            )\n        _supported_routers[router_class] = fn\n        return fn\n\n    return wraps\n\n\n# router inspection registry\n_supported_routers = {}  # type: Dict[Type, Callable]\n\n\n\n\n# The code to be completed is:\ndef inspect_static_routes(app: App) -> 'List[StaticRouteInfo]':\n\n    \"\"\"\n    This function inspects the static routes of an application. It retrieves the information about the static routes that have been added to the application.\n    Input-Output Arguments\n    :param app: falcon.App. The application to inspect. It can be an instance of either falcon.App or falcon.asgi.App.\n    :return: List[StaticRouteInfo]. A list of StaticRouteInfo objects that represent the static routes added to the application.\n    \"\"\""}
{"namespace": "chatette.utils.Singleton.reset_instance", "prompt": "# Please complete the reset_instance function based on the contexts above the function.\n\n# The contexts above the function are:\n#!/usr/bin/env python3\n# coding: utf-8\n\"\"\"\nModule `chatette.utils`\nContains utility functions and classes used everywhere in the project.\n\"\"\"\n\nfrom __future__ import print_function\nimport sys\nfrom random import sample, choice\nfrom copy import deepcopy\n\nfrom string import ascii_letters\n\nfrom enum import Enum\n\n\nclass UnitType(Enum):\n    alias = \"alias\"\n    slot = \"slot\"\n    intent = \"intent\"\n\n\nclass Singleton(object):\n    \"\"\"\n    The base class for all singleton objects.\n    Every class that subclasses this class will have the behavior\n    of a singleton: their constructor will always return the same instance.\n    @pre: In order to work, a sub-class needs to have an `_instance` class\n          variable.\n    \"\"\"\n    _instance = None\n    def __new__(cls, *args, **kwargs):\n        if cls._instance is None:\n            cls._instance = super(Singleton, cls).__new__(cls)\n        return cls._instance\n\n    @classmethod\n    def get_or_create(cls, *args, **kwargs):\n        if cls._instance is None:\n            cls._instance = cls(*args, **kwargs)\n        return cls._instance\n\n    @classmethod\n\n\n# The code to be completed is:\n    def reset_instance(cls, *args, **kwargs):\n\n        \"\"\"\n        This function completely resets the instance of the Singleton class, creates a new instance with the given arguments, and returns the new instance.\n        Input-Output Arguments\n        :param cls: The Singleton class.\n        :param *args: Variable length argument list. The arguments to be passed to the new instance of the class.\n        :param **kwargs: Arbitrary keyword arguments. The keyword arguments to be passed to the new instance of the class.\n        :return: The new instance of the Singleton class.\n        \"\"\""}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.laplace_smooth.laplace_smooth_value_counts", "prompt": "# Please complete the laplace_smooth_value_counts function based on the contexts above the function.\n\n# The contexts above the function are:\n# -------------------------------------------------------------------------\n# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT License. See License.txt in the project root for\n# license information.\n# --------------------------------------------------------------------------\n\"\"\"Helper module for laplace smoothing counts.\"\"\"\n\nimport copy\nfrom typing import DefaultDict, List, Tuple\n\n\ndef laplace_smooth_cmd_counts(\n    seq1_counts: DefaultDict[str, int],\n    seq2_counts: DefaultDict[str, DefaultDict[str, int]],\n    start_token: str,\n    end_token: str,\n    unk_token: str,\n) -> Tuple[DefaultDict[str, int], DefaultDict[str, DefaultDict[str, int]]]:\n    \"\"\"\n    Apply laplace smoothing to the input counts for the cmds.\n\n    In particular, add 1 to each of the counts, including the unk_token. By including the\n    unk_token, we can handle unseen commands.\n\n    Parameters\n    ----------\n    seq1_counts: DefaultDict[str, int]\n        individual command counts\n    seq2_counts: DefaultDict[str, DefaultDict[str, int]]\n        sequence command (length 2) counts\n    start_token: str\n        dummy command to signify the start of a session (e.g. \"##START##\")\n    end_token: str\n        dummy command to signify the end of a session (e.g. \"##END##\")\n    unk_token: str\n        dummy command to signify an unseen command (e.g. \"##UNK##\")\n\n    Returns\n    -------\n    tuple of laplace smoothed counts:\n        individual command counts,\n        sequence command (length 2) counts\n\n    \"\"\"\n    seq1_counts_ls = copy.deepcopy(seq1_counts)\n    seq2_counts_ls = copy.deepcopy(seq2_counts)\n\n    cmds: List[str] = list(seq1_counts_ls.keys()) + [unk_token]\n    for cmd1 in cmds:\n        for cmd2 in cmds:\n            if cmd1 != end_token and cmd2 != start_token:\n                seq1_counts_ls[cmd1] += 1\n                seq2_counts_ls[cmd1][cmd2] += 1\n                seq1_counts_ls[cmd2] += 1\n\n    return seq1_counts_ls, seq2_counts_ls\n\n\ndef laplace_smooth_param_counts(\n    cmds: List[str],\n    param_counts: DefaultDict[str, int],\n    cmd_param_counts: DefaultDict[str, DefaultDict[str, int]],\n    unk_token: str,\n) -> Tuple[DefaultDict[str, int], DefaultDict[str, DefaultDict[str, int]]]:\n    \"\"\"\n    Apply laplace smoothing to the input counts for the params.\n\n    In particular, add 1 to each of the counts, including the unk_token. By including the\n    unk_token, we can handle unseen params.\n\n    Parameters\n    ----------\n    cmds: List[str]\n        list of all the possible commands (including the unk_token)\n    param_counts: DefaultDict[str, int]\n        individual param counts\n    cmd_param_counts: DefaultDict[str, DefaultDict[str, int]]\n        param conditional on command counts\n    unk_token: str\n        dummy command to signify an unseen command (e.g. \"##UNK##\")\n\n    Returns\n    -------\n    Tuple:\n        individual param probabilities,\n        param conditional on command probabilities\n\n    \"\"\"\n    param_counts_ls = copy.deepcopy(param_counts)\n    cmd_param_counts_ls = copy.deepcopy(cmd_param_counts)\n\n    params: List[str] = list(param_counts.keys()) + [unk_token]\n    for cmd in cmds:\n        for param in params:\n            if param in cmd_param_counts_ls[cmd] or param == unk_token:\n                param_counts_ls[param] += 1\n                cmd_param_counts_ls[cmd][param] += 1\n\n    return param_counts_ls, cmd_param_counts_ls\n\n\n\n\n# The code to be completed is:\ndef laplace_smooth_value_counts(\n    params: List[str],\n    value_counts: DefaultDict[str, int],\n    param_value_counts: DefaultDict[str, DefaultDict[str, int]],\n    unk_token: str,\n) -> Tuple[DefaultDict[str, int], DefaultDict[str, DefaultDict[str, int]]]:\n\n    \"\"\"\n    Apply laplace smoothing to the input counts for the values. It adds 1 to each of the counts, including the unk_token. By including the unk_token, it can handle unseen values. It smooths individual value counts (value_counts) and value conditional on parameter counts (param_value_counts).\n    Input-Output Arguments\n    :param params: List of string. List of all possible params, including the unk_token.\n    :param value_counts: DefaultDict of string and integer. Individual value counts.\n    :param param_value_counts: DefaultDict of string and DefaultDict of string and integer. Value conditional on param counts.\n    :param unk_token: String. Dummy command to signify an unseen command (e.g. \"##UNK##\").\n    :return: Tuple of DefaultDict of string and integer, DefaultDict of string and DefaultDict of string and integer. Individual value probabilities, value conditional on param probabilities.\n    \"\"\""}
{"namespace": "pytube.extract.is_private", "prompt": "# Please complete the is_private function based on the contexts above the function.\n\n# The contexts above the function are:\n\"\"\"This module contains all non-cipher related data extraction logic.\"\"\"\nimport logging\nimport urllib.parse\nimport re\nfrom collections import OrderedDict\nfrom datetime import datetime\nfrom typing import Any, Dict, List, Optional, Tuple\nfrom urllib.parse import parse_qs, quote, urlencode, urlparse\n\nfrom pytube.cipher import Cipher\nfrom pytube.exceptions import HTMLParseError, LiveStreamError, RegexMatchError\nfrom pytube.helpers import regex_search\nfrom pytube.metadata import YouTubeMetadata\nfrom pytube.parser import parse_for_object, parse_for_all_objects\n\n\nlogger = logging.getLogger(__name__)\n\n\ndef publish_date(watch_html: str):\n    \"\"\"Extract publish date\n    :param str watch_html:\n        The html contents of the watch page.\n    :rtype: str\n    :returns:\n        Publish date of the video.\n    \"\"\"\n    try:\n        result = regex_search(\n            r\"(?<=itemprop=\\\"datePublished\\\" content=\\\")\\d{4}-\\d{2}-\\d{2}\",\n            watch_html, group=0\n        )\n    except RegexMatchError:\n        return None\n    return datetime.strptime(result, '%Y-%m-%d')\n\n\ndef recording_available(watch_html):\n    \"\"\"Check if live stream recording is available.\n\n    :param str watch_html:\n        The html contents of the watch page.\n    :rtype: bool\n    :returns:\n        Whether or not the content is private.\n    \"\"\"\n    unavailable_strings = [\n        'This live stream recording is not available.'\n    ]\n    for string in unavailable_strings:\n        if string in watch_html:\n            return False\n    return True\n\n\n\n\n# The code to be completed is:\ndef is_private(watch_html):\n\n    \"\"\"\n    This function checks if the content is private by searching for specific strings in the HTML content of the watch page.\n    Input-Output Arguments\n    :param watch_html: str. The html contents of the watch page.\n    :return: bool. Whether or not the content is private.\n    \"\"\""}
{"namespace": "pythonforandroid.archs.Arch.target", "prompt": "# Please complete the target function based on the contexts above the function.\n\n# The contexts above the function are:\nfrom distutils.spawn import find_executable\nfrom os import environ\nfrom os.path import join\nfrom multiprocessing import cpu_count\n\nfrom pythonforandroid.recipe import Recipe\nfrom pythonforandroid.util import BuildInterruptingException, build_platform\n\n\nclass Arch:\n\n    command_prefix = None\n    '''The prefix for NDK commands such as gcc.'''\n\n    arch = \"\"\n    '''Name of the arch such as: `armeabi-v7a`, `arm64-v8a`, `x86`...'''\n\n    arch_cflags = []\n    '''Specific arch `cflags`, expect to be overwrote in subclass if needed.'''\n\n    common_cflags = [\n        '-target {target}',\n        '-fomit-frame-pointer'\n    ]\n\n    common_cppflags = [\n        '-DANDROID',\n        '-I{ctx.ndk.sysroot_include_dir}',\n        '-I{python_includes}',\n    ]\n\n    common_ldflags = ['-L{ctx_libs_dir}']\n\n    common_ldlibs = ['-lm']\n\n    common_ldshared = [\n        '-pthread',\n        '-shared',\n        '-Wl,-O1',\n        '-Wl,-Bsymbolic-functions',\n    ]\n\n    def __init__(self, ctx):\n        self.ctx = ctx\n\n        # Allows injecting additional linker paths used by any recipe.\n        # This can also be modified by recipes (like the librt recipe)\n        # to make sure that some sort of global resource is available &\n        # linked for all others.\n        self.extra_global_link_paths = []\n\n    def __str__(self):\n        return self.arch\n\n    @property\n    def ndk_lib_dir(self):\n        return join(self.ctx.ndk.sysroot_lib_dir, self.command_prefix)\n\n    @property\n    def ndk_lib_dir_versioned(self):\n        return join(self.ndk_lib_dir, str(self.ctx.ndk_api))\n\n    @property\n    def include_dirs(self):\n        return [\n            \"{}/{}\".format(\n                self.ctx.include_dir,\n                d.format(arch=self))\n            for d in self.ctx.include_dirs]\n\n    @property\n\n\n# The code to be completed is:\n    def target(self):\n        # As of NDK r19, the toolchains installed by default with the\n        # NDK may be used in-place. The make_standalone_toolchain.py script\n        # is no longer needed for interfacing with arbitrary build systems.\n        # See: https://developer.android.com/ndk/guides/other_build_systems\n\n        \"\"\"\n        This function returns the target architecture for the Android NDK build. It combines the command prefix and the NDK API version to form the target architecture string.\n        Input-Output Arguments\n        :param self: Arch. An instance of the Arch class.\n        :return: String. The target architecture string.\n        \"\"\""}
{"namespace": "rows.fields.BoolField.deserialize", "prompt": "# Please complete the deserialize function based on the contexts above the function.\n\n# The contexts above the function are:\n# coding: utf-8\n\n# Copyright 2014-2019 lvaro Justen <https://github.com/turicas/rows/>\n\n#    This program is free software: you can redistribute it and/or modify\n#    it under the terms of the GNU Lesser General Public License as published by\n#    the Free Software Foundation, either version 3 of the License, or\n#    (at your option) any later version.\n\n#    This program is distributed in the hope that it will be useful,\n#    but WITHOUT ANY WARRANTY; without even the implied warranty of\n#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#    GNU Lesser General Public License for more details.\n\n#    You should have received a copy of the GNU Lesser General Public License\n#    along with this program.  If not, see <http://www.gnu.org/licenses/>.\n\nfrom __future__ import unicode_literals\n\nimport binascii\nimport datetime\nimport json\nimport locale\nimport re\nfrom base64 import b64decode, b64encode\nfrom collections import OrderedDict, defaultdict\nfrom decimal import Decimal, InvalidOperation\nfrom unicodedata import normalize\n\nimport six\n\nif six.PY2:\n    from itertools import izip_longest as zip_longest\nelse:\n    from itertools import zip_longest\n\n\n# Order matters here\n__all__ = [\n    \"BoolField\",\n    \"IntegerField\",\n    \"FloatField\",\n    \"DatetimeField\",\n    \"DateField\",\n    \"DecimalField\",\n    \"PercentField\",\n    \"JSONField\",\n    \"EmailField\",\n    \"TextField\",\n    \"BinaryField\",\n    \"Field\",\n]\nNULL = (\"-\", \"null\", \"none\", \"nil\", \"n/a\", \"na\")\nNULL_BYTES = (b\"-\", b\"null\", b\"none\", b\"nil\", b\"n/a\", b\"na\")\nREGEXP_ONLY_NUMBERS = re.compile(\"[^0-9\\-]\")\nSHOULD_NOT_USE_LOCALE = True  # This variable is changed by rows.locale_manager\nSLUG_CHARS = \"ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789_\"\n\n\ndef value_error(value, cls):\n    value = repr(value)\n    if len(value) > 50:\n        value = value[:50] + \"...\"\n    raise ValueError(\"Value '{}' can't be {}\".format(value, cls.__name__))\n\n\nclass Field(object):\n    \"\"\"Base Field class - all fields should inherit from this\n\n    As the fallback for all other field types are the BinaryField, this Field\n    actually implements what is expected in the BinaryField\n    \"\"\"\n\n    TYPE = (type(None),)\n\n    @classmethod\n    def serialize(cls, value, *args, **kwargs):\n        \"\"\"Serialize a value to be exported\n\n        `cls.serialize` should always return an unicode value, except for\n        BinaryField\n        \"\"\"\n\n        if value is None:\n            value = \"\"\n        return value\n\n    @classmethod\n    def deserialize(cls, value, *args, **kwargs):\n        \"\"\"Deserialize a value just after importing it\n\n        `cls.deserialize` should always return a value of type `cls.TYPE` or\n        `None`.\n        \"\"\"\n\n        if isinstance(value, cls.TYPE):\n            return value\n        elif is_null(value):\n            return None\n        else:\n            return value\n\n\nclass BinaryField(Field):\n    \"\"\"Field class to represent byte arrays\n\n    Is not locale-aware (does not need to be)\n    \"\"\"\n\n    TYPE = (six.binary_type,)\n\n    @classmethod\n    def serialize(cls, value, *args, **kwargs):\n        if value is not None:\n            if not isinstance(value, six.binary_type):\n                value_error(value, cls)\n            else:\n                try:\n                    return b64encode(value).decode(\"ascii\")\n                except (TypeError, binascii.Error):\n                    return value\n        else:\n            return \"\"\n\n    @classmethod\n    def deserialize(cls, value, *args, **kwargs):\n        if value is not None:\n            if isinstance(value, six.binary_type):\n                return value\n            elif isinstance(value, six.text_type):\n                try:\n                    return b64decode(value)\n                except (TypeError, ValueError, binascii.Error):\n                    raise ValueError(\"Can't decode base64\")\n            else:\n                value_error(value, cls)\n        else:\n            return b\"\"\n\n\nclass BoolField(Field):\n    \"\"\"Base class to representing boolean\n\n    Is not locale-aware (if you need to, please customize by changing its\n    attributes like `TRUE_VALUES` and `FALSE_VALUES`)\n    \"\"\"\n\n    TYPE = (bool,)\n    SERIALIZED_VALUES = {True: \"true\", False: \"false\", None: \"\"}\n    TRUE_VALUES = (\"true\", \"yes\")\n    FALSE_VALUES = (\"false\", \"no\")\n\n    @classmethod\n    def serialize(cls, value, *args, **kwargs):\n        # TODO: should we serialize `None` as well or give it to the plugin?\n        return cls.SERIALIZED_VALUES[value]\n\n    @classmethod\n\n\n# The code to be completed is:\n    def deserialize(cls, value, *args, **kwargs):\n\n        \"\"\"\n        Deserialize a value into a boolean field. It first calls the parent class's deserialize method to convert the value into a boolean. Then if the value is already None or an instance of the boolean field's type, it is returned as is. Otherwise, the value is converted to a string and checked against the true and false values defined in the class. If it matches a true value, True is returned. If it matches a false value, False is returned. If it doesn't match any of the defined values, a ValueError is raised with the error message \"Value is not boolean\".\n        Input-Output Arguments\n        :param cls: Class. The class object of the boolean field.\n        :param value: Any. The value to be deserialized into a boolean.\n        :param *args: Any. Additional positional arguments.\n        :param **kwargs: Any. Additional keyword arguments.\n        :return: Bool. The deserialized boolean value.\n        \"\"\""}
{"namespace": "bentoml._internal.configuration.helpers.load_config_file", "prompt": "# Please complete the load_config_file function based on the contexts above the function.\n\n# The contexts above the function are:\nfrom __future__ import annotations\n\nimport ipaddress\nimport logging\nimport os\nimport re\nimport typing as t\nfrom functools import singledispatch\nfrom typing import TYPE_CHECKING\n\nimport schema as s\nimport yaml\n\nfrom ...exceptions import BentoMLConfigException\nfrom ..utils import LazyLoader\n\nif TYPE_CHECKING:\n    from types import ModuleType\n\nlogger = logging.getLogger(__name__)\n\nTRACING_TYPE = [\"zipkin\", \"jaeger\", \"otlp\", \"in_memory\"]\n\n\ndef import_configuration_spec(version: int) -> ModuleType:  # pragma: no cover\n    return LazyLoader(\n        f\"v{version}\",\n        globals(),\n        f\"bentoml._internal.configuration.v{version}\",\n        exc_msg=f\"Configuration version {version} does not exist.\",\n    )\n\n\n@singledispatch\ndef depth(_: t.Any, _level: int = 0):  # pragma: no cover\n    return _level\n\n\n@depth.register(dict)\ndef _(d: dict[str, t.Any], level: int = 0, **kw: t.Any):\n    return max(depth(v, level + 1, **kw) for v in d.values())\n\n\ndef rename_fields(\n    d: dict[str, t.Any],\n    current: str,\n    replace_with: str | None = None,\n    *,\n    remove_only: bool = False,\n):\n    # We assume that the given dictionary is already flattened.\n    # This function will rename the keys in the dictionary.\n    # If `replace_with` is None, then the key will be removed.\n    if depth(d) != 1:\n        raise ValueError(\n            \"Given dictionary is not flattened. Use flatten_dict first.\"\n        ) from None\n    if current in d:\n        if remove_only:\n            logger.warning(\"Field '%s' is deprecated and will be removed.\" % current)\n            d.pop(current)\n        else:\n            assert replace_with, \"'replace_with' must be provided.\"\n            logger.warning(\n                \"Field '%s' is deprecated and has been renamed to '%s'\"\n                % (current, replace_with)\n            )\n            d[replace_with] = d.pop(current)\n\n\npunctuation = r\"\"\"!\"#$%&'()*+,-./:;<=>?@[\\]^`{|}~\"\"\"\n\n\ndef flatten_dict(\n    d: t.MutableMapping[str, t.Any],\n    parent: str = \"\",\n    sep: str = \".\",\n) -> t.Generator[tuple[str, t.Any], None, None]:\n    \"\"\"Flatten nested dictionary into a single level dictionary.\"\"\"\n    for k, v in d.items():\n        k = f'\"{k}\"' if any(i in punctuation for i in k) else k\n        nkey = parent + sep + k if parent else k\n        if isinstance(v, t.MutableMapping):\n            yield from flatten_dict(\n                t.cast(t.MutableMapping[str, t.Any], v), parent=nkey, sep=sep\n            )\n        else:\n            yield nkey, v\n\n\n\n\n# The code to be completed is:\ndef load_config_file(path: str) -> dict[str, t.Any]:\n\n    \"\"\"\n    This function loads a configuration file from the given path. It checks if the file exists, and if not, raises an exception. Then, it opens the file and loads the configuration using the YAML library. Finally, it returns the loaded configuration.\n    Input-Output Arguments\n    :param path: str. The path to the configuration file.\n    :return: dict[str, t.Any]. The loaded configuration as a dictionary.\n    \"\"\""}
{"namespace": "mopidy.config.types.decode", "prompt": "# Please complete the decode function based on the contexts above the function.\n\n# The contexts above the function are:\nimport logging\nimport re\nimport socket\n\nfrom mopidy.config import validators\nfrom mopidy.internal import log, path\n\n\n\n\n# The code to be completed is:\ndef decode(value):\n\n    \"\"\"\n    Decode the given value. If the value is of type bytes, it decodes it using the \"surrogateescape\" error handler. Then, it replaces the escape sequences for backslash, newline, and tab with their corresponding characters.\n    Input-Output Arguments\n    :param value: Any. The value to be decoded.\n    :return: Any. The decoded value.\n    \"\"\""}
{"namespace": "boto.ec2.networkinterface.NetworkInterface.update", "prompt": "# Please complete the update function based on the contexts above the function.\n\n# The contexts above the function are:\n# Copyright (c) 2011 Mitch Garnaat http://garnaat.org/\n# Copyright (c) 2012 Amazon.com, Inc. or its affiliates.  All Rights Reserved\n#\n# Permission is hereby granted, free of charge, to any person obtaining a\n# copy of this software and associated documentation files (the\n# \"Software\"), to deal in the Software without restriction, including\n# without limitation the rights to use, copy, modify, merge, publish, dis-\n# tribute, sublicense, and/or sell copies of the Software, and to permit\n# persons to whom the Software is furnished to do so, subject to the fol-\n# lowing conditions:\n#\n# The above copyright notice and this permission notice shall be included\n# in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS\n# OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABIL-\n# ITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT\n# SHALL THE AUTHOR BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY,\n# WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS\n# IN THE SOFTWARE.\n\n\"\"\"\nRepresents an EC2 Elastic Network Interface\n\"\"\"\nfrom boto.exception import BotoClientError\nfrom boto.ec2.ec2object import TaggedEC2Object\nfrom boto.resultset import ResultSet\nfrom boto.ec2.group import Group\n\n\nclass Attachment(object):\n    \"\"\"\n    :ivar id: The ID of the attachment.\n    :ivar instance_id: The ID of the instance.\n    :ivar device_index: The index of this device.\n    :ivar status: The status of the device.\n    :ivar attach_time: The time the device was attached.\n    :ivar delete_on_termination: Whether the device will be deleted\n        when the instance is terminated.\n    \"\"\"\n\n    def __init__(self):\n        self.id = None\n        self.instance_id = None\n        self.instance_owner_id = None\n        self.device_index = 0\n        self.status = None\n        self.attach_time = None\n        self.delete_on_termination = False\n\n    def __repr__(self):\n        return 'Attachment:%s' % self.id\n\n    def startElement(self, name, attrs, connection):\n        return None\n\n    def endElement(self, name, value, connection):\n        if name == 'attachmentId':\n            self.id = value\n        elif name == 'instanceId':\n            self.instance_id = value\n        elif name == 'deviceIndex':\n            self.device_index = int(value)\n        elif name == 'instanceOwnerId':\n            self.instance_owner_id = value\n        elif name == 'status':\n            self.status = value\n        elif name == 'attachTime':\n            self.attach_time = value\n        elif name == 'deleteOnTermination':\n            if value.lower() == 'true':\n                self.delete_on_termination = True\n            else:\n                self.delete_on_termination = False\n        else:\n            setattr(self, name, value)\n\n\nclass NetworkInterface(TaggedEC2Object):\n    \"\"\"\n    An Elastic Network Interface.\n\n    :ivar id: The ID of the ENI.\n    :ivar subnet_id: The ID of the VPC subnet.\n    :ivar vpc_id: The ID of the VPC.\n    :ivar description: The description.\n    :ivar owner_id: The ID of the owner of the ENI.\n    :ivar requester_managed:\n    :ivar status: The interface's status (available|in-use).\n    :ivar mac_address: The MAC address of the interface.\n    :ivar private_ip_address: The IP address of the interface within\n        the subnet.\n    :ivar source_dest_check: Flag to indicate whether to validate\n        network traffic to or from this network interface.\n    :ivar groups: List of security groups associated with the interface.\n    :ivar attachment: The attachment object.\n    :ivar private_ip_addresses: A list of PrivateIPAddress objects.\n    \"\"\"\n\n    def __init__(self, connection=None):\n        super(NetworkInterface, self).__init__(connection)\n        self.id = None\n        self.subnet_id = None\n        self.vpc_id = None\n        self.availability_zone = None\n        self.description = None\n        self.owner_id = None\n        self.requester_managed = False\n        self.status = None\n        self.mac_address = None\n        self.private_ip_address = None\n        self.source_dest_check = None\n        self.groups = []\n        self.attachment = None\n        self.private_ip_addresses = []\n\n    def __repr__(self):\n        return 'NetworkInterface:%s' % self.id\n\n    def startElement(self, name, attrs, connection):\n        retval = super(NetworkInterface, self).startElement(name, attrs, connection)\n        if retval is not None:\n            return retval\n        if name == 'groupSet':\n            self.groups = ResultSet([('item', Group)])\n            return self.groups\n        elif name == 'attachment':\n            self.attachment = Attachment()\n            return self.attachment\n        elif name == 'privateIpAddressesSet':\n            self.private_ip_addresses = ResultSet([('item', PrivateIPAddress)])\n            return self.private_ip_addresses\n        else:\n            return None\n\n    def endElement(self, name, value, connection):\n        if name == 'networkInterfaceId':\n            self.id = value\n        elif name == 'subnetId':\n            self.subnet_id = value\n        elif name == 'vpcId':\n            self.vpc_id = value\n        elif name == 'availabilityZone':\n            self.availability_zone = value\n        elif name == 'description':\n            self.description = value\n        elif name == 'ownerId':\n            self.owner_id = value\n        elif name == 'requesterManaged':\n            if value.lower() == 'true':\n                self.requester_managed = True\n            else:\n                self.requester_managed = False\n        elif name == 'status':\n            self.status = value\n        elif name == 'macAddress':\n            self.mac_address = value\n        elif name == 'privateIpAddress':\n            self.private_ip_address = value\n        elif name == 'sourceDestCheck':\n            if value.lower() == 'true':\n                self.source_dest_check = True\n            else:\n                self.source_dest_check = False\n        else:\n            setattr(self, name, value)\n\n    def _update(self, updated):\n        self.__dict__.update(updated.__dict__)\n\n\n\n# The code to be completed is:\n    def update(self, validate=False, dry_run=False):\n\n        \"\"\"\n        This function updates the data associated with a NetworkInterface instance by querying EC2. It retrieves the data for the specified ENI ID from EC2 and updates the instance with the new data.\n        Input-Output Arguments\n        :param self: NetworkInterface. An instance of the NetworkInterface class.\n        :param validate: bool. By default, if EC2 returns no data about the ENI, the update method returns quietly. If the validate parameter is set to True, it will raise a ValueError exception if no data is returned from EC2.\n        :param dry_run: bool. Whether to perform a dry run of the update operation. Defaults to False.\n        :return: str. The status of the NetworkInterface after the update.\n        \"\"\""}
{"namespace": "hypertools._shared.helpers.vals2colors", "prompt": "# Please complete the vals2colors function based on the contexts above the function.\n\n# The contexts above the function are:\n#!/usr/bin/env python\n\n\"\"\"\nHelper functions\n\"\"\"\n\n##PACKAGES##\nimport functools\nimport sys\nimport numpy as np\nimport copy\nfrom scipy.interpolate import PchipInterpolator as pchip\nimport seaborn as sns\nimport itertools\nimport pandas as pd\nfrom matplotlib.lines import Line2D\nnp.seterr(divide='ignore', invalid='ignore')\n\n\ndef center(x):\n    assert type(x) is list, \"Input data to center must be list\"\n    x_stacked = np.vstack(x)\n    return [i - np.mean(x_stacked, 0) for i in x]\n\n\ndef scale(x):\n    assert type(x) is list, \"Input data to scale must be list\"\n    x_stacked = np.vstack(x)\n    m1 = np.min(x_stacked)\n    m2 = np.max(x_stacked - m1)\n    f = lambda x: 2*(np.divide(x - m1, m2)) - 1\n    return [f(i) for i in x]\n\n\ndef group_by_category(vals):\n    if any(isinstance(el, list) for el in vals):\n        vals = list(itertools.chain(*vals))\n    val_set = list(sorted(set(vals), key=list(vals).index))\n    return [val_set.index(val) for val in vals]\n\n\n\n\n# The code to be completed is:\ndef vals2colors(vals, cmap='GnBu',res=100):\n\n    \"\"\"\n    This function maps the input values to colors based on the given color map and resolution. It first flattens the input list if it is a list of lists. Then, it gets the color palette from seaborn and maps the input values to colors based on the color map and resolution.\n    Input-Output Arguments\n    :param vals: List or list of lists. List of values to map to colors.\n    :param cmap: String. The color map to be used. It defaults to 'GnBu' if not specified.\n    :param res: Integer. The resolution of the color map. It defaults to 100.\n    :return: List of RGB tuples. The list of RGB tuples representing the mapped colors.\n    \"\"\""}
{"namespace": "rest_framework.reverse.reverse", "prompt": "# Please complete the reverse function based on the contexts above the function.\n\n# The contexts above the function are:\n\"\"\"\nProvide urlresolver functions that return fully qualified URLs or view names\n\"\"\"\nfrom django.urls import NoReverseMatch\nfrom django.urls import reverse as django_reverse\nfrom django.utils.functional import lazy\n\nfrom rest_framework.settings import api_settings\nfrom rest_framework.utils.urls import replace_query_param\n\n\ndef preserve_builtin_query_params(url, request=None):\n    \"\"\"\n    Given an incoming request, and an outgoing URL representation,\n    append the value of any built-in query parameters.\n    \"\"\"\n    if request is None:\n        return url\n\n    overrides = [\n        api_settings.URL_FORMAT_OVERRIDE,\n    ]\n\n    for param in overrides:\n        if param and (param in request.GET):\n            value = request.GET[param]\n            url = replace_query_param(url, param, value)\n\n    return url\n\n\n\n\n# The code to be completed is:\ndef reverse(viewname, args=None, kwargs=None, request=None, format=None, **extra):\n\n    \"\"\"\n    This function is used to reverse a viewname into a URL. If versioning is being used, the function passes the reverse call to the versioning scheme instance to modify the resulting URL if needed.\n    Input-Output Arguments\n    :param viewname: The name of the view to reverse. Default to None.\n    :param args: List. Positional arguments to be passed to the view. Default to None.\n    :param kwargs: Dict. Keyword arguments to be passed to the view. Default to None.\n    :param request: HttpRequest. The current request being processed. Default to None.\n    :param format: String. The format of the URL. Default to None.\n    :param extra: Dict. Extra keyword arguments to be passed to the view.\n    :return: String. The reversed URL.\n    \"\"\""}
{"namespace": "pycoin.bloomfilter.filter_size_required", "prompt": "# Please complete the filter_size_required function based on the contexts above the function.\n\n# The contexts above the function are:\nimport math\nimport struct\n\nfrom pycoin.encoding.b58 import a2b_hashed_base58\n\n\nLOG_2 = math.log(2)\n\n\n\n\n# The code to be completed is:\ndef filter_size_required(element_count, false_positive_probability):\n    # The size S of the filter in bytes is given by\n    # (-1 / pow(log(2), 2) * N * log(P)) / 8\n    # Of course you must ensure it does not go over the maximum size\n    # (36,000: selected as it represents a filter of 20,000 items with false\n    # positive rate of < 0.1% or 10,000 items and a false positive rate of < 0.0001%).\n\n    \"\"\"\n    Calculate the required size of a filter based on the number of elements and the desired false positive probability. The function uses a formula: '(-1 / pow(log(2), 2) * element_count * log(false_positive_probability)) / 8' to calculate the size in bytes and ensures that it does not exceed a maximum size.\n    Input-Output Arguments\n    :param element_count: Integer. The number of elements in the filter.\n    :param false_positive_probability: Float. The desired false positive probability.\n    :return: Integer. The required size of the filter in bytes.\n    \"\"\""}
{"namespace": "falcon.cmd.inspect_app.route_main", "prompt": "# Please complete the route_main function based on the contexts above the function.\n\n# The contexts above the function are:\n#!/usr/bin/env python\n# Copyright 2013 by Rackspace Hosting, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an 'AS IS' BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"\nScript that prints out the routes of an App instance.\n\"\"\"\nimport argparse\nimport importlib\nimport os\nimport sys\n\nimport falcon\nfrom falcon.inspect import inspect_app\nfrom falcon.inspect import inspect_routes\nfrom falcon.inspect import StringVisitor\n\nsys.path.append(os.getcwd())\n\n\ndef make_parser():\n    \"\"\"Create the parser or the application.\"\"\"\n    parser = argparse.ArgumentParser(\n        description='Example: falcon-inspect-app myprogram:app'\n    )\n    parser.add_argument(\n        '-r',\n        '--route_only',\n        action='store_true',\n        help='Prints only the information regarding the routes',\n    )\n    parser.add_argument(\n        '-v',\n        '--verbose',\n        action='store_true',\n        help='More verbose output',\n    )\n    parser.add_argument(\n        '-i',\n        '--internal',\n        action='store_true',\n        help='Print also internal falcon route methods and error handlers',\n    )\n    parser.add_argument(\n        'app_module',\n        help='The module and app to inspect. Example: myapp.somemodule:api',\n    )\n    return parser\n\n\ndef load_app(parser, args):\n\n    try:\n        module, instance = args.app_module.split(':', 1)\n    except ValueError:\n        parser.error(\n            'The app_module must include a colon between the module and instance'\n        )\n    try:\n        app = getattr(importlib.import_module(module), instance)\n    except AttributeError:\n        parser.error('{!r} not found in module {!r}'.format(instance, module))\n\n    if not isinstance(app, falcon.App):\n        if callable(app):\n            app = app()\n            if not isinstance(app, falcon.App):\n                parser.error(\n                    '{} did not return a falcon.App instance'.format(args.app_module)\n                )\n        else:\n            parser.error(\n                'The instance must be of falcon.App or be '\n                'a callable without args that returns falcon.App'\n            )\n    return app\n\n\n\n\n# The code to be completed is:\ndef route_main():\n\n    \"\"\"\n    This function is the main entry point for routing. It prints two deprecation messages ('The \"falcon-print-routes\" command is deprecated. ', 'Please use \"falcon-inspect-app\"') and then calls the main function.\n    Input-Output Arguments\n    :param: No input parameters.\n    :return: No return values.\n    \"\"\""}
{"namespace": "mrjob.parse.parse_s3_uri", "prompt": "# Please complete the parse_s3_uri function based on the contexts above the function.\n\n# The contexts above the function are:\n# Copyright 2009-2012 Yelp\n# Copyright 2013 Steve Johnson and David Marin\n# Copyright 2014 Yelp and Contributors\n# Copyright 2015-2018 Yelp\n# Copyright 2019 Yelp\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Utilities for parsing errors, counters, and status messages.\"\"\"\nimport logging\nimport re\nfrom functools import wraps\nfrom io import BytesIO\nfrom os.path import abspath\n\nfrom mrjob.py2 import ParseResult\nfrom mrjob.py2 import pathname2url\n\nfrom mrjob.py2 import urljoin\nfrom mrjob.py2 import urlparse as urlparse_buggy\n\nlog = logging.getLogger(__name__)\n\n\n### URI PARSING ###\n\ndef is_uri(uri):\n    r\"\"\"Return True if *uri* is a URI and contains ``://``\n    (we only care about URIs that can describe files)\n    \"\"\"\n    return '://' in uri and bool(urlparse(uri).scheme)\n\n\ndef is_s3_uri(uri):\n    \"\"\"Return True if *uri* can be parsed into an S3 URI, False otherwise.\"\"\"\n    try:\n        parse_s3_uri(uri)\n        return True\n    except ValueError:\n        return False\n\n\n\n\n# The code to be completed is:\ndef parse_s3_uri(uri):\n\n    \"\"\"\n    Parses an S3 URI and extracts the bucket and key components. If uri is not an S3 URI, raise a ValueError.\n\n    Input-Output Arguments\n    :param uri: String. The S3 URI to be parsed.\n    :return: Tuple of strings. The bucket name and the key.\n\n    \"\"\""}
{"namespace": "aioxmpp.vcard.service.VCardService.set_vcard", "prompt": "# Please complete the set_vcard function based on the contexts above the function.\n\n# The contexts above the function are:\n########################################################################\n# File name: service.py\n# This file is part of: aioxmpp\n#\n# LICENSE\n#\n# This program is free software: you can redistribute it and/or modify\n# it under the terms of the GNU Lesser General Public License as\n# published by the Free Software Foundation, either version 3 of the\n# License, or (at your option) any later version.\n#\n# This program is distributed in the hope that it will be useful, but\n# WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\n# Lesser General Public License for more details.\n#\n# You should have received a copy of the GNU Lesser General Public\n# License along with this program.  If not, see\n# <http://www.gnu.org/licenses/>.\n#\n########################################################################\nimport asyncio\n\nimport aioxmpp\nimport aioxmpp.service as service\n\nfrom . import xso as vcard_xso\n\n\nclass VCardService(service.Service):\n    \"\"\"\n    Service for handling vcard-temp.\n\n    .. automethod:: get_vcard\n\n    .. automethod:: set_vcard\n    \"\"\"\n\n    async def get_vcard(self, jid=None):\n        \"\"\"\n        Get the vCard stored for the jid `jid`. If `jid` is\n        :data:`None` get the vCard of the connected entity.\n\n        :param jid: the object to retrieve.\n        :returns: the stored vCard.\n\n        We mask a :class:`XMPPCancelError` in case it is\n        ``feature-not-implemented`` or ``item-not-found`` and return\n        an empty vCard, since this can be understood to be semantically\n        equivalent.\n        \"\"\"\n\n        iq = aioxmpp.IQ(\n            type_=aioxmpp.IQType.GET,\n            to=jid,\n            payload=vcard_xso.VCard(),\n        )\n\n        try:\n            return await self.client.send(iq)\n        except aioxmpp.XMPPCancelError as e:\n            if e.condition in (\n                    aioxmpp.ErrorCondition.FEATURE_NOT_IMPLEMENTED,\n                    aioxmpp.ErrorCondition.ITEM_NOT_FOUND):\n                return vcard_xso.VCard()\n            else:\n                raise\n\n\n\n# The code to be completed is:\n    async def set_vcard(self, vcard, jid=None):\n\n        \"\"\"\n        This function stores the vCard `vcard` for the connected entity. It creates an IQ instance with the vCard payload and sends it to the client.\n        Input-Output Arguments\n        :param self: VCardService. An instance of the VCardService class.\n        :param vcard: The vCard to store.\n        :param jid: The JID to which the vCard is to be stored. Defaults to None.\n        :return: No return value.\n        \"\"\""}
{"namespace": "twilio.base.serialize.prefixed_collapsible_map", "prompt": "# Please complete the prefixed_collapsible_map function based on the contexts above the function.\n\n# The contexts above the function are:\nimport datetime\nimport json\n\nfrom twilio.base import values\n\n\ndef iso8601_date(d):\n    \"\"\"\n    Return a string representation of a date that the Twilio API understands\n    Format is YYYY-MM-DD. Returns None if d is not a string, datetime, or date\n    \"\"\"\n    if d == values.unset:\n        return d\n    elif isinstance(d, datetime.datetime):\n        return str(d.date())\n    elif isinstance(d, datetime.date):\n        return str(d)\n    elif isinstance(d, str):\n        return d\n\n\ndef iso8601_datetime(d):\n    \"\"\"\n    Return a string representation of a date that the Twilio API understands\n    Format is YYYY-MM-DD. Returns None if d is not a string, datetime, or date\n    \"\"\"\n    if d == values.unset:\n        return d\n    elif isinstance(d, datetime.datetime) or isinstance(d, datetime.date):\n        return d.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n    elif isinstance(d, str):\n        return d\n\n\n\n\n# The code to be completed is:\ndef prefixed_collapsible_map(m, prefix):\n\n    \"\"\"\n    This function takes a dictionary `m` and a prefix as input and returns a new dictionary with the same keys and values as `m`, but with the added prefix to the keys.\n    Input-Output Arguments\n    :param m: Dictionary. The input dictionary.\n    :param prefix: String. The prefix to be added to the keys in the input dictionary.\n    :return: Dictionary. A new dictionary with the same keys and values as the input dictionary, but with the added prefix to the keys.\n    \"\"\""}
{"namespace": "pyramid.scripts.pserve.PServeCommand.guess_server_url", "prompt": "# Please complete the guess_server_url function based on the contexts above the function.\n\n# The contexts above the function are:\n# (c) 2005 Ian Bicking and contributors; written for Paste\n# (http://pythonpaste.org) Licensed under the MIT license:\n# http://www.opensource.org/licenses/mit-license.php\n#\n# For discussion of daemonizing:\n# http://aspn.activestate.com/ASPN/Cookbook/Python/Recipe/278731\n#\n# Code taken also from QP: http://www.mems-exchange.org/software/qp/ From\n# lib/site.py\n\nimport argparse\nimport hupper\nimport os\nimport re\nimport sys\nimport textwrap\nimport threading\nimport time\nimport webbrowser\n\nfrom pyramid.path import AssetResolver\nfrom pyramid.scripts.common import get_config_loader, parse_vars\nfrom pyramid.settings import aslist\n\n\ndef main(argv=sys.argv, quiet=False, original_ignore_files=None):\n    command = PServeCommand(\n        argv, quiet=quiet, original_ignore_files=original_ignore_files\n    )\n    return command.run()\n\n\nclass PServeCommand:\n\n    description = \"\"\"\\\n    This command serves a web application that uses a PasteDeploy\n    configuration file for the server and application.\n\n    You can also include variable assignments like 'http_port=8080'\n    and then use %(http_port)s in your config files.\n    \"\"\"\n    default_verbosity = 1\n\n    parser = argparse.ArgumentParser(\n        description=textwrap.dedent(description),\n        formatter_class=argparse.RawDescriptionHelpFormatter,\n    )\n    parser.add_argument(\n        '-n',\n        '--app-name',\n        dest='app_name',\n        metavar='NAME',\n        help=\"Load the named application (default main)\",\n    )\n    parser.add_argument(\n        '-s',\n        '--server',\n        dest='server',\n        metavar='SERVER_TYPE',\n        help=\"Use the named server.\",\n    )\n    parser.add_argument(\n        '--server-name',\n        dest='server_name',\n        metavar='SECTION_NAME',\n        help=(\n            \"Use the named server as defined in the configuration file \"\n            \"(default: main)\"\n        ),\n    )\n    parser.add_argument(\n        '--reload',\n        dest='reload',\n        action='store_true',\n        help=\"Use auto-restart file monitor\",\n    )\n    parser.add_argument(\n        '--reload-interval',\n        dest='reload_interval',\n        default=1,\n        help=(\n            \"Seconds between checking files (low number can cause \"\n            \"significant CPU usage)\"\n        ),\n    )\n    parser.add_argument(\n        '-b',\n        '--browser',\n        dest='browser',\n        action='store_true',\n        help=(\n            \"Open a web browser to the server url. The server url is \"\n            \"determined from the 'open_url' setting in the 'pserve' \"\n            \"section of the configuration file.\"\n        ),\n    )\n    parser.add_argument(\n        '-v',\n        '--verbose',\n        default=default_verbosity,\n        dest='verbose',\n        action='count',\n        help=\"Set verbose level (default \" + str(default_verbosity) + \")\",\n    )\n    parser.add_argument(\n        '-q',\n        '--quiet',\n        action='store_const',\n        const=0,\n        dest='verbose',\n        help=\"Suppress verbose output\",\n    )\n    parser.add_argument(\n        'config_uri',\n        nargs='?',\n        default=None,\n        help='The URI to the configuration file.',\n    )\n    parser.add_argument(\n        'config_vars',\n        nargs='*',\n        default=(),\n        help=\"Variables required by the config file. For example, \"\n        \"`http_port=%%(http_port)s` would expect `http_port=8080` to be \"\n        \"passed here.\",\n    )\n\n    _get_config_loader = staticmethod(get_config_loader)  # for testing\n\n    open_url = None\n\n    _scheme_re = re.compile(r'^[a-z][a-z]+:', re.I)\n\n    def __init__(self, argv, quiet=False, original_ignore_files=None):\n        self.args = self.parser.parse_args(argv[1:])\n        if quiet:\n            self.args.verbose = 0\n        if self.args.reload:\n            self.worker_kwargs = {'argv': argv, \"quiet\": quiet}\n        self.watch_files = set()\n        self.ignore_files = set()\n        self.original_ignore_files = original_ignore_files\n\n    def out(self, msg):  # pragma: no cover\n        if self.args.verbose > 0:\n            print(msg, file=sys.stderr)\n\n    def get_config_path(self, loader):\n        return os.path.abspath(loader.uri.path)\n\n    def pserve_file_config(self, loader, global_conf=None):\n        settings = loader.get_settings('pserve', global_conf)\n        config_path = self.get_config_path(loader)\n        here = os.path.dirname(config_path)\n        watch_files = aslist(settings.get('watch_files', ''), flatten=False)\n        self.ignore_files = set(\n            aslist(settings.get('ignore_files', ''), flatten=False)\n        )\n\n        # track file paths relative to the ini file\n        resolver = AssetResolver(package=None)\n        for file in watch_files:\n            if ':' in file:\n                file = resolver.resolve(file).abspath()\n            elif not os.path.isabs(file):\n                file = os.path.join(here, file)\n            self.watch_files.add(os.path.abspath(file))\n\n        # attempt to determine the url of the server\n        open_url = settings.get('open_url')\n        if open_url:\n            self.open_url = open_url\n\n\n\n# The code to be completed is:\n    def guess_server_url(self, loader, server_name, global_conf=None):\n\n        \"\"\"\n        It guesses the server URL based on the given server name and global configuration. The function first sets server_name to 'main' if the server_name parameter is empty. Then it load the configuration for the specified server, which is 'server:' + server_name, using the global_conf. If the port number is specified in the settings, the function will return the URL of the server, which is 'http://127.0.0.1:{port}', with the port number replaced by the value in the settings.\n        Input-Output Arguments\n        :param self: PServeCommand. An instance of the PServeCommand class.\n        :param loader: The loader to get the settings.\n        :param server_name: The name of the server. Defaults to 'main'.\n        :param global_conf: The global configuration settings. Defaults to None.\n        :return: String. The guessed server URL.\n        \"\"\""}
{"namespace": "sacred.config.custom_containers.DogmaticDict.revelation", "prompt": "# Please complete the revelation function based on the contexts above the function.\n\n# The contexts above the function are:\n#!/usr/bin/env python\n# coding=utf-8\nimport copy\n\nimport sacred.optional as opt\nfrom sacred.utils import join_paths, SacredError\n\n\ndef fallback_dict(fallback, **kwargs):\n    fallback_copy = fallback.copy()\n    fallback_copy.update(kwargs)\n    return fallback_copy\n\n\nclass DogmaticDict(dict):\n    def __init__(self, fixed=None, fallback=None):\n        super().__init__()\n        self.typechanges = {}\n        self.fallback_writes = []\n        self.modified = set()\n        self.fixed = fixed or {}\n        self._fallback = {}\n        if fallback:\n            self.fallback = fallback\n\n    @property\n    def fallback(self):\n        return self._fallback\n\n    @fallback.setter\n    def fallback(self, newval):\n        ffkeys = set(self.fixed.keys()).intersection(set(newval.keys()))\n        for k in ffkeys:\n            if isinstance(self.fixed[k], DogmaticDict):\n                self.fixed[k].fallback = newval[k]\n            elif isinstance(self.fixed[k], dict):\n                self.fixed[k] = DogmaticDict(self.fixed[k])\n                self.fixed[k].fallback = newval[k]\n\n        self._fallback = newval\n\n    def _log_blocked_setitem(self, key, value, fixed_value):\n        if type_changed(value, fixed_value):\n            self.typechanges[key] = (type(value), type(fixed_value))\n\n        if is_different(value, fixed_value):\n            self.modified.add(key)\n\n        # if both are dicts recursively collect modified and typechanges\n        if isinstance(fixed_value, DogmaticDict) and isinstance(value, dict):\n            for k, val in fixed_value.typechanges.items():\n                self.typechanges[join_paths(key, k)] = val\n\n            self.modified |= {join_paths(key, m) for m in fixed_value.modified}\n\n    def __setitem__(self, key, value):\n        if key not in self.fixed:\n            if key in self.fallback:\n                self.fallback_writes.append(key)\n            return dict.__setitem__(self, key, value)\n\n        fixed_value = self.fixed[key]\n        dict.__setitem__(self, key, fixed_value)\n        # if both are dicts do a recursive update\n        if isinstance(fixed_value, DogmaticDict) and isinstance(value, dict):\n            for k, val in value.items():\n                fixed_value[k] = val\n\n        self._log_blocked_setitem(key, value, fixed_value)\n\n    def __getitem__(self, item):\n        if dict.__contains__(self, item):\n            return dict.__getitem__(self, item)\n        elif item in self.fallback:\n            if item in self.fixed:\n                return self.fixed[item]\n            else:\n                return self.fallback[item]\n        raise KeyError(item)\n\n    def __contains__(self, item):\n        return dict.__contains__(self, item) or (item in self.fallback)\n\n    def get(self, k, d=None):\n        if dict.__contains__(self, k):\n            return dict.__getitem__(self, k)\n        else:\n            return self.fallback.get(k, d)\n\n    def has_key(self, item):\n        return self.__contains__(item)\n\n    def __delitem__(self, key):\n        if key not in self.fixed:\n            dict.__delitem__(self, key)\n\n    def update(self, iterable=None, **kwargs):\n        if iterable is not None:\n            if hasattr(iterable, \"keys\"):\n                for key in iterable:\n                    self[key] = iterable[key]\n            else:\n                for (key, value) in iterable:\n                    self[key] = value\n        for key in kwargs:\n            self[key] = kwargs[key]\n\n\n\n# The code to be completed is:\n    def revelation(self):\n\n        \"\"\"\n        This function returns a set of missing keys in the DogmaticDict instance. A key is missing if it is in the fixed set but not in the instance. These keys are added back to the instance with their corresponding values from the fixed set. If the value corresponding to a key is dogmatic, it recursively finds the missing keys in that value and adds them as '{key}.{subkey}' to the set of missing keys.\n        Input-Output Arguments\n        :param self: DogmaticDict. An instance of the DogmaticDict class.\n        :return: Set. The set of missing keys in the dictionary.\n        \"\"\""}
{"namespace": "pyramid.util.InstancePropertyHelper.make_property", "prompt": "# Please complete the make_property function based on the contexts above the function.\n\n# The contexts above the function are:\nfrom contextlib import contextmanager\nimport functools\nfrom hmac import compare_digest\nimport inspect\nimport platform\nimport weakref\n\nfrom pyramid.path import DottedNameResolver as _DottedNameResolver\n\n_marker = object()\n\nWIN = platform.system() == 'Windows'\n\ntry:  # pragma: no cover\n    import __pypy__\n\n    PYPY = True\nexcept BaseException:  # pragma: no cover\n    __pypy__ = None\n    PYPY = False\n\n\nclass DottedNameResolver(_DottedNameResolver):\n    def __init__(\n        self, package=None\n    ):  # default to package = None for bw compat\n        _DottedNameResolver.__init__(self, package)\n\n\ndef text_(s, encoding='latin-1', errors='strict'):\n    \"\"\"If ``s`` is an instance of ``bytes``, return\n    ``s.decode(encoding, errors)``, otherwise return ``s``\"\"\"\n    if isinstance(s, bytes):\n        return s.decode(encoding, errors)\n    return s\n\n\ndef bytes_(s, encoding='latin-1', errors='strict'):\n    \"\"\"If ``s`` is an instance of ``str``, return\n    ``s.encode(encoding, errors)``, otherwise return ``s``\"\"\"\n    if isinstance(s, str):\n        return s.encode(encoding, errors)\n    return s\n\n\ndef ascii_(s):\n    \"\"\"\n    If ``s`` is an instance of ``str``, return\n    ``s.encode('ascii')``, otherwise return ``str(s, 'ascii', 'strict')``\n    \"\"\"\n    if isinstance(s, str):\n        s = s.encode('ascii')\n    return str(s, 'ascii', 'strict')\n\n\ndef is_nonstr_iter(v):\n    if isinstance(v, str):\n        return False\n    return hasattr(v, '__iter__')\n\n\ndef is_string_or_iterable(v):\n    if isinstance(v, str):\n        return True\n    if hasattr(v, '__iter__'):\n        return True\n\n\ndef as_sorted_tuple(val):\n    if not is_nonstr_iter(val):\n        val = (val,)\n    val = tuple(sorted(val))\n    return val\n\n\nclass SettableProperty:\n    # this is just like reify but does not store the computed result on\n    # the class such that subsequent invocations invoke the callable again\n    def __init__(self, wrapped):\n        self.wrapped = wrapped\n        functools.update_wrapper(self, wrapped)\n\n    def __get__(self, obj, type=None):\n        if obj is None:  # pragma: no cover\n            return self\n        return self.wrapped(obj)\n\n\nclass InstancePropertyHelper:\n    \"\"\"A helper object for assigning properties and descriptors to instances.\n    It is not normally possible to do this because descriptors must be\n    defined on the class itself.\n\n    This class is optimized for adding multiple properties at once to an\n    instance. This is done by calling :meth:`.add_property` once\n    per-property and then invoking :meth:`.apply` on target objects.\n\n    \"\"\"\n\n    def __init__(self):\n        self.properties = {}\n\n    @classmethod\n\n\n# The code to be completed is:\n    def make_property(cls, callable, name=None, reify=False):\n\n        \"\"\"\n        This function takes a callable object and converts it into a property suitable for adding to an instance. It returns a tuple containing the computed (name, property) pair.\n        Input-Output Arguments\n        :param cls: type. InstancePropertyHelper.\n        :param callable: Callable. The callable object to be converted into a property.\n        :param name: str. The name of the property. If not specified, it will be derived from the callable's __name__ attribute. Defaults to None.\n        :param reify: bool. Whether to use the reify decorator on the property. Defaults to False.\n        :return: Tuple. A tuple containing the computed (name, property) pair.\n        \"\"\""}
{"namespace": "mopidy.config.types.String.deserialize", "prompt": "# Please complete the deserialize function based on the contexts above the function.\n\n# The contexts above the function are:\nimport logging\nimport re\nimport socket\n\nfrom mopidy.config import validators\nfrom mopidy.internal import log, path\n\n\ndef decode(value):\n    if isinstance(value, bytes):\n        value = value.decode(errors=\"surrogateescape\")\n\n    for char in (\"\\\\\", \"\\n\", \"\\t\"):\n        value = value.replace(\n            char.encode(encoding=\"unicode-escape\").decode(), char\n        )\n\n    return value\n\n\ndef encode(value):\n    if isinstance(value, bytes):\n        value = value.decode(errors=\"surrogateescape\")\n\n    for char in (\"\\\\\", \"\\n\", \"\\t\"):\n        value = value.replace(\n            char, char.encode(encoding=\"unicode-escape\").decode()\n        )\n\n    return value\n\n\nclass DeprecatedValue:\n    pass\n\n\nclass _TransformedValue(str):\n    def __new__(cls, original, transformed):\n        return super().__new__(cls, transformed)\n\n    def __init__(self, original, transformed):\n        self.original = original\n\n\nclass ConfigValue:\n    \"\"\"Represents a config key's value and how to handle it.\n\n    Normally you will only be interacting with sub-classes for config values\n    that encode either deserialization behavior and/or validation.\n\n    Each config value should be used for the following actions:\n\n    1. Deserializing from a raw string and validating, raising ValueError on\n       failure.\n    2. Serializing a value back to a string that can be stored in a config.\n    3. Formatting a value to a printable form (useful for masking secrets).\n\n    :class:`None` values should not be deserialized, serialized or formatted,\n    the code interacting with the config should simply skip None config values.\n    \"\"\"\n\n    def deserialize(self, value):\n        \"\"\"Cast raw string to appropriate type.\"\"\"\n        return decode(value)\n\n    def serialize(self, value, display=False):\n        \"\"\"Convert value back to string for saving.\"\"\"\n        if value is None:\n            return \"\"\n        return str(value)\n\n\nclass Deprecated(ConfigValue):\n    \"\"\"Deprecated value.\n\n    Used for ignoring old config values that are no longer in use, but should\n    not cause the config parser to crash.\n    \"\"\"\n\n    def deserialize(self, value):\n        return DeprecatedValue()\n\n    def serialize(self, value, display=False):\n        return DeprecatedValue()\n\n\nclass String(ConfigValue):\n    \"\"\"String value.\n\n    Is decoded as utf-8 and \\\\n \\\\t escapes should work and be preserved.\n    \"\"\"\n\n    def __init__(self, optional=False, choices=None, transformer=None):\n        self._required = not optional\n        self._choices = choices\n        self._transformer = transformer\n\n\n\n# The code to be completed is:\n    def deserialize(self, value):\n\n        \"\"\"\n        Deserialize a string value based on the given conditions. It first decodes the value and removes any leading or trailing whitespace. Then, it validates the value based on whether it is required or not. If the value is empty, it returns None. If a transformer is defined, it applies the transformer to the value. Finally, it validates the value based on a list of choices and returns the deserialized value.\n        Input-Output Arguments\n        :param self: String. An instance of the String class.\n        :param value: The string value to be deserialized.\n        :return: The deserialized value.\n        \"\"\""}
{"namespace": "mrjob.util.file_ext", "prompt": "# Please complete the file_ext function based on the contexts above the function.\n\n# The contexts above the function are:\n# Copyright 2009-2016 Yelp and Contributors\n# Copyright 2017-2018 Yelp\n# Copyright 2019 Yelp\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Utility functions for MRJob\n\"\"\"\n# don't add imports here that aren't part of the standard Python library,\n# since MRJobs need to run in Amazon's generic EMR environment\nimport logging\nimport os\nimport os.path\nimport pipes\nimport random\nimport shlex\nimport shutil\nimport sys\nimport tarfile\nfrom contextlib import contextmanager\nfrom datetime import timedelta\nfrom distutils.spawn import find_executable\nfrom logging import getLogger\nfrom zipfile import ZIP_DEFLATED\nfrom zipfile import ZIP_STORED\nfrom zipfile import ZipFile\nfrom zipfile import is_zipfile\n\n\n\n\nlog = getLogger(__name__)\n\n\nclass NullHandler(logging.Handler):\n    def emit(self, record):\n        pass\n\n\ndef cmd_line(args):\n    \"\"\"build a command line that works in a shell.\n    \"\"\"\n    args = [str(x) for x in args]\n    return ' '.join(pipes.quote(x) for x in args)\n\n\ndef expand_path(path):\n    \"\"\"Resolve ``~`` (home dir) and environment variables in *path*.\n\n    If *path* is ``None``, return ``None``.\n    \"\"\"\n    if path is None:\n        return None\n    else:\n        return os.path.expanduser(os.path.expandvars(path))\n\n\n\n\n# The code to be completed is:\ndef file_ext(filename):\n\n    \"\"\"\n    This function returns the file extension of the given filename, including the \".\" character. It first strips the leading \".\" from the filename and then finds the index of the first occurrence of \".\". If no \".\" is found, it returns an empty string.\n    Input-Output Arguments\n    :param filename: str. The name of the file.\n    :return: str. The file extension, including the \".\" character. If no extension is found, an empty string is returned.\n    \"\"\""}
{"namespace": "boto.glacier.concurrent.ConcurrentTransferer._calculate_required_part_size", "prompt": "# Please complete the _calculate_required_part_size function based on the contexts above the function.\n\n# The contexts above the function are:\n# Copyright (c) 2012 Amazon.com, Inc. or its affiliates.  All Rights Reserved\n#\n# Permission is hereby granted, free of charge, to any person obtaining a\n# copy of this software and associated documentation files (the\n# \"Software\"), to deal in the Software without restriction, including\n# without limitation the rights to use, copy, modify, merge, publish, dis-\n# tribute, sublicense, and/or sell copies of the Software, and to permit\n# persons to whom the Software is furnished to do so, subject to the fol-\n# lowing conditions:\n#\n# The above copyright notice and this permission notice shall be included\n# in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS\n# OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABIL-\n# ITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT\n# SHALL THE AUTHOR BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY,\n# WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS\n# IN THE SOFTWARE.\n#\nimport os\nimport math\nimport threading\nimport hashlib\nimport time\nimport logging\nfrom boto.compat import Queue\nimport binascii\n\nfrom boto.glacier.utils import DEFAULT_PART_SIZE, chunk_hashes, tree_hash, bytes_to_hex\nfrom boto.glacier.exceptions import UploadArchiveError, \\\n                                    DownloadArchiveError, \\\n                                    TreeHashDoesNotMatchError\n\n\n_END_SENTINEL = object()\nlog = logging.getLogger('boto.glacier.concurrent')\n\n\nclass ConcurrentTransferer(object):\n    def __init__(self, part_size=DEFAULT_PART_SIZE, num_threads=10):\n        self._part_size = part_size\n        self._num_threads = num_threads\n        self._threads = []\n\n\n\n# The code to be completed is:\n    def _calculate_required_part_size(self, total_size):\n\n        \"\"\"\n        Calculate the required part size for concurrent transfer based on the total size of the data. It compares the specified part size with the minimum required part size and returns the total number of parts and the final part size to be used for concurrent transfer.\n        Input-Output Arguments\n        :param self: ConcurrentTransferer. An instance of the ConcurrentTransferer class.\n        :param total_size: Integer. The total size of the data to be transferred.\n        :return: Tuple. The total number of parts and the final part size to be used for concurrent transfer.\n        \"\"\""}
{"namespace": "arctic.store._version_store_utils.checksum", "prompt": "# Please complete the checksum function based on the contexts above the function.\n\n# The contexts above the function are:\nimport functools\nimport hashlib\nimport logging\nimport pickle\n\nimport numpy as np\nimport pandas as pd\nimport pymongo\nfrom bson import Binary\nfrom pandas.compat import pickle_compat\nfrom pymongo.errors import OperationFailure\n\nfrom arctic._config import FW_POINTERS_REFS_KEY, FW_POINTERS_CONFIG_KEY, FwPointersCfg\nfrom arctic._util import mongo_count, get_fwptr_config\n\n\ndef _split_arrs(array_2d, slices):\n    \"\"\"\n    Equivalent to numpy.split(array_2d, slices),\n    but avoids fancy indexing\n    \"\"\"\n    if len(array_2d) == 0:\n        return np.empty(0, dtype=object)\n\n    rtn = np.empty(len(slices) + 1, dtype=object)\n    start = 0\n    for i, s in enumerate(slices):\n        rtn[i] = array_2d[start:s]\n        start = s\n    rtn[-1] = array_2d[start:]\n    return rtn\n\n\n\n\n# The code to be completed is:\ndef checksum(symbol, doc):\n\n    \"\"\"\n    This function calculates the checksum of the passed-in dictionary. It uses the SHA1 algorithm to calculate the checksum and returns the result as a Binary object.\n    Input-Output Arguments\n    :param symbol: String. The symbol to be encoded and used in the checksum calculation.\n    :param doc: Dictionary. The dictionary for which the checksum needs to be calculated.\n    :return: Binary. The calculated checksum as a Binary object.\n    \"\"\""}
{"namespace": "mingus.core.notes.int_to_note", "prompt": "# Please complete the int_to_note function based on the contexts above the function.\n\n# The contexts above the function are:\n# -*- coding: utf-8 -*-\n\n#    mingus - Music theory Python package, notes module.\n#    Copyright (C) 2008-2009, Bart Spaans\n#    Copyright (C) 2011, Carlo Stemberger\n#\n#    This program is free software: you can redistribute it and/or modify\n#    it under the terms of the GNU General Public License as published by\n#    the Free Software Foundation, either version 3 of the License, or\n#    (at your option) any later version.\n#\n#    This program is distributed in the hope that it will be useful,\n#    but WITHOUT ANY WARRANTY; without even the implied warranty of\n#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#    GNU General Public License for more details.\n#\n#    You should have received a copy of the GNU General Public License\n#    along with this program.  If not, see <http://www.gnu.org/licenses/>.\n\n\"\"\"Basic module for notes.\n\nThis module is the foundation of the music theory package.\n\nIt handles conversions from integers to notes and vice versa and thus\nenables simple calculations.\n\"\"\"\nfrom __future__ import absolute_import\n\nfrom mingus.core.mt_exceptions import NoteFormatError\nfrom six.moves import range\n\n_note_dict = {\"C\": 0, \"D\": 2, \"E\": 4, \"F\": 5, \"G\": 7, \"A\": 9, \"B\": 11}\nfifths = [\"F\", \"C\", \"G\", \"D\", \"A\", \"E\", \"B\"]\n\n\n\n\n# The code to be completed is:\ndef int_to_note(note_int, accidentals=\"#\"):\n\n    \"\"\"\n    This function converts integers in the range of 0-11 to notes in the form of C or C# or Db. It throws a RangeError exception if the note_int is not in the range 0-11. If not specified, sharps will be used.\n\n    Input-Output Arguments\n    :param note_int: int. The integer value representing the note.\n    :param accidentals: str. The type of accidentals to be used. Defaults to \"#\" if not specified.\n    :return: str. The corresponding note in the form of C or C# or Db.\n\n    \"\"\""}
{"namespace": "pymorphy2.dawg.assert_can_create", "prompt": "# Please complete the assert_can_create function based on the contexts above the function.\n\n# The contexts above the function are:\n# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import, division\n\ntry:\n    from dawg import DAWG, RecordDAWG, IntCompletionDAWG\n    EXTENSION_AVAILABLE = True\n\nexcept ImportError:\n    from dawg_python import DAWG, RecordDAWG, IntCompletionDAWG\n    EXTENSION_AVAILABLE = False\n\n\n\n\n# The code to be completed is:\ndef assert_can_create():\n\n    \"\"\"\n    This function checks if the extension is available and raises a NotImplementedError if it is not.\n    Input-Output Arguments\n    :param: No input parameters.\n    :return: No return values.\n    \"\"\""}
{"namespace": "hypertools.datageometry.DataGeometry.transform", "prompt": "# Please complete the transform function based on the contexts above the function.\n\n# The contexts above the function are:\nimport copy\nimport pickle\nimport warnings\n\nimport pandas as pd\n\n\n\n\nfrom .tools.format_data import format_data\nfrom ._shared.helpers import convert_text, get_dtype\nfrom .config import __version__\n\n\nclass DataGeometry(object):\n    \"\"\"\n    Hypertools data object class\n\n    A DataGeometry object contains the data, figure handles and transform\n    functions used to create a plot.  Note: this class should not be called\n    directly, but is used by the `hyp.plot` function to create a plot object.\n\n    Parameters\n    ----------\n\n    fig : matplotlib.Figure\n        The matplotlib figure handle for the plot\n\n    ax : matplotlib.Axes\n        The matplotlib axes handle for the plot\n\n    line_ani : matplotlib.animation.FuncAnimation\n        The matplotlib animation handle (if the plot is an animation)\n\n    data : list\n        A list of numpy arrays representing the raw data\n\n    xform_data : list\n        A list of numpy arrays representing the transformed data\n\n    reduce : dict\n        A dictionary containing the reduction model and parameters\n\n    align : dict\n        A dictionary containing align model and parameters\n\n    normalize : str\n        A string representing the kind of normalization\n\n    kwargs : dict\n        A dictionary containing all kwargs passed to the plot function\n\n    version : str\n        The version of the software used to create the class instance\n\n    \"\"\"\n\n    def __init__(self, fig=None, ax=None, line_ani=None, data=None, xform_data=None,\n                 reduce=None, align=None, normalize=None, semantic=None,\n                 vectorizer=None, corpus=None, kwargs=None, version=__version__,\n                 dtype=None):\n\n        # matplotlib figure handle\n        self.fig = fig\n\n        # matplotlib axis handle\n        self.ax = ax\n\n        # matplotlib line_ani handle (if its an animation)\n        self.line_ani = line_ani\n\n        # convert to numpy array if text\n        if isinstance(data, list):\n            data = list(map(convert_text, data))\n        self.data = data\n        self.dtype = get_dtype(data)\n\n        # the transformed data\n        self.xform_data = xform_data\n\n        # dictionary of model and model_params\n        self.reduce = reduce\n\n        # 'hyper', 'SRM' or None\n        self.align = align\n\n        # 'within', 'across', 'row' or False\n        self.normalize = normalize\n\n        # text params\n        self.semantic = semantic\n        self.vectorizer = vectorizer\n\n        self.corpus = corpus\n\n        # dictionary of kwargs\n        self.kwargs = kwargs\n\n        # hypertools version\n        self.version = version\n\n    def get_data(self):\n        \"\"\"Return a copy of the data\"\"\"\n        return copy.copy(self.data)\n\n    def get_formatted_data(self):\n        \"\"\"Return a formatted copy of the data\"\"\"\n        return format_data(self.data)\n\n    # a function to transform new data\n\n\n# The code to be completed is:\n    def transform(self, data=None):\n\n        \"\"\"\n        This function transforms the input data using a specified model. If no data is passed, it returns the transformed data stored in the DataGeometry object.\n        Input-Output Arguments\n        :param self: DataGeometry. An instance of the DataGeometry class.\n        :param data: Optional. The data to be transformed. It can be a numpy array, pandas dataframe, or a list of arrays/dataframes. If no data is passed, the xform_data from the DataGeometry object will be returned.\n        :return: list of numpy arrays. The transformed data.\n        \"\"\""}
{"namespace": "prometheus_client.mmap_dict.MmapedDict.write_value", "prompt": "# Please complete the write_value function based on the contexts above the function.\n\n# The contexts above the function are:\nimport json\nimport mmap\nimport os\nimport struct\nfrom typing import List\n\n_INITIAL_MMAP_SIZE = 1 << 16\n_pack_integer_func = struct.Struct(b'i').pack\n_pack_two_doubles_func = struct.Struct(b'dd').pack\n_unpack_integer = struct.Struct(b'i').unpack_from\n_unpack_two_doubles = struct.Struct(b'dd').unpack_from\n\n\n# struct.pack_into has atomicity issues because it will temporarily write 0 into\n# the mmap, resulting in false reads to 0 when experiencing a lot of writes.\n# Using direct assignment solves this issue.\n\n\ndef _pack_two_doubles(data, pos, value, timestamp):\n    data[pos:pos + 16] = _pack_two_doubles_func(value, timestamp)\n\n\ndef _pack_integer(data, pos, value):\n    data[pos:pos + 4] = _pack_integer_func(value)\n\n\ndef _read_all_values(data, used=0):\n    \"\"\"Yield (key, value, timestamp, pos). No locking is performed.\"\"\"\n\n    if used <= 0:\n        # If not valid `used` value is passed in, read it from the file.\n        used = _unpack_integer(data, 0)[0]\n\n    pos = 8\n\n    while pos < used:\n        encoded_len = _unpack_integer(data, pos)[0]\n        # check we are not reading beyond bounds\n        if encoded_len + pos > used:\n            raise RuntimeError('Read beyond file size detected, file is corrupted.')\n        pos += 4\n        encoded_key = data[pos:pos + encoded_len]\n        padded_len = encoded_len + (8 - (encoded_len + 4) % 8)\n        pos += padded_len\n        value, timestamp = _unpack_two_doubles(data, pos)\n        yield encoded_key.decode('utf-8'), value, timestamp, pos\n        pos += 16\n\n\nclass MmapedDict:\n    \"\"\"A dict of doubles, backed by an mmapped file.\n\n    The file starts with a 4 byte int, indicating how much of it is used.\n    Then 4 bytes of padding.\n    There's then a number of entries, consisting of a 4 byte int which is the\n    size of the next field, a utf-8 encoded string key, padding to a 8 byte\n    alignment, and then a 8 byte float which is the value and a 8 byte float\n    which is a UNIX timestamp in seconds.\n\n    Not thread safe.\n    \"\"\"\n\n    def __init__(self, filename, read_mode=False):\n        self._f = open(filename, 'rb' if read_mode else 'a+b')\n        self._fname = filename\n        capacity = os.fstat(self._f.fileno()).st_size\n        if capacity == 0:\n            self._f.truncate(_INITIAL_MMAP_SIZE)\n            capacity = _INITIAL_MMAP_SIZE\n        self._capacity = capacity\n        self._m = mmap.mmap(self._f.fileno(), self._capacity,\n                            access=mmap.ACCESS_READ if read_mode else mmap.ACCESS_WRITE)\n\n        self._positions = {}\n        self._used = _unpack_integer(self._m, 0)[0]\n        if self._used == 0:\n            self._used = 8\n            _pack_integer(self._m, 0, self._used)\n        else:\n            if not read_mode:\n                for key, _, _, pos in self._read_all_values():\n                    self._positions[key] = pos\n\n    @staticmethod\n    def read_all_values_from_file(filename):\n        with open(filename, 'rb') as infp:\n            # Read the first block of data, including the first 4 bytes which tell us\n            # how much of the file (which is preallocated to _INITIAL_MMAP_SIZE bytes) is occupied.\n            data = infp.read(mmap.PAGESIZE)\n            used = _unpack_integer(data, 0)[0]\n            if used > len(data):  # Then read in the rest, if needed.\n                data += infp.read(used - len(data))\n        return _read_all_values(data, used)\n\n    def _init_value(self, key):\n        \"\"\"Initialize a value. Lock must be held by caller.\"\"\"\n        encoded = key.encode('utf-8')\n        # Pad to be 8-byte aligned.\n        padded = encoded + (b' ' * (8 - (len(encoded) + 4) % 8))\n        value = struct.pack(f'i{len(padded)}sdd'.encode(), len(encoded), padded, 0.0, 0.0)\n        while self._used + len(value) > self._capacity:\n            self._capacity *= 2\n            self._f.truncate(self._capacity)\n            self._m = mmap.mmap(self._f.fileno(), self._capacity)\n        self._m[self._used:self._used + len(value)] = value\n\n        # Update how much space we've used.\n        self._used += len(value)\n        _pack_integer(self._m, 0, self._used)\n        self._positions[key] = self._used - 16\n\n    def _read_all_values(self):\n        \"\"\"Yield (key, value, pos). No locking is performed.\"\"\"\n        return _read_all_values(data=self._m, used=self._used)\n\n    def read_all_values(self):\n        \"\"\"Yield (key, value, timestamp). No locking is performed.\"\"\"\n        for k, v, ts, _ in self._read_all_values():\n            yield k, v, ts\n\n    def read_value(self, key):\n        if key not in self._positions:\n            self._init_value(key)\n        pos = self._positions[key]\n        return _unpack_two_doubles(self._m, pos)\n\n\n\n# The code to be completed is:\n    def write_value(self, key, value, timestamp):\n\n        \"\"\"\n        This function writes a value to a key in the MmapedDict instance. If the key does not exist in the instance, it initializes the key and then writes the value and timestamp to the corresponding position in the memory-mapped file.\n        Input-Output Arguments\n        :param self: MmapedDict. An instance of the MmapedDict class.\n        :param key: The key to write the value to.\n        :param value: The value to be written.\n        :param timestamp: The timestamp associated with the value.\n        :return: No return values.\n        \"\"\""}
{"namespace": "mrjob.fs.local.LocalFilesystem.mkdir", "prompt": "# Please complete the mkdir function based on the contexts above the function.\n\n# The contexts above the function are:\n# Copyright 2009-2012 Yelp and Contributors\n# Copyright 2015 Yelp\n# Copyright 2017 Yelp\n# Copyright 2019 Yelp\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport glob\nimport hashlib\nimport logging\nimport os\nimport shutil\n\n\nfrom mrjob.fs.base import Filesystem\nfrom mrjob.parse import is_uri\n\n\nlog = logging.getLogger(__name__)\n\n\nclass LocalFilesystem(Filesystem):\n    \"\"\"Filesystem for local files. Typically you will get one of these via\n    ``MRJobRunner().fs``.\n    \"\"\"\n    def can_handle_path(self, path):\n        return path.startswith('file:///') or not is_uri(path)\n\n    def du(self, path_glob):\n        path_glob = _from_file_uri(path_glob)\n        return sum(os.path.getsize(path) for path in self.ls(path_glob))\n\n    def ls(self, path_glob):\n        bare_path_glob = _from_file_uri(path_glob)\n        uri_scheme = path_glob[0:-len(bare_path_glob)]  # 'file:///' or ''\n\n        for path in glob.glob(bare_path_glob):\n            if os.path.isdir(path):\n                for dirname, _, filenames in os.walk(path, followlinks=True):\n                    for filename in filenames:\n                        yield uri_scheme + os.path.join(dirname, filename)\n            else:\n                yield uri_scheme + path\n\n    def _cat_file(self, path):\n        from mrjob.cat import decompress\n        path = _from_file_uri(path)\n        with open(path, 'rb') as f:\n            for chunk in decompress(f, path):\n                yield chunk\n\n    def exists(self, path_glob):\n        path_glob = _from_file_uri(path_glob)\n        return bool(glob.glob(path_glob))\n\n\n\n# The code to be completed is:\n    def mkdir(self, path):\n\n        \"\"\"\n        Create a new directory in the local filesystem. It first converts the input path from a file URI to a local path, and then checks if the directory already exists. If not, it creates the directory.\n        Input-Output Arguments\n        :param self: LocalFilesystem. An instance of the LocalFilesystem class.\n        :param path: String. The path of the directory to be created, in file URI format.\n        :return: No return values.\n        \"\"\""}
{"namespace": "rest_framework.exceptions._get_error_details", "prompt": "# Please complete the _get_error_details function based on the contexts above the function.\n\n# The contexts above the function are:\n\"\"\"\nHandled exceptions raised by REST framework.\n\nIn addition, Django's built in 403 and 404 exceptions are handled.\n(`django.http.Http404` and `django.core.exceptions.PermissionDenied`)\n\"\"\"\nimport math\n\nfrom django.http import JsonResponse\nfrom django.utils.encoding import force_str\nfrom django.utils.translation import gettext_lazy as _\nfrom django.utils.translation import ngettext\n\nfrom rest_framework import status\n\n\n\n\n\n# The code to be completed is:\ndef _get_error_details(data, default_code=None):\n\n    \"\"\"\n    This function takes a nested data structure as input and recursively converts any lazy translation strings or regular strings into `ErrorDetail` objects. It returns the modified data structure with all the strings converted.\n    Input-Output Arguments\n    :param data: Any. The nested data structure to be processed.\n    :param default_code: Any. The default error code to be used if not specified in the data structure.\n    :return: Any. The modified data structure with all the strings converted to `ErrorDetail` objects.\n    \"\"\""}
{"namespace": "fs._ftp_parse.parse", "prompt": "# Please complete the parse function based on the contexts above the function.\n\n# The contexts above the function are:\nfrom __future__ import absolute_import, print_function, unicode_literals\n\nimport re\nimport time\nimport unicodedata\nfrom datetime import datetime\n\ntry:\n    from datetime import timezone\nexcept ImportError:\n    from ._tzcompat import timezone  # type: ignore\n\nfrom .enums import ResourceType\nfrom .permissions import Permissions\n\nEPOCH_DT = datetime.fromtimestamp(0, timezone.utc)\n\n\nRE_LINUX = re.compile(\n    r\"\"\"\n    ^\n    ([-dlpscbD])\n    ([r-][w-][xsS-][r-][w-][xsS-][r-][w-][xtT-][\\.\\+]?)\n    \\s+?\n    (\\d+)\n    \\s+?\n    ([A-Za-z0-9][A-Za-z0-9\\-\\.\\_\\@]*\\$?)\n    \\s+?\n    ([A-Za-z0-9][A-Za-z0-9\\-\\.\\_\\@]*\\$?)\n    \\s+?\n    (\\d+)\n    \\s+?\n    (\\w{3}\\s+\\d{1,2}\\s+[\\w:]+)\n    \\s+\n    (.*?)\n    $\n    \"\"\",\n    re.VERBOSE,\n)\n\n\nRE_WINDOWSNT = re.compile(\n    r\"\"\"\n    ^\n    (?P<modified_date>\\S+)\n    \\s+\n    (?P<modified_time>\\S+(AM|PM)?)\n    \\s+\n    (?P<size>(<DIR>|\\d+))\n    \\s+\n    (?P<name>.*)\n    $\n    \"\"\",\n    re.VERBOSE,\n)\n\n\ndef get_decoders():\n    \"\"\"Return all available FTP LIST line decoders with their matching regexes.\"\"\"\n    decoders = [\n        (RE_LINUX, decode_linux),\n        (RE_WINDOWSNT, decode_windowsnt),\n    ]\n    return decoders\n\n\n\n\n# The code to be completed is:\ndef parse(lines):\n\n    \"\"\"\n    Parse a list of lines and extract information from each line that is not blank.\n\n    Input-Output Arguments\n    :param lines: List[String], the input list of lines to be parsed.\n    :return: List, the list of parsed information extracted from the input lines.\n\n    \"\"\""}
{"namespace": "django.utils._os.to_path", "prompt": "# Please complete the to_path function based on the contexts above the function.\n\n# The contexts above the function are:\nimport os\nimport tempfile\nfrom os.path import abspath, dirname, join, normcase, sep\nfrom pathlib import Path\n\nfrom django.core.exceptions import SuspiciousFileOperation\n\n\ndef safe_join(base, *paths):\n    \"\"\"\n    Join one or more path components to the base path component intelligently.\n    Return a normalized, absolute version of the final path.\n\n    Raise ValueError if the final path isn't located inside of the base path\n    component.\n    \"\"\"\n    final_path = abspath(join(base, *paths))\n    base_path = abspath(base)\n    # Ensure final_path starts with base_path (using normcase to ensure we\n    # don't false-negative on case insensitive operating systems like Windows),\n    # further, one of the following conditions must be true:\n    #  a) The next character is the path separator (to prevent conditions like\n    #     safe_join(\"/dir\", \"/../d\"))\n    #  b) The final path must be the same as the base path.\n    #  c) The base path must be the most root path (meaning either \"/\" or \"C:\\\\\")\n    if (\n        not normcase(final_path).startswith(normcase(base_path + sep))\n        and normcase(final_path) != normcase(base_path)\n        and dirname(normcase(base_path)) != normcase(base_path)\n    ):\n        raise SuspiciousFileOperation(\n            \"The joined path ({}) is located outside of the base path \"\n            \"component ({})\".format(final_path, base_path)\n        )\n    return final_path\n\n\ndef symlinks_supported():\n    \"\"\"\n    Return whether or not creating symlinks are supported in the host platform\n    and/or if they are allowed to be created (e.g. on Windows it requires admin\n    permissions).\n    \"\"\"\n    with tempfile.TemporaryDirectory() as temp_dir:\n        original_path = os.path.join(temp_dir, \"original\")\n        symlink_path = os.path.join(temp_dir, \"symlink\")\n        os.makedirs(original_path)\n        try:\n            os.symlink(original_path, symlink_path)\n            supported = True\n        except (OSError, NotImplementedError):\n            supported = False\n        return supported\n\n\n\n\n# The code to be completed is:\ndef to_path(value):\n\n    \"\"\"\n    Convert the input value to a pathlib.Path instance if it is not already a Path. If the input value is a string, it creates a Path instance with the string value.\n    Input-Output Arguments\n    :param value: Any. The value to be converted to a Path instance.\n    :return: Path. The converted Path instance.\n    \"\"\""}
{"namespace": "ydata_profiling.report.presentation.flavours.html.image.HTMLImage.render", "prompt": "# Please complete the render function based on the contexts above the function.\n\n# The contexts above the function are:\nfrom ydata_profiling.report.presentation.core import Image\n\n\n\nclass HTMLImage(Image):\n\n\n# The code to be completed is:\n    def render(self) -> str:\n\n        \"\"\"\n        Render the HTML content of an image. It uses a template file called \"diagram.html\" and passes the content of the image as arguments to the template.\n        Input-Output Arguments\n        :param self: HTMLImage. An instance of the HTMLImage class.\n        :return: str. The rendered HTML content of the image.\n        \"\"\""}
{"namespace": "sqlite_utils.utils.column_affinity", "prompt": "# Please complete the column_affinity function based on the contexts above the function.\n\n# The contexts above the function are:\nimport base64\nimport contextlib\nimport csv\nimport enum\nimport hashlib\nimport io\nimport itertools\nimport json\nimport os\nimport sys\nfrom . import recipes\nfrom typing import Dict, cast, BinaryIO, Iterable, Optional, Tuple, Type\n\nimport click\n\ntry:\n    import pysqlite3 as sqlite3  # noqa: F401\n    from pysqlite3 import dbapi2  # noqa: F401\n\n    OperationalError = dbapi2.OperationalError\nexcept ImportError:\n    try:\n        import sqlean as sqlite3  # noqa: F401\n        from sqlean import dbapi2  # noqa: F401\n\n        OperationalError = dbapi2.OperationalError\n    except ImportError:\n        import sqlite3  # noqa: F401\n        from sqlite3 import dbapi2  # noqa: F401\n\n        OperationalError = dbapi2.OperationalError\n\n\nSPATIALITE_PATHS = (\n    \"/usr/lib/x86_64-linux-gnu/mod_spatialite.so\",\n    \"/usr/local/lib/mod_spatialite.dylib\",\n    \"/usr/local/lib/mod_spatialite.so\",\n    \"/opt/homebrew/lib/mod_spatialite.dylib\",\n)\n\n# Mainly so we can restore it if needed in the tests:\nORIGINAL_CSV_FIELD_SIZE_LIMIT = csv.field_size_limit()\n\n\ndef maximize_csv_field_size_limit():\n    \"\"\"\n    Increase the CSV field size limit to the maximum possible.\n    \"\"\"\n    # https://stackoverflow.com/a/15063941\n    field_size_limit = sys.maxsize\n\n    while True:\n        try:\n            csv.field_size_limit(field_size_limit)\n            break\n        except OverflowError:\n            field_size_limit = int(field_size_limit / 10)\n\n\ndef find_spatialite() -> Optional[str]:\n    \"\"\"\n    The ``find_spatialite()`` function searches for the `SpatiaLite <https://www.gaia-gis.it/fossil/libspatialite/index>`__\n    SQLite extension in some common places. It returns a string path to the location, or ``None`` if SpatiaLite was not found.\n\n    You can use it in code like this:\n\n    .. code-block:: python\n\n        from sqlite_utils import Database\n        from sqlite_utils.utils import find_spatialite\n\n        db = Database(\"mydb.db\")\n        spatialite = find_spatialite()\n        if spatialite:\n            db.conn.enable_load_extension(True)\n            db.conn.load_extension(spatialite)\n\n        # or use with db.init_spatialite like this\n        db.init_spatialite(find_spatialite())\n\n    \"\"\"\n    for path in SPATIALITE_PATHS:\n        if os.path.exists(path):\n            return path\n    return None\n\n\ndef suggest_column_types(records):\n    all_column_types = {}\n    for record in records:\n        for key, value in record.items():\n            all_column_types.setdefault(key, set()).add(type(value))\n    return types_for_column_types(all_column_types)\n\n\ndef types_for_column_types(all_column_types):\n    column_types = {}\n    for key, types in all_column_types.items():\n        # Ignore null values if at least one other type present:\n        if len(types) > 1:\n            types.discard(None.__class__)\n        if {None.__class__} == types:\n            t = str\n        elif len(types) == 1:\n            t = list(types)[0]\n            # But if it's a subclass of list / tuple / dict, use str\n            # instead as we will be storing it as JSON in the table\n            for superclass in (list, tuple, dict):\n                if issubclass(t, superclass):\n                    t = str\n        elif {int, bool}.issuperset(types):\n            t = int\n        elif {int, float, bool}.issuperset(types):\n            t = float\n        elif {bytes, str}.issuperset(types):\n            t = bytes\n        else:\n            t = str\n        column_types[key] = t\n    return column_types\n\n\n\n\n# The code to be completed is:\ndef column_affinity(column_type):\n    # Implementation of SQLite affinity rules from\n    # https://www.sqlite.org/datatype3.html#determination_of_column_affinity\n\n    \"\"\"\n    This function returns the affinity of the given column type based on SQLite affinity rules including \"INT\", \"CHAR\", \"CLOB\", \"TEXT\", \"BLOB\", \"REAL\", \"FLOA\", \"DOUB\".\n    Input-Output Arguments\n    :param column_type: str. The type of the column.\n    :return: The affinity of the given column type.\n    \"\"\""}
{"namespace": "aiohappybase._util.bytes_increment", "prompt": "# Please complete the bytes_increment function based on the contexts above the function.\n\n# The contexts above the function are:\n\"\"\"\nAIOHappyBase utility module.\n\nThese functions are not part of the public API.\n\"\"\"\n\nimport re\nfrom typing import Dict, List, Any, AnyStr, Optional, TypeVar, Callable\n\nT = TypeVar('T')\n\nKTI = TypeVar('KTI')\nVTI = TypeVar('VTI')\n\nKTO = TypeVar('KTO')\nVTO = TypeVar('VTO')\n\nCAPITALS = re.compile('([A-Z])')\n\n\ndef camel_case_to_pep8(name: str) -> str:\n    \"\"\"Convert a camel cased name to PEP8 style.\"\"\"\n    converted = CAPITALS.sub(lambda m: '_' + m.groups()[0].lower(), name)\n    return converted[1:] if converted[0] == '_' else converted\n\n\ndef pep8_to_camel_case(name: str, initial: bool = False) -> str:\n    \"\"\"Convert a PEP8 style name to camel case.\"\"\"\n    chunks = name.split('_')\n    converted = [s.capitalize() for s in chunks]\n    if initial:\n        return ''.join(converted)\n    else:\n        return chunks[0].lower() + ''.join(converted[1:])\n\n\ndef thrift_attrs(obj_or_cls) -> List[str]:\n    \"\"\"Obtain Thrift data type attribute names for an instance or class.\"\"\"\n    return [v[1] for v in obj_or_cls.thrift_spec.values()]\n\n\ndef thrift_type_to_dict(obj: Any) -> Dict[bytes, Any]:\n    \"\"\"Convert a Thrift data type to a regular dictionary.\"\"\"\n    return {\n        camel_case_to_pep8(attr): getattr(obj, attr)\n        for attr in thrift_attrs(obj)\n    }\n\n\ndef ensure_bytes(value: AnyStr) -> bytes:\n    \"\"\"Convert text into bytes, and leaves bytes as-is.\"\"\"\n    if isinstance(value, bytes):\n        return value \n    if isinstance(value, str):\n        return value.encode('utf-8')\n    raise TypeError(\n        f\"input must be a text or byte string, got {type(value).__name__}\"\n    )\n\n\n\n\n# The code to be completed is:\ndef bytes_increment(b: bytes) -> Optional[bytes]:\n\n    \"\"\"\n    This function increments and truncates a byte string for sorting purposes. It returns the shortest string that sorts after the given string when compared using regular string comparison semantics. It increments the last byte that is smaller than 0xFF and drops everything after it. If the input string only contains 0xFF bytes, None is returned.\n    Input-Output Arguments\n    :param b: bytes. The byte string to be incremented and truncated.\n    :return: Optional[bytes]. The incremented and truncated byte string. If the string only contains ``0xFF`` bytes, `None` is returned.\n    \"\"\""}
{"namespace": "asyncssh.kex.register_kex_alg", "prompt": "# Please complete the register_kex_alg function based on the contexts above the function.\n\n# The contexts above the function are:\n# Copyright (c) 2013-2021 by Ron Frederick <ronf@timeheart.net> and others.\n#\n# This program and the accompanying materials are made available under\n# the terms of the Eclipse Public License v2.0 which accompanies this\n# distribution and is available at:\n#\n#     http://www.eclipse.org/legal/epl-2.0/\n#\n# This program may also be made available under the following secondary\n# licenses when the conditions for such availability set forth in the\n# Eclipse Public License v2.0 are satisfied:\n#\n#    GNU General Public License, Version 2.0, or any later versions of\n#    that license\n#\n# SPDX-License-Identifier: EPL-2.0 OR GPL-2.0-or-later\n#\n# Contributors:\n#     Ron Frederick - initial implementation, API, and documentation\n\n\"\"\"SSH key exchange handlers\"\"\"\n\nimport binascii\nfrom hashlib import md5\nfrom typing import TYPE_CHECKING, Dict, List, Sequence, Tuple, Type\n\nfrom .logging import SSHLogger\nfrom .misc import HashType\nfrom .packet import SSHPacketHandler\n\n\nif TYPE_CHECKING:\n    # pylint: disable=cyclic-import\n    from .connection import SSHConnection\n\n\n_KexAlgList = List[bytes]\n_KexAlgMap = Dict[bytes, Tuple[Type['Kex'], HashType, object]]\n\n\n_kex_algs: _KexAlgList = []\n_default_kex_algs:_KexAlgList = []\n_kex_handlers: _KexAlgMap = {}\n\n_gss_kex_algs: _KexAlgList = []\n_default_gss_kex_algs: _KexAlgList = []\n_gss_kex_handlers: _KexAlgMap = {}\n\n\nclass Kex(SSHPacketHandler):\n    \"\"\"Parent class for key exchange handlers\"\"\"\n\n    def __init__(self, alg: bytes, conn: 'SSHConnection', hash_alg: HashType):\n        self.algorithm = alg\n\n        self._conn = conn\n        self._logger = conn.logger\n        self._hash_alg = hash_alg\n\n\n    def start(self) -> None:\n        \"\"\"Start key exchange\"\"\"\n\n        raise NotImplementedError\n\n    def send_packet(self, pkttype: int, *args: bytes) -> None:\n        \"\"\"Send a kex packet\"\"\"\n\n        self._conn.send_packet(pkttype, *args, handler=self)\n\n    @property\n    def logger(self) -> SSHLogger:\n        \"\"\"A logger associated with this connection\"\"\"\n\n        return self._logger\n\n    def compute_key(self, k: bytes, h: bytes, x: bytes,\n                    session_id: bytes, keylen: int) -> bytes:\n        \"\"\"Compute keys from output of key exchange\"\"\"\n\n        key = b''\n        while len(key) < keylen:\n            hash_obj = self._hash_alg()\n            hash_obj.update(k)\n            hash_obj.update(h)\n            hash_obj.update(key if key else x + session_id)\n            key += hash_obj.digest()\n\n        return key[:keylen]\n\n\n\n\n# The code to be completed is:\ndef register_kex_alg(alg: bytes, handler: Type[Kex], hash_alg: HashType,\n                     args: Tuple, default: bool) -> None:\n\n    \"\"\"\n    This function is used to register a key exchange algorithm. It adds the algorithm to the list of supported key exchange algorithms, and if specified as default, adds it to the list of default key exchange algorithms. It also associates the algorithm with its corresponding handler, hash algorithm, and arguments.\n    Input-Output Arguments\n    :param alg: bytes. The key exchange algorithm to register.\n    :param handler: Type[Kex]. The handler class for the key exchange algorithm.\n    :param hash_alg: HashType. The hash algorithm to be used with the key exchange algorithm.\n    :param args: Tuple. Additional arguments required for the key exchange algorithm.\n    :param default: bool. Whether the key exchange algorithm should be set as the default.\n    :return: No return values.\n    \"\"\""}
{"namespace": "boltons.mathutils.clamp", "prompt": "# Please complete the clamp function based on the contexts above the function.\n\n# The contexts above the function are:\n# -*- coding: utf-8 -*-\n\n# Copyright (c) 2013, Mahmoud Hashemi\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are\n# met:\n#\n#    * Redistributions of source code must retain the above copyright\n#      notice, this list of conditions and the following disclaimer.\n#\n#    * Redistributions in binary form must reproduce the above\n#      copyright notice, this list of conditions and the following\n#      disclaimer in the documentation and/or other materials provided\n#      with the distribution.\n#\n#    * The names of the contributors may not be used to endorse or\n#      promote products derived from this software without specific\n#      prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\n# \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\n# LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR\n# A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT\n# OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,\n# SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT\n# LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,\n# DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY\n# THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n# (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\n\"\"\"This module provides useful math functions on top of Python's\nbuilt-in :mod:`math` module.\n\"\"\"\nfrom __future__ import division\n\nfrom math import ceil as _ceil, floor as _floor\nimport bisect\nimport binascii\n\n\n\n\n# The code to be completed is:\ndef clamp(x, lower=float('-inf'), upper=float('inf')):\n\n    \"\"\"\n    Limit a value to a given range. The function takes a number and limits it to a given range. The returned value is guaranteed to be between the lower and upper bounds.\n    Input-Output Arguments\n    :param x: int or float. Number to be clamped.\n    :param lower: int or float. Minimum value for x. Defaults to negative infinity.\n    :param upper: int or float. Maximum value for x. Defaults to positive infinity.\n    :return: int or float. The clamped value.\n    The returned value is guaranteed to be between *lower* and *upper*. Integers, floats, and other comparable types can be mixed.\n    The function also provides examples of how to use it.\n    \"\"\""}
{"namespace": "mackup.utils.delete", "prompt": "# Please complete the delete function based on the contexts above the function.\n\n# The contexts above the function are:\n\"\"\"System static utilities being used by the modules.\"\"\"\nimport base64\nimport os\nimport platform\nimport shutil\nimport stat\nimport subprocess\nimport sys\nimport sqlite3\nfrom six.moves import input\n\nfrom . import constants\n\n\n# Flag that controls how user confirmation works.\n# If True, the user wants to say \"yes\" to everything.\nFORCE_YES = False\n\n# Flag that control if mackup can be run as root\nCAN_RUN_AS_ROOT = False\n\n\ndef confirm(question):\n    \"\"\"\n    Ask the user if he really wants something to happen.\n\n    Args:\n        question(str): What can happen\n\n    Returns:\n        (boolean): Confirmed or not\n    \"\"\"\n    if FORCE_YES:\n        return True\n\n    while True:\n        answer = input(question + \" <Yes|No> \").lower()\n\n        if answer == \"yes\" or answer == \"y\":\n            confirmed = True\n            break\n        if answer == \"no\" or answer == \"n\":\n            confirmed = False\n            break\n\n    return confirmed\n\n\n\n\n# The code to be completed is:\ndef delete(filepath):\n\n    \"\"\"\n    This function deletes the given file, directory, or link. It first removes any ACLs (Access Control Lists) associated with the file, then removes any immutable attributes. Finally, it deletes the file or directory using the appropriate method based on its type.\n    Input-Output Arguments\n    :param filepath: str. The absolute full path to the file, directory, or link to be deleted.\n    :return: No return values.\n    \"\"\""}
{"namespace": "datasets.utils.sharding._number_of_shards_in_gen_kwargs", "prompt": "# Please complete the _number_of_shards_in_gen_kwargs function based on the contexts above the function.\n\n# The contexts above the function are:\nfrom typing import List\n\nimport numpy as np\n\n\n\n\n# The code to be completed is:\ndef _number_of_shards_in_gen_kwargs(gen_kwargs: dict) -> int:\n\n    \"\"\"\n    This function returns the number of possible shards according to the input gen_kwargs. It checks the length of the lists in the input dictionary and raises an error if the lengths are different.\n    Input-Output Arguments\n    :param gen_kwargs: dict. The input dictionary containing the gen_kwargs.\n    :return: int. The number of possible shards.\n    \"\"\""}
{"namespace": "arctic.store.versioned_item.VersionedItem.__str__", "prompt": "# Please complete the __str__ function based on the contexts above the function.\n\n# The contexts above the function are:\nfrom collections import namedtuple\n\n\nclass VersionedItem(namedtuple('VersionedItem', ['symbol', 'library', 'data', 'version', 'metadata', 'host'])):\n    \"\"\"\n    Class representing a Versioned object in VersionStore.\n    \"\"\"\n\n    def __new__(cls, symbol, library, data, version, metadata, host=None):\n        return super(VersionedItem, cls).__new__(cls, symbol, library, data, version, metadata, host)\n\n    def metadata_dict(self):\n        return {'symbol': self.symbol, 'library': self.library, 'version': self.version}\n\n    def __repr__(self):\n        return str(self)\n\n\n\n# The code to be completed is:\n    def __str__(self):\n\n        \"\"\"\n        Return a string representation of the VersionedItem instance in the format \"VersionedItem(symbol={symbol},library={library},data={data},version={version},metadata={metadata},host={host})\".\n        Input-Output Arguments\n        :param self: VersionedItem. An instance of the VersionedItem class.\n        :return: String. The string representation of the VersionedItem instance.\n        \"\"\""}
{"namespace": "bentoml._internal.utils.analytics.usage_stats._track_serve_init", "prompt": "# Please complete the _track_serve_init function based on the contexts above the function.\n\n# The contexts above the function are:\nfrom __future__ import annotations\n\nimport contextlib\nimport logging\nimport os\nimport secrets\nimport threading\nimport typing as t\nfrom datetime import datetime\nfrom datetime import timezone\nfrom functools import lru_cache\nfrom functools import wraps\nfrom typing import TYPE_CHECKING\n\nimport attr\nimport httpx\nfrom simple_di import Provide\nfrom simple_di import inject\n\nfrom ...configuration import get_debug_mode\nfrom ...configuration.containers import BentoMLContainer\nfrom ...utils import compose\nfrom .schemas import CommonProperties\nfrom .schemas import EventMeta\nfrom .schemas import ServeInitEvent\nfrom .schemas import ServeUpdateEvent\nfrom .schemas import TrackingPayload\n\nif TYPE_CHECKING:\n    P = t.ParamSpec(\"P\")\n    T = t.TypeVar(\"T\")\n    AsyncFunc = t.Callable[P, t.Coroutine[t.Any, t.Any, t.Any]]\n\n    from prometheus_client.samples import Sample\n\n    from bentoml import Service\n\n    from ...server.metrics.prometheus import PrometheusClient\n\nlogger = logging.getLogger(__name__)\n\nBENTOML_DO_NOT_TRACK = \"BENTOML_DO_NOT_TRACK\"\nBENTOML_SERVE_FROM_SERVER_API = \"__BENTOML_SERVE_FROM_SERVER_API\"\nUSAGE_TRACKING_URL = \"https://t.bentoml.com\"\nSERVE_USAGE_TRACKING_INTERVAL_SECONDS = int(12 * 60 * 60)  # every 12 hours\nUSAGE_REQUEST_TIMEOUT_SECONDS = 1\n\n\n@lru_cache(maxsize=None)\ndef _bentoml_serve_from_server_api() -> bool:\n    return os.environ.get(BENTOML_SERVE_FROM_SERVER_API, str(False)).lower() == \"true\"\n\n\n@lru_cache(maxsize=1)\ndef do_not_track() -> bool:  # pragma: no cover\n    # Returns True if and only if the environment variable is defined and has value True.\n    # The function is cached for better performance.\n    return os.environ.get(BENTOML_DO_NOT_TRACK, str(False)).lower() == \"true\"\n\n\n@lru_cache(maxsize=1)\ndef _usage_event_debugging() -> bool:\n    # For BentoML developers only - debug and print event payload if turned on\n    return os.environ.get(\"__BENTOML_DEBUG_USAGE\", str(False)).lower() == \"true\"\n\n\ndef silent(func: t.Callable[P, T]) -> t.Callable[P, T]:  # pragma: no cover\n    # Silent errors when tracking\n    @wraps(func)\n    def wrapper(*args: P.args, **kwargs: P.kwargs) -> t.Any:\n        try:\n            return func(*args, **kwargs)\n        except Exception as err:  # pylint: disable=broad-except\n            if _usage_event_debugging():\n                if get_debug_mode():\n                    logger.error(\n                        \"Tracking Error: %s\", err, stack_info=True, stacklevel=3\n                    )\n                else:\n                    logger.info(\"Tracking Error: %s\", err)\n            else:\n                logger.debug(\"Tracking Error: %s\", err)\n\n    return wrapper\n\n\n@attr.define\nclass ServeInfo:\n    serve_id: str\n    serve_started_timestamp: datetime\n\n\ndef get_serve_info() -> ServeInfo:  # pragma: no cover\n    # Returns a safe token for serve as well as timestamp of creating this token\n    return ServeInfo(\n        serve_id=secrets.token_urlsafe(32),\n        serve_started_timestamp=datetime.now(timezone.utc),\n    )\n\n\n@inject\ndef get_payload(\n    event_properties: EventMeta,\n    session_id: str = Provide[BentoMLContainer.session_id],\n) -> t.Dict[str, t.Any]:\n    return TrackingPayload(\n        session_id=session_id,\n        common_properties=CommonProperties(),\n        event_properties=event_properties,\n        event_type=event_properties.event_name,\n    ).to_dict()\n\n\n@silent\ndef track(event_properties: EventMeta):\n    if do_not_track():\n        return\n    payload = get_payload(event_properties=event_properties)\n\n    if _usage_event_debugging():\n        # For internal debugging purpose\n        logger.info(\"Tracking Payload: %s\", payload)\n        return\n\n    httpx.post(USAGE_TRACKING_URL, json=payload, timeout=USAGE_REQUEST_TIMEOUT_SECONDS)\n\n\n@inject\n\n\n# The code to be completed is:\ndef _track_serve_init(\n    svc: Service,\n    production: bool,\n    serve_kind: str,\n    from_server_api: bool,\n    serve_info: ServeInfo = Provide[BentoMLContainer.serve_info],\n):\n\n    \"\"\"\n    This function tracks the initialization of a service and sends an event with relevant information. It creates an instance of a serve init event with information about the service, such as the serve ID, whether it is served from a BentoML container or a server API, production status, serve kind, creation timestamp of the BentoML container (if applicable), number of models, runners, and APIs in the service, and the types of models, runners, API inputs, and API outputs.\n    Input-Output Arguments\n    :param svc: Service. The service instance being initialized.\n    :param production: Bool. Whether the service is in production mode.\n    :param serve_kind: String. The kind of serve being initialized.\n    :param from_server_api: Bool. Whether the serve is from a server API.\n    :param serve_info: ServeInfo. The serve information obtained from the BentoML container. Defaults to the serve_info provided by the BentoML container.\n    :return: No return values.\n    \"\"\""}
{"namespace": "hypertools._shared.helpers.parse_kwargs", "prompt": "# Please complete the parse_kwargs function based on the contexts above the function.\n\n# The contexts above the function are:\n#!/usr/bin/env python\n\n\"\"\"\nHelper functions\n\"\"\"\n\n##PACKAGES##\nimport functools\nimport sys\nimport numpy as np\nimport copy\nfrom scipy.interpolate import PchipInterpolator as pchip\nimport seaborn as sns\nimport itertools\nimport pandas as pd\nfrom matplotlib.lines import Line2D\nnp.seterr(divide='ignore', invalid='ignore')\n\n\ndef center(x):\n    assert type(x) is list, \"Input data to center must be list\"\n    x_stacked = np.vstack(x)\n    return [i - np.mean(x_stacked, 0) for i in x]\n\n\ndef scale(x):\n    assert type(x) is list, \"Input data to scale must be list\"\n    x_stacked = np.vstack(x)\n    m1 = np.min(x_stacked)\n    m2 = np.max(x_stacked - m1)\n    f = lambda x: 2*(np.divide(x - m1, m2)) - 1\n    return [f(i) for i in x]\n\n\ndef group_by_category(vals):\n    if any(isinstance(el, list) for el in vals):\n        vals = list(itertools.chain(*vals))\n    val_set = list(sorted(set(vals), key=list(vals).index))\n    return [val_set.index(val) for val in vals]\n\n\ndef vals2colors(vals, cmap='GnBu',res=100):\n    \"\"\"Maps values to colors\n    Args:\n    values (list or list of lists) - list of values to map to colors\n    cmap (str) - color map (default is 'GnBu')\n    res (int) - resolution of the color map (default: 100)\n    Returns:\n    list of rgb tuples\n    \"\"\"\n    # flatten if list of lists\n    if any(isinstance(el, list) for el in vals):\n        vals = list(itertools.chain(*vals))\n\n    # get palette from seaborn\n    palette = np.array(sns.color_palette(cmap, res))\n    ranks = np.digitize(vals, np.linspace(np.min(vals), np.max(vals)+1, res+1)) - 1\n    return [tuple(i) for i in palette[ranks, :]]\n\n\ndef vals2bins(vals,res=100):\n    \"\"\"Maps values to bins\n    Args:\n    values (list or list of lists) - list of values to map to colors\n    res (int) - resolution of the color map (default: 100)\n    Returns:\n    list of numbers representing bins\n    \"\"\"\n    # flatten if list of lists\n    if any(isinstance(el, list) for el in vals):\n        vals = list(itertools.chain(*vals))\n    return list(np.digitize(vals, np.linspace(np.min(vals), np.max(vals)+1, res+1)) - 1)\n\n\ndef interp_array(arr,interp_val=10):\n    x=np.arange(0, len(arr), 1)\n    xx=np.arange(0, len(arr)-1, 1/interp_val)\n    q=pchip(x,arr)\n    return q(xx)\n\n\ndef interp_array_list(arr_list,interp_val=10):\n    smoothed= [np.zeros(arr_list[0].shape) for item in arr_list]\n    for idx,arr in enumerate(arr_list):\n        smoothed[idx] = interp_array(arr,interp_val)\n    return smoothed\n\n\ndef parse_args(x,args):\n    args_list = []\n    for i,item in enumerate(x):\n        tmp = []\n        for ii, arg in enumerate(args):\n            if isinstance(arg, (tuple, list)):\n                if len(arg) == len(x):\n                    tmp.append(arg[i])\n                else:\n                    print('Error: arguments must be a list of the same length as x')\n                    sys.exit(1)\n            else:\n                tmp.append(arg)\n        args_list.append(tuple(tmp))\n    return args_list\n\n\n\n\n# The code to be completed is:\ndef parse_kwargs(x, kwargs):\n\n    \"\"\"\n    This function creates a list of dictionaries based on the input list and the input keyword arguments. It iterates through the input list and creates a dictionary for each item in the list using the keyword arguments.\n    Input-Output Arguments\n    :param x: List. The input list.\n    :param kwargs: Dictionary. The keyword arguments.\n    :return: List of Dictionary. A list of dictionaries created based on the input list and keyword arguments.\n    \"\"\""}
{"namespace": "pycoin.crack.bip32.crack_bip32", "prompt": "# Please complete the crack_bip32 function based on the contexts above the function.\n\n# The contexts above the function are:\nimport hmac\nimport hashlib\nimport struct\n\nfrom pycoin.encoding.bytes32 import from_bytes_32\nfrom pycoin.encoding.sec import public_pair_to_sec\n\n\ndef ascend_bip32(bip32_pub_node, secret_exponent, child):\n    \"\"\"\n    Given a BIP32Node with public derivation child \"child\" with a known private key,\n    return the secret exponent for the bip32_pub_node.\n    \"\"\"\n    i_as_bytes = struct.pack(\">l\", child)\n    sec = public_pair_to_sec(bip32_pub_node.public_pair(), compressed=True)\n    data = sec + i_as_bytes\n    I64 = hmac.HMAC(key=bip32_pub_node._chain_code, msg=data, digestmod=hashlib.sha512).digest()\n    I_left_as_exponent = from_bytes_32(I64[:32])\n    return (secret_exponent - I_left_as_exponent) % bip32_pub_node._generator.order()\n\n\n\n\n# The code to be completed is:\ndef crack_bip32(bip32_pub_node, secret_exponent, path):\n\n    \"\"\"\n    This function cracks a BIP32 public node by iterating through a given path and updating the secret exponent. It returns a new BIP32 public node with the updated secret exponent.\n    Input-Output Arguments\n    :param bip32_pub_node: BIP32PublicNode. The BIP32 public node to crack.\n    :param secret_exponent: int. The secret exponent to update.\n    :param path: str. The path to iterate through.\n    :return: BIP32PublicNode. The new BIP32 public node with the updated secret exponent.\n    \"\"\""}
{"namespace": "csvkit.cli.CSVKitUtility.run", "prompt": "# Please complete the run function based on the contexts above the function.\n\n# The contexts above the function are:\n#!/usr/bin/env python\n\nimport argparse\nimport bz2\nimport datetime\nimport decimal\nimport gzip\nimport itertools\nimport lzma\nimport sys\nimport warnings\nfrom os.path import splitext\n\nimport agate\nfrom agate.data_types.base import DEFAULT_NULL_VALUES\n\nfrom csvkit.exceptions import ColumnIdentifierError, RequiredHeaderError\n\n\nclass LazyFile:\n    \"\"\"\n    A proxy for a File object that delays opening it until\n    a read method is called.\n\n    Currently this implements only the minimum methods to be useful,\n    but it could easily be expanded.\n    \"\"\"\n\n    def __init__(self, init, *args, **kwargs):\n        self.init = init\n        self.f = None\n        self._is_lazy_opened = False\n\n        self._lazy_args = args\n        self._lazy_kwargs = kwargs\n\n    def __getattr__(self, name):\n        self._open()\n        return getattr(self.f, name)\n\n    def __iter__(self):\n        return self\n\n    def close(self):\n        if self._is_lazy_opened:\n            self.f.close()\n            self.f = None\n            self._is_lazy_opened = False\n\n    def __next__(self):\n        self._open()\n        return next(self.f).replace('\\0', '')\n\n    def _open(self):\n        if not self._is_lazy_opened:\n            self.f = self.init(*self._lazy_args, **self._lazy_kwargs)\n            self._is_lazy_opened = True\n\n\nclass CSVKitUtility:\n    description = ''\n    epilog = ''\n    override_flags = ''\n\n    def __init__(self, args=None, output_file=None):\n        \"\"\"\n        Perform argument processing and other setup for a CSVKitUtility.\n        \"\"\"\n        self._init_common_parser()\n        self.add_arguments()\n        self.args = self.argparser.parse_args(args)\n        # Output file is only set during testing.\n        if output_file is None:\n            self.output_file = sys.stdout\n        else:\n            self.output_file = output_file\n\n        self.reader_kwargs = self._extract_csv_reader_kwargs()\n        self.writer_kwargs = self._extract_csv_writer_kwargs()\n\n        self._install_exception_handler()\n\n        # Ensure SIGPIPE doesn't throw an exception\n        # Prevents [Errno 32] Broken pipe errors, e.g. when piping to 'head'\n        # To test from the shell:\n        #  python -c \"for i in range(5000): print('a,b,c')\" | csvlook | head\n        # Without this fix you will see at the end:\n        #  [Errno 32] Broken pipe\n        # With this fix, there should be no error\n        # For details on Python and SIGPIPE, see https://bugs.python.org/issue1652\n        try:\n            import signal\n            signal.signal(signal.SIGPIPE, signal.SIG_DFL)\n        except (ImportError, AttributeError):\n            # Do nothing on platforms that don't have signals or don't have SIGPIPE\n            pass\n\n    def add_arguments(self):\n        \"\"\"\n        Called upon initialization once the parser for common arguments has been constructed.\n\n        Should be overriden by individual utilities.\n        \"\"\"\n        raise NotImplementedError('add_arguments must be provided by each subclass of CSVKitUtility.')\n\n\n\n# The code to be completed is:\n    def run(self):\n\n        \"\"\"\n        This function is a wrapper around the main loop of a utility. It handles opening and closing files. It first checks if the 'f' flag is not present in the override flags. If not present, it opens the input file. Then, it executes the main loop of the utility, ignoring warnings related to column names if the 'no_header_row' option is present. Finally, it closes the input file if the 'f' flag is not present in the override flags.\n        Input-Output Arguments\n        :param self: CSVKitUtility. An instance of the CSVKitUtility class.\n        :return: No return values.\n        \"\"\""}
{"namespace": "pyramid.csrf.SessionCSRFStoragePolicy.get_csrf_token", "prompt": "# Please complete the get_csrf_token function based on the contexts above the function.\n\n# The contexts above the function are:\nfrom urllib.parse import urlparse\nimport uuid\nfrom webob.cookies import CookieProfile\nfrom zope.interface import implementer\n\nfrom pyramid.exceptions import BadCSRFOrigin, BadCSRFToken\nfrom pyramid.interfaces import ICSRFStoragePolicy\nfrom pyramid.settings import aslist\nfrom pyramid.util import (\n    SimpleSerializer,\n    bytes_,\n    is_same_domain,\n    strings_differ,\n    text_,\n)\n\n\n@implementer(ICSRFStoragePolicy)\nclass LegacySessionCSRFStoragePolicy:\n    \"\"\"A CSRF storage policy that defers control of CSRF storage to the\n    session.\n\n    This policy maintains compatibility with legacy ISession implementations\n    that know how to manage CSRF tokens themselves via\n    ``ISession.new_csrf_token`` and ``ISession.get_csrf_token``.\n\n    Note that using this CSRF implementation requires that\n    a :term:`session factory` is configured.\n\n    .. versionadded:: 1.9\n\n    \"\"\"\n\n    def new_csrf_token(self, request):\n        \"\"\"Sets a new CSRF token into the session and returns it.\"\"\"\n        return request.session.new_csrf_token()\n\n    def get_csrf_token(self, request):\n        \"\"\"Returns the currently active CSRF token from the session,\n        generating a new one if needed.\"\"\"\n        return request.session.get_csrf_token()\n\n    def check_csrf_token(self, request, supplied_token):\n        \"\"\"Returns ``True`` if the ``supplied_token`` is valid.\"\"\"\n        expected_token = self.get_csrf_token(request)\n        return not strings_differ(\n            bytes_(expected_token), bytes_(supplied_token)\n        )\n\n\n@implementer(ICSRFStoragePolicy)\nclass SessionCSRFStoragePolicy:\n    \"\"\"A CSRF storage policy that persists the CSRF token in the session.\n\n    Note that using this CSRF implementation requires that\n    a :term:`session factory` is configured.\n\n    ``key``\n\n        The session key where the CSRF token will be stored.\n        Default: `_csrft_`.\n\n    .. versionadded:: 1.9\n\n    \"\"\"\n\n    _token_factory = staticmethod(lambda: text_(uuid.uuid4().hex))\n\n    def __init__(self, key='_csrft_'):\n        self.key = key\n\n    def new_csrf_token(self, request):\n        \"\"\"Sets a new CSRF token into the session and returns it.\"\"\"\n        token = self._token_factory()\n        request.session[self.key] = token\n        return token\n\n\n\n# The code to be completed is:\n    def get_csrf_token(self, request):\n\n        \"\"\"\n        This function retrieves the currently active CSRF token from the session. If the token is not found in the session, a new one is generated and returned.\n        Input-Output Arguments\n        :param self: SessionCSRFStoragePolicy. An instance of the SessionCSRFStoragePolicy class.\n        :param request: The request object.\n        :return: The CSRF token from the session.\n        \"\"\""}
{"namespace": "trailscraper.iam.known_iam_actions", "prompt": "# Please complete the known_iam_actions function based on the contexts above the function.\n\n# The contexts above the function are:\n\"\"\"Classes to deal with IAM Policies\"\"\"\nimport json\nimport os\n\nimport re\n\nimport six\nfrom toolz import pipe\nfrom toolz.curried import groupby as groupbyz\nfrom toolz.curried import map as mapz\n\nBASE_ACTION_PREFIXES = [\"Describe\", \"Create\", \"Delete\", \"Update\", \"Detach\", \"Attach\", \"List\", \"Put\", \"Get\", ]\n\n# pylint: disable=invalid-name\nclass BaseElement:\n    \"\"\"Base Class for all IAM Policy classes\"\"\"\n\n    def json_repr(self):\n        \"\"\"JSON representation of the class\"\"\"\n        raise NotImplementedError\n\n    def __eq__(self, other):\n        if isinstance(other, self.__class__):\n            return self.json_repr() == other.json_repr()\n\n        return False\n\n    def __ne__(self, other):\n        return not self == other\n\n    def __hash__(self):\n        return hash(self.json_repr())\n\n    def __repr__(self):\n        return str(self.json_repr())\n\n\nclass Action(BaseElement):\n    \"\"\"Action in an IAM Policy.\"\"\"\n\n    def __init__(self, prefix, action):\n        self.action = action\n        self.prefix = prefix\n\n    def json_repr(self):\n        return ':'.join([self.prefix, self.action])\n\n    def _base_action(self):\n        without_prefix = self.action\n        for prefix in BASE_ACTION_PREFIXES:\n            without_prefix = re.sub(prefix, \"\", without_prefix)\n\n        without_plural = re.sub(r\"s$\", \"\", without_prefix)\n\n        return without_plural\n\n    def matching_actions(self, allowed_prefixes):\n        \"\"\"Return a matching create action for this Action\"\"\"\n\n        if not allowed_prefixes:\n            allowed_prefixes = BASE_ACTION_PREFIXES\n\n        potential_matches = [Action(prefix=self.prefix, action=action_prefix + self._base_action())\n                             for action_prefix in allowed_prefixes]\n\n        potential_matches += [Action(prefix=self.prefix, action=action_prefix + self._base_action() + \"s\")\n                              for action_prefix in allowed_prefixes]\n\n        return [potential_match\n                for potential_match in potential_matches\n                if potential_match in known_iam_actions(self.prefix) and potential_match != self]\n\n\n\nclass Statement(BaseElement):\n    \"\"\"Statement in an IAM Policy.\"\"\"\n\n    def __init__(self, Action, Effect, Resource):  # pylint: disable=redefined-outer-name\n        self.Action = Action  # pylint: disable=invalid-name\n        self.Effect = Effect  # pylint: disable=invalid-name\n        self.Resource = Resource  # pylint: disable=invalid-name\n\n    def json_repr(self):\n        return {\n            'Action': self.Action,\n            'Effect': self.Effect,\n            'Resource': self.Resource,\n        }\n\n    def merge(self, other):\n        \"\"\"Merge two statements into one.\"\"\"\n        if self.Effect != other.Effect:\n            raise ValueError(f\"Trying to combine two statements with differing effects: {self.Effect} {other.Effect}\")\n\n        effect = self.Effect\n\n        actions = list(sorted(set(self.Action + other.Action), key=lambda action: action.json_repr()))\n        resources = list(sorted(set(self.Resource + other.Resource)))\n\n        return Statement(\n            Effect=effect,\n            Action=actions,\n            Resource=resources,\n        )\n\n    def __action_list_strings(self):\n        return \"-\".join([a.json_repr() for a in self.Action])\n\n    def __lt__(self, other):\n        if self.Effect != other.Effect:\n            return self.Effect < other.Effect\n        if self.Action != other.Action:\n            # pylint: disable=W0212\n            return self.__action_list_strings() < other.__action_list_strings()\n\n        return \"\".join(self.Resource) < \"\".join(other.Resource)\n\n\nclass PolicyDocument(BaseElement):\n    \"\"\"IAM Policy Doument.\"\"\"\n\n    def __init__(self, Statement, Version=\"2012-10-17\"):  # pylint: disable=redefined-outer-name\n        self.Version = Version  # pylint: disable=invalid-name\n        self.Statement = Statement  # pylint: disable=invalid-name\n\n    def json_repr(self):\n        return {\n            'Version': self.Version,\n            'Statement': self.Statement\n        }\n\n    def to_json(self):\n        \"\"\"Render object into IAM Policy JSON\"\"\"\n        return json.dumps(self.json_repr(), cls=IAMJSONEncoder, indent=4, sort_keys=True)\n\n\nclass IAMJSONEncoder(json.JSONEncoder):\n    \"\"\"JSON Encoder using the json_repr functions\"\"\"\n\n    def default(self, o):  # pylint: disable=method-hidden\n        if hasattr(o, 'json_repr'):\n            return o.json_repr()\n        return json.JSONEncoder.default(self, o)\n\n\ndef _parse_action(action):\n    parts = action.split(\":\")\n    return Action(parts[0], parts[1])\n\n\ndef _parse_statement(statement):\n    return Statement(Action=[_parse_action(action) for action in statement['Action']],\n                     Effect=statement['Effect'],\n                     Resource=statement['Resource'])\n\n\ndef _parse_statements(json_data):\n    # TODO: jsonData could also be dict, aka one statement; similar things happen in the rest of the policy pylint: disable=fixme\n    # https://github.com/flosell/iam-policy-json-to-terraform/blob/fafc231/converter/decode.go#L12-L22\n    return [_parse_statement(statement) for statement in json_data]\n\n\ndef parse_policy_document(stream):\n    \"\"\"Parse a stream of JSON data to a PolicyDocument object\"\"\"\n    if isinstance(stream, six.string_types):\n        json_dict = json.loads(stream)\n    else:\n        json_dict = json.load(stream)\n\n    return PolicyDocument(_parse_statements(json_dict['Statement']), Version=json_dict['Version'])\n\n\ndef all_known_iam_permissions():\n    \"Return a list of all known IAM actions\"\n    with open(os.path.join(os.path.dirname(__file__), 'known-iam-actions.txt'), encoding=\"UTF-8\") as iam_file:\n        return {line.rstrip('\\n') for line in iam_file.readlines()}\n\n\n\n\n# The code to be completed is:\ndef known_iam_actions(prefix):\n\n    \"\"\"\n    This function returns a list of known IAM actions for a given prefix. It retrieves all known IAM permissions, parses the actions, and groups them by prefix. It then returns the list of actions corresponding to the given prefix.\n    Input-Output Arguments\n    :param prefix: String. The prefix for which known IAM actions are to be retrieved.\n    :return: List of String. The list of known IAM actions for the given prefix.\n    \"\"\""}
{"namespace": "rest_framework.utils.mediatypes._MediaType.precedence", "prompt": "# Please complete the precedence function based on the contexts above the function.\n\n# The contexts above the function are:\n\"\"\"\nHandling of media types, as found in HTTP Content-Type and Accept headers.\n\nSee https://www.w3.org/Protocols/rfc2616/rfc2616-sec3.html#sec3.7\n\"\"\"\nfrom rest_framework.compat import parse_header_parameters\n\n\ndef media_type_matches(lhs, rhs):\n    \"\"\"\n    Returns ``True`` if the media type in the first argument <= the\n    media type in the second argument.  The media types are strings\n    as described by the HTTP spec.\n\n    Valid media type strings include:\n\n    'application/json; indent=4'\n    'application/json'\n    'text/*'\n    '*/*'\n    \"\"\"\n    lhs = _MediaType(lhs)\n    rhs = _MediaType(rhs)\n    return lhs.match(rhs)\n\n\ndef order_by_precedence(media_type_lst):\n    \"\"\"\n    Returns a list of sets of media type strings, ordered by precedence.\n    Precedence is determined by how specific a media type is:\n\n    3. 'type/subtype; param=val'\n    2. 'type/subtype'\n    1. 'type/*'\n    0. '*/*'\n    \"\"\"\n    ret = [set(), set(), set(), set()]\n    for media_type in media_type_lst:\n        precedence = _MediaType(media_type).precedence\n        ret[3 - precedence].add(media_type)\n    return [media_types for media_types in ret if media_types]\n\n\nclass _MediaType:\n    def __init__(self, media_type_str):\n        self.orig = '' if (media_type_str is None) else media_type_str\n        self.full_type, self.params = parse_header_parameters(self.orig)\n        self.main_type, sep, self.sub_type = self.full_type.partition('/')\n\n    def match(self, other):\n        \"\"\"Return true if this MediaType satisfies the given MediaType.\"\"\"\n        for key in self.params:\n            if key != 'q' and other.params.get(key, None) != self.params.get(key, None):\n                return False\n\n        if self.sub_type != '*' and other.sub_type != '*' and other.sub_type != self.sub_type:\n            return False\n\n        if self.main_type != '*' and other.main_type != '*' and other.main_type != self.main_type:\n            return False\n\n        return True\n\n    @property\n\n\n# The code to be completed is:\n    def precedence(self):\n\n        \"\"\"\n        This function calculates the precedence level of a media type based on its specificity. It checks the main type, sub type, and parameters of the media type to determine the precedence level.\n        Input-Output Arguments\n        :param self: _MediaType. An instance of the _MediaType class.\n        :return: int. The precedence level of the media type, ranging from 0 to 3.\n        \"\"\""}
{"namespace": "faker.utils.loading.get_path", "prompt": "# Please complete the get_path function based on the contexts above the function.\n\n# The contexts above the function are:\nimport pkgutil\nimport sys\n\nfrom importlib import import_module\nfrom pathlib import Path\nfrom types import ModuleType\nfrom typing import List\n\n\n\n\n# The code to be completed is:\ndef get_path(module: ModuleType) -> str:\n\n    \"\"\"\n    Get the path of the given module. It first checks if the system is frozen. If it is, it checks if it is frozen by PyInstaller or others and then returns the path accordingly. If the system is not frozen, it returns the path of the module. If the file is None, it raises RuntimeError(f\"Can't find path from module `{module}.\").\n    Input-Output Arguments\n    :param module: ModuleType. The module for which the path is to be found.\n    :return: str. The path of the given module.\n    \"\"\""}
{"namespace": "boto.configservice.connect_to_region", "prompt": "# Please complete the connect_to_region function based on the contexts above the function.\n\n# The contexts above the function are:\n# Copyright (c) 2015 Amazon.com, Inc. or its affiliates.\n# All Rights Reserved\n#\n# Permission is hereby granted, free of charge, to any person obtaining a\n# copy of this software and associated documentation files (the\n# \"Software\"), to deal in the Software without restriction, including\n# without limitation the rights to use, copy, modify, merge, publish, dis-\n# tribute, sublicense, and/or sell copies of the Software, and to permit\n# persons to whom the Software is furnished to do so, subject to the fol-\n# lowing conditions:\n#\n# The above copyright notice and this permission notice shall be included\n# in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS\n# OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABIL-\n# ITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT\n# SHALL THE AUTHOR BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY,\n# WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS\n# IN THE SOFTWARE.\n#\nfrom boto.regioninfo import RegionInfo, get_regions\n\n\n\ndef regions():\n    \"\"\"\n    Get all available regions for the AWS Config service.\n\n    :rtype: list\n    :return: A list of :class:`boto.regioninfo.RegionInfo`\n    \"\"\"\n    from boto.configservice.layer1 import ConfigServiceConnection\n    return get_regions('configservice', connection_cls=ConfigServiceConnection)\n\n\n\n\n# The code to be completed is:\ndef connect_to_region(region_name, **kw_params):\n\n    \"\"\"\n    Connect to a specific region in the AWS Config service. It creates a connection to the Config service in the specified region using the provided parameters.\n    Input-Output Arguments\n    :param region_name: String. The name of the region to connect to.\n    :param **kw_params: Additional keyword arguments that can be passed to the connection.\n    :return: ConfigServiceConnection. The connection object to the Config service in the specified region.\n    \"\"\""}
{"namespace": "authlib.jose.util.extract_header", "prompt": "# Please complete the extract_header function based on the contexts above the function.\n\n# The contexts above the function are:\nimport binascii\nfrom authlib.common.encoding import urlsafe_b64decode, json_loads, to_unicode\nfrom authlib.jose.errors import DecodeError\n\n\n\n\n# The code to be completed is:\ndef extract_header(header_segment, error_cls):\n\n    \"\"\"\n    This function extracts the header from a given header segment. It first extracts the header segment. Then, it decodes the extracted header data using UTF-8 encoding and loads it as a JSON object. If the loaded header is not a dictionary, it raises an error. Finally, it returns the extracted header.\n    Input-Output Arguments\n    :param header_segment: The header segment to extract the header from.\n    :param error_cls: The error class to raise if there is an error during the extraction process.\n    :return: The extracted header as a dictionary.\n    \"\"\""}
{"namespace": "bentoml_cli.env_manager.remove_env_arg", "prompt": "# Please complete the remove_env_arg function based on the contexts above the function.\n\n# The contexts above the function are:\nfrom __future__ import annotations\n\nimport functools\nimport logging\nimport os\nimport re\nimport sys\nimport typing as t\nfrom shutil import which\n\nimport click\nimport fs\nfrom simple_di import Provide\nfrom simple_di import inject\n\nfrom bentoml._internal.bento.bento import BENTO_YAML_FILENAME\nfrom bentoml._internal.bento.bento import DEFAULT_BENTO_BUILD_FILE\nfrom bentoml._internal.bento.bento import Bento\nfrom bentoml._internal.bento.bento import BentoStore\nfrom bentoml._internal.configuration import get_debug_mode\nfrom bentoml._internal.configuration.containers import BentoMLContainer\nfrom bentoml._internal.env_manager import EnvManager\nfrom bentoml._internal.env_manager.envs import Environment\nfrom bentoml.exceptions import BentoMLException\nfrom bentoml.exceptions import NotFound as BentoNotFound\n\nif t.TYPE_CHECKING:\n    P = t.ParamSpec(\"P\")\n    F = t.Callable[P, t.Any]\n\nlogger = logging.getLogger(__name__)\n\n\n\n\n# The code to be completed is:\ndef remove_env_arg(cmd_args: list[str]) -> list[str]:\n\n    \"\"\"\n    This function removes `--env <env_name>` or `--env=<env_name>` from the input list of command line arguments.\n    Input-Output Arguments\n    :param cmd_args: List of string. The list of command line arguments.\n    :return: List of string. The updated list of command line arguments after removing `--env <env_name>` or `--env=<env_name>`.\n    \"\"\""}
{"namespace": "diffprivlib.tools.quantiles.quantile", "prompt": "# Please complete the quantile function based on the contexts above the function.\n\n# The contexts above the function are:\n# MIT License\n#\n# Copyright (C) IBM Corporation 2020\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the\n# rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit\n# persons to whom the Software is furnished to do so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the\n# Software.\n#\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE\n# WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,\n# TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\"\"\"\nQuantile functions with differential privacy\n\"\"\"\nimport warnings\n\nimport numpy as np\n\n\nfrom diffprivlib.mechanisms import Exponential\nfrom diffprivlib.utils import warn_unused_args\n\n\n\n\n\n\n# The code to be completed is:\ndef quantile(array, quant, epsilon=1.0, bounds=None, axis=None, keepdims=False, random_state=None, accountant=None,\n             **unused_args):\n\n    \"\"\"\n    This function calculates the differentially private quantile of an array. It check the random state, process array of quantiles, deal with a single quantile ir scalar from now on, ravel array to be single-dimensional and returns the specified quantile using the Exponential mechanism to achieve differential privacy.\n    Input-Output Arguments\n    :param array: array_like. The input array containing numbers whose quantile is sought.\n    :param quant: float or array-like. The quantile(s) to be calculated. Each quantile must be in the unit interval [0, 1]. If quant is array-like, quantiles are returned over the flattened array.\n    :param epsilon: float, default: 1.0. The privacy parameter epsilon. Differential privacy is achieved over the entire output, with epsilon split evenly between each output value.\n    :param bounds: tuple, optional. Bounds of the values of the array, of the form (min, max).\n    :param axis: None or int or tuple of ints, optional. Axis or axes along which a sum is performed. The default, axis=None, will sum all of the elements of the input array.\n    :param keepdims: bool, default: False. If this is set to True, the axes which are reduced are left in the result as dimensions with size one.\n    :param random_state: int or RandomState, optional. Controls the randomness of the algorithm.\n    :param accountant: BudgetAccountant, optional. Accountant to keep track of privacy budget.\n    :param **unused_args: Should warn the user if any other parameters are passed.\n    :return: ndarray. Returns a new array containing the quantile values.\n    \"\"\""}
{"namespace": "aioxmpp.entitycaps.caps115.build_identities_string", "prompt": "# Please complete the build_identities_string function based on the contexts above the function.\n\n# The contexts above the function are:\n########################################################################\n# File name: caps115.py\n# This file is part of: aioxmpp\n#\n# LICENSE\n#\n# This program is free software: you can redistribute it and/or modify\n# it under the terms of the GNU Lesser General Public License as\n# published by the Free Software Foundation, either version 3 of the\n# License, or (at your option) any later version.\n#\n# This program is distributed in the hope that it will be useful, but\n# WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\n# Lesser General Public License for more details.\n#\n# You should have received a copy of the GNU Lesser General Public\n# License along with this program.  If not, see\n# <http://www.gnu.org/licenses/>.\n#\n########################################################################\nimport base64\nimport collections\nimport hashlib\nimport pathlib\nimport urllib.parse\n\nfrom xml.sax.saxutils import escape\n\nfrom .common import AbstractKey, AbstractImplementation\nfrom . import xso as caps_xso\n\n\n\n\n# The code to be completed is:\ndef build_identities_string(identities):\n\n    \"\"\"\n    This function builds a string of identities based on the given list of identities. It first processes each identity in the list and encodes it into a byte string. Then, it checks for duplicate identities and sorts the identities before joining them into a single byte string.\n    Input-Output Arguments\n    :param identities: List of Identity. A list of identity objects.\n    :return: Byte string. The concatenated byte string of identities which is seperated by '<'.\n    \"\"\""}
{"namespace": "chatette.adapters.factory.create_adapter", "prompt": "# Please complete the create_adapter function based on the contexts above the function.\n\n# The contexts above the function are:\n\"\"\"\nModule `chatette.adapters.factory`.\nDefines a factory method that allows to create an adapter from a string name.\n\"\"\"\n\n\n\n\n\n\n\n\n# The code to be completed is:\ndef create_adapter(adapter_name, base_filepath=None):\n\n    \"\"\"\n    This function creates and returns an instance of an adapter based on the given adapter name. The adapter names are used to determine which adapter class to instantiate. The mames are the following format:'rasa','rasa-md' or 'rasamd','jsonl'.\n    Input-Output Arguments\n    :param adapter_name: String. The name of the adapter to be instantiated.\n    :param base_filepath: String. The base file path to be used by the adapter. Defaults to None.\n    :return: Adapter. The instantiated adapter instance based on the given adapter name.\n    \"\"\""}
{"namespace": "faker.utils.loading.find_available_providers", "prompt": "# Please complete the find_available_providers function based on the contexts above the function.\n\n# The contexts above the function are:\nimport pkgutil\nimport sys\n\nfrom importlib import import_module\nfrom pathlib import Path\nfrom types import ModuleType\nfrom typing import List\n\n\ndef get_path(module: ModuleType) -> str:\n    if getattr(sys, \"frozen\", False):\n        # frozen\n\n        if getattr(sys, \"_MEIPASS\", False):\n            # PyInstaller\n            lib_dir = Path(getattr(sys, \"_MEIPASS\"))\n        else:\n            # others\n            lib_dir = Path(sys.executable).parent / \"lib\"\n\n        path = lib_dir.joinpath(*module.__package__.split(\".\"))  # type: ignore\n    else:\n        # unfrozen\n        if module.__file__ is not None:\n            path = Path(module.__file__).parent\n        else:\n            raise RuntimeError(f\"Can't find path from module `{module}.\")\n    return str(path)\n\n\ndef list_module(module: ModuleType) -> List[str]:\n    path = get_path(module)\n\n    if getattr(sys, \"_MEIPASS\", False):\n        # PyInstaller\n        return [file.parent.name for file in Path(path).glob(\"*/__init__.py\")]\n    else:\n        return [name for _, name, is_pkg in pkgutil.iter_modules([str(path)]) if is_pkg]\n\n\ndef find_available_locales(providers: List[str]) -> List[str]:\n    available_locales = set()\n\n    for provider_path in providers:\n        provider_module = import_module(provider_path)\n        if getattr(provider_module, \"localized\", False):\n            langs = list_module(provider_module)\n            available_locales.update(langs)\n    return sorted(available_locales)\n\n\n\n\n# The code to be completed is:\ndef find_available_providers(modules: List[ModuleType]) -> List[str]:\n\n    \"\"\"\n    This function takes a list of modules as input and finds the available providers. It iterates over each module in the input list, checks if the module has a package, and then creates a list of providers by joining the package name with each module name (excluding \"__pycache__\"). The function then updates a set of available providers with the newly created list and returns the sorted list of available providers.\n    Input-Output Arguments\n    :param modules: List of ModuleType. A list of modules to search for available providers.\n    :return: List of str. The sorted list of available providers.\n    \"\"\""}
{"namespace": "threatingestor.state.State.save_state", "prompt": "# Please complete the save_state function based on the contexts above the function.\n\n# The contexts above the function are:\nimport sqlite3\nfrom loguru import logger\n\nimport threatingestor.exceptions\n\nclass State:\n    \"\"\"State DB management.\"\"\"\n    def __init__(self, dbname):\n        \"\"\"Set up a connection to the state DB.\"\"\"\n        try:\n            self.conn = sqlite3.connect(dbname)\n            self.cursor = self.conn.cursor()\n            self._create_table()\n        except sqlite3.Error:\n            raise threatingestor.exceptions.IngestorError(\"State database seems broken\")\n\n\n    def _create_table(self):\n        \"\"\"Create table if it doesn't already exist.\"\"\"\n        self.cursor.execute('CREATE TABLE IF NOT EXISTS states (name text UNIQUE, state text)')\n        self.conn.commit()\n\n\n\n\n# The code to be completed is:\n    def save_state(self, name, state):\n\n        \"\"\"\n        This function is used to create or update a state record in a database. It takes a name and state as input parameters, and inserts or replaces the corresponding values in the \"states\" table of the database.\n        Input-Output Arguments\n        :param self: State. An instance of the State class.\n        :param name: String. The name of the state record.\n        :param state: Any data type. The state value to be stored.\n        :return: No return values.\n        \"\"\""}
{"namespace": "arctic.decorators._get_host", "prompt": "# Please complete the _get_host function based on the contexts above the function.\n\n# The contexts above the function are:\nimport logging\nimport sys\nfrom functools import wraps\nfrom time import sleep\n\nfrom pymongo.errors import (AutoReconnect, OperationFailure, DuplicateKeyError, ServerSelectionTimeoutError,\n                            BulkWriteError)\n\nfrom .hooks import log_exception as _log_exception\n\nlogger = logging.getLogger(__name__)\n\n_MAX_RETRIES = 15\n\n\n\n\n# The code to be completed is:\ndef _get_host(store):\n\n    \"\"\"\n    This function returns a dictionary containing the host information of the given store. It first checks if the store is not empty. If the store is not empty, it checks whether it's a list or tuple and takes the first element if so. The function then gathers the store's library name, the MongoDB nodes (formatted as \"host:port\"), and the MongoDB host associated with the Arctic library.\n    Input-Output Arguments\n    :param store: Object. The store object from which the host information is to be retrieved.\n    :return: Dictionary. A dictionary containing the host information of the given store.\n    \"\"\""}
{"namespace": "mrjob.logs.spark._parse_spark_log", "prompt": "# Please complete the _parse_spark_log function based on the contexts above the function.\n\n# The contexts above the function are:\n# Copyright 2019 Yelp\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Parse Spark driver and executor output. This can appear as either a \"step\"\nlog (output of the spark-submit binary) or as a \"task\" log (executors in\nYARN containers), but has more or less the same format in either case.\"\"\"\nfrom .ids import _add_implied_task_id\nfrom .log4j import _parse_hadoop_log4j_records\nfrom .step import _SUBMITTED_APPLICATION_RE\nfrom .wrap import _cat_log_lines\n\n\n# if a message ends with this, it's the beginning of a traceback\n_TRACEBACK_ENDS_WITH = 'Traceback (most recent call last):'\n\n# if a traceback starts with this, strip it from the error message\n_CAUSED_BY = 'Caused by: '\n\n\n\n\n# The code to be completed is:\ndef _parse_spark_log(lines, record_callback=None):\n\n    \"\"\"\n    Parses a Spark log, extracting errors and application ID. \n\n    Input-Output Arguments\n\n    \"\"\""}
{"namespace": "zulipterminal.ui_tools.buttons.TopButton.keypress", "prompt": "# Please complete the keypress function based on the contexts above the function.\n\n# The contexts above the function are:\nimport re\nfrom functools import partial\nfrom typing import Any, Callable, Dict, List, Optional, Tuple, Union, cast\nfrom urllib.parse import urljoin, urlparse\n\nimport urwid\nfrom typing_extensions import TypedDict\n\nfrom zulipterminal.api_types import RESOLVED_TOPIC_PREFIX, EditPropagateMode\nfrom zulipterminal.config.keys import is_command_key, primary_key_for_command\n\nfrom zulipterminal.config.symbols import CHECK_MARK, MUTE_MARKER\nfrom zulipterminal.config.ui_mappings import EDIT_MODE_CAPTIONS, STREAM_ACCESS_TYPE\nfrom zulipterminal.helper import Message, StreamData, hash_util_decode\nfrom zulipterminal.urwid_types import urwid_Size\n\n\nclass TopButton(urwid.Button):\n    def __init__(\n        self,\n        *,\n        controller: Any,\n        caption: str,\n        show_function: Callable[[], Any],\n        prefix_character: Union[str, Tuple[Any, str]] = \"\\N{BULLET}\",\n        text_color: Optional[str] = None,\n        count: int = 0,\n        count_style: Optional[str] = None,\n    ) -> None:\n        self.controller = controller\n        self._caption = caption\n        self.show_function = show_function\n        self.prefix_character = prefix_character\n        self.original_color = text_color\n        self.count = count\n        self.count_style = count_style\n\n        super().__init__(\"\")\n\n        self.button_prefix = urwid.Text(\"\")\n        self._label.set_wrap_mode(\"ellipsis\")\n        self._label.get_cursor_coords = lambda x: None\n        self.button_suffix = urwid.Text(\"\")\n\n        cols = urwid.Columns(\n            [\n                (\"pack\", self.button_prefix),\n                self._label,\n                (\"pack\", self.button_suffix),\n            ]\n        )\n        self._w = urwid.AttrMap(cols, None, \"selected\")\n\n        self.update_count(count, text_color)\n\n        urwid.connect_signal(self, \"click\", self.activate)\n\n    def update_count(self, count: int, text_color: Optional[str] = None) -> None:\n        new_color = self.original_color if text_color is None else text_color\n\n        self.count = count\n        if count == 0:\n            count_text = \"\"\n        else:\n            count_text = str(count)\n\n        self.update_widget((self.count_style, count_text), new_color)\n\n    def update_widget(\n        self, count_text: Tuple[Optional[str], str], text_color: Optional[str]\n    ) -> Any:\n        if self.prefix_character:\n            prefix = [\" \", self.prefix_character, \" \"]\n        else:\n            prefix = [\" \"]\n        if count_text[1]:\n            suffix = [\" \", count_text, \" \"]\n        else:\n            suffix = [\"  \"]\n        self.button_prefix.set_text(prefix)\n        self.set_label(self._caption)\n        self.button_suffix.set_text(suffix)\n        self._w.set_attr_map({None: text_color})\n\n    def activate(self, key: Any) -> None:\n        self.controller.view.show_left_panel(visible=False)\n        self.controller.view.show_right_panel(visible=False)\n        self.controller.view.body.focus_col = 1\n        self.show_function()\n\n\n\n# The code to be completed is:\n    def keypress(self, size: urwid_Size, key: str) -> Optional[str]:\n\n        \"\"\"\n        Handle keypress events for the TopButton class. If the key is the \"ENTER\" key, it activates the button. Otherwise, it calls the keypress method of the superclass to handle the keypress event.\n        Input-Output Arguments\n        :param self: TopButton. An instance of the TopButton class.\n        :param size: urwid_Size. The size of the widget.\n        :param key: str. The key that was pressed.\n        :return: Optional[str]. If the key is the \"ENTER\" key, it returns None. Otherwise, it returns the result of the keypress method of the superclass.\n        \"\"\""}
{"namespace": "faker.providers.misc.Provider.binary", "prompt": "# Please complete the binary function based on the contexts above the function.\n\n# The contexts above the function are:\nimport csv\nimport hashlib\nimport io\nimport json\nimport os\nimport re\nimport string\nimport tarfile\nimport uuid\nimport zipfile\n\nfrom typing import Any, Callable, Dict, List, Optional, Sequence, Set, Tuple, Type, Union\n\nfrom faker.exceptions import UnsupportedFeature\n\nfrom .. import BaseProvider\nfrom ..python import TypesSpec\n\nlocalized = True\n\ncsv.register_dialect(\"faker-csv\", csv.excel, quoting=csv.QUOTE_ALL)\n\n\nclass Provider(BaseProvider):\n    def boolean(self, chance_of_getting_true: int = 50) -> bool:\n        \"\"\"Generate a random boolean value based on ``chance_of_getting_true``.\n\n        :sample: chance_of_getting_true=25\n        :sample: chance_of_getting_true=50\n        :sample: chance_of_getting_true=75\n        \"\"\"\n        return self.generator.random.randint(1, 100) <= chance_of_getting_true\n\n    def null_boolean(self) -> Optional[bool]:\n        \"\"\"Generate ``None``, ``True``, or ``False``, each with equal probability.\"\"\"\n\n        return {\n            0: None,\n            1: True,\n            -1: False,\n        }[self.generator.random.randint(-1, 1)]\n\n\n\n# The code to be completed is:\n    def binary(self, length: int = (1 * 1024 * 1024)) -> bytes:\n\n        \"\"\"\n        Generate a random binary blob of the specified length. If the faker instance has been seeded, the performance will be significantly reduced to conform to the seeding.\n        Input-Output Arguments\n        :param self: Provider. An instance of the Provider class.\n        :param length: int. The length of the binary blob to be generated. Defaults to 1 * 1024 * 1024.\n        :return: bytes. The generated random binary blob.\n        \"\"\""}
{"namespace": "mopidy.internal.validation.check_uris", "prompt": "# Please complete the check_uris function based on the contexts above the function.\n\n# The contexts above the function are:\nimport urllib\nfrom collections.abc import Iterable, Mapping\n\nfrom mopidy import exceptions\n\nPLAYBACK_STATES = {\"paused\", \"stopped\", \"playing\"}\n\nTRACK_FIELDS_WITH_TYPES = {\n    \"uri\": str,\n    \"track_name\": str,\n    \"album\": str,\n    \"artist\": str,\n    \"albumartist\": str,\n    \"composer\": str,\n    \"performer\": str,\n    \"track_no\": int,\n    \"genre\": str,\n    \"date\": str,\n    \"comment\": str,\n    \"disc_no\": int,\n    \"musicbrainz_albumid\": str,\n    \"musicbrainz_artistid\": str,\n    \"musicbrainz_trackid\": str,\n}\n\nSEARCH_FIELDS = set(TRACK_FIELDS_WITH_TYPES).union({\"any\"})\n\nPLAYLIST_FIELDS = {\"uri\", \"name\"}  # TODO: add length and last_modified?\n\nTRACKLIST_FIELDS = {  # TODO: add bitrate, length, disc_no, track_no, modified?\n    \"uri\",\n    \"name\",\n    \"genre\",\n    \"date\",\n    \"comment\",\n    \"musicbrainz_id\",\n}\n\nDISTINCT_FIELDS = dict(TRACK_FIELDS_WITH_TYPES)\n\n\n# TODO: _check_iterable(check, msg, **kwargs) + [check(a) for a in arg]?\ndef _check_iterable(arg, msg, **kwargs):\n    \"\"\"Ensure we have an iterable which is not a string or an iterator\"\"\"\n    if isinstance(arg, str):\n        raise exceptions.ValidationError(msg.format(arg=arg, **kwargs))\n    elif not isinstance(arg, Iterable):\n        raise exceptions.ValidationError(msg.format(arg=arg, **kwargs))\n    elif iter(arg) is iter(arg):\n        raise exceptions.ValidationError(msg.format(arg=arg, **kwargs))\n\n\ndef check_choice(arg, choices, msg=\"Expected one of {choices}, not {arg!r}\"):\n    if arg not in choices:\n        raise exceptions.ValidationError(\n            msg.format(arg=arg, choices=tuple(choices))\n        )\n\n\ndef check_boolean(arg, msg=\"Expected a boolean, not {arg!r}\"):\n    check_instance(arg, bool, msg=msg)\n\n\ndef check_instance(arg, cls, msg=\"Expected a {name} instance, not {arg!r}\"):\n    if not isinstance(arg, cls):\n        raise exceptions.ValidationError(msg.format(arg=arg, name=cls.__name__))\n\n\ndef check_instances(arg, cls, msg=\"Expected a list of {name}, not {arg!r}\"):\n    _check_iterable(arg, msg, name=cls.__name__)\n    if not all(isinstance(instance, cls) for instance in arg):\n        raise exceptions.ValidationError(msg.format(arg=arg, name=cls.__name__))\n\n\ndef check_integer(arg, min=None, max=None):\n    if not isinstance(arg, int):\n        raise exceptions.ValidationError(f\"Expected an integer, not {arg!r}\")\n    elif min is not None and arg < min:\n        raise exceptions.ValidationError(\n            f\"Expected number larger or equal to {min}, not {arg!r}\"\n        )\n    elif max is not None and arg > max:\n        raise exceptions.ValidationError(\n            f\"Expected number smaller or equal to {max}, not {arg!r}\"\n        )\n\n\ndef check_query(arg, fields=None, list_values=True):\n    if fields is None:\n        fields = SEARCH_FIELDS\n    # TODO: normalize name  -> track_name\n    # TODO: normalize value -> [value]\n    # TODO: normalize blank -> [] or just remove field?\n    # TODO: remove list_values?\n\n    if not isinstance(arg, Mapping):\n        raise exceptions.ValidationError(\n            f\"Expected a query dictionary, not {arg!r}\"\n        )\n\n    for key, value in arg.items():\n        check_choice(\n            key,\n            fields,\n            msg=\"Expected query field to be one of \" \"{choices}, not {arg!r}\",\n        )\n        if list_values:\n            msg = 'Expected \"{key}\" to be list of strings, not {arg!r}'\n            _check_iterable(value, msg, key=key)\n            [_check_query_value(key, v, msg) for v in value]\n        else:\n            _check_query_value(\n                key, value, 'Expected \"{key}\" to be a string, not {arg!r}'\n            )\n\n\ndef _check_query_value(key, arg, msg):\n    if not isinstance(arg, str) or not arg.strip():\n        raise exceptions.ValidationError(msg.format(arg=arg, key=key))\n\n\ndef check_uri(arg, msg=\"Expected a valid URI, not {arg!r}\"):\n    if not isinstance(arg, str):\n        raise exceptions.ValidationError(msg.format(arg=arg))\n    elif urllib.parse.urlparse(arg).scheme == \"\":\n        raise exceptions.ValidationError(msg.format(arg=arg))\n\n\n\n\n# The code to be completed is:\ndef check_uris(arg, msg=\"Expected a list of URIs, not {arg!r}\"):\n\n    \"\"\"\n    This function checks if the input argument is a list of URIs. If it is not, it raises an exception with a custom error message. It then iterates over each URI in the list and calls the check_uri function to validate each URI.\n    Input-Output Arguments\n    :param arg: Any. The input argument to be checked.\n    :param msg: String. The custom error message to be displayed if the input argument is not a list of URIs. It defaults to \"Expected a list of URIs, not {arg!r}\".\n    :return: No return values.\n    \"\"\""}
{"namespace": "sacred.config.custom_containers.DogmaticDict.get", "prompt": "# Please complete the get function based on the contexts above the function.\n\n# The contexts above the function are:\n#!/usr/bin/env python\n# coding=utf-8\nimport copy\n\nimport sacred.optional as opt\nfrom sacred.utils import join_paths, SacredError\n\n\ndef fallback_dict(fallback, **kwargs):\n    fallback_copy = fallback.copy()\n    fallback_copy.update(kwargs)\n    return fallback_copy\n\n\nclass DogmaticDict(dict):\n    def __init__(self, fixed=None, fallback=None):\n        super().__init__()\n        self.typechanges = {}\n        self.fallback_writes = []\n        self.modified = set()\n        self.fixed = fixed or {}\n        self._fallback = {}\n        if fallback:\n            self.fallback = fallback\n\n    @property\n    def fallback(self):\n        return self._fallback\n\n    @fallback.setter\n    def fallback(self, newval):\n        ffkeys = set(self.fixed.keys()).intersection(set(newval.keys()))\n        for k in ffkeys:\n            if isinstance(self.fixed[k], DogmaticDict):\n                self.fixed[k].fallback = newval[k]\n            elif isinstance(self.fixed[k], dict):\n                self.fixed[k] = DogmaticDict(self.fixed[k])\n                self.fixed[k].fallback = newval[k]\n\n        self._fallback = newval\n\n    def _log_blocked_setitem(self, key, value, fixed_value):\n        if type_changed(value, fixed_value):\n            self.typechanges[key] = (type(value), type(fixed_value))\n\n        if is_different(value, fixed_value):\n            self.modified.add(key)\n\n        # if both are dicts recursively collect modified and typechanges\n        if isinstance(fixed_value, DogmaticDict) and isinstance(value, dict):\n            for k, val in fixed_value.typechanges.items():\n                self.typechanges[join_paths(key, k)] = val\n\n            self.modified |= {join_paths(key, m) for m in fixed_value.modified}\n\n    def __setitem__(self, key, value):\n        if key not in self.fixed:\n            if key in self.fallback:\n                self.fallback_writes.append(key)\n            return dict.__setitem__(self, key, value)\n\n        fixed_value = self.fixed[key]\n        dict.__setitem__(self, key, fixed_value)\n        # if both are dicts do a recursive update\n        if isinstance(fixed_value, DogmaticDict) and isinstance(value, dict):\n            for k, val in value.items():\n                fixed_value[k] = val\n\n        self._log_blocked_setitem(key, value, fixed_value)\n\n    def __getitem__(self, item):\n        if dict.__contains__(self, item):\n            return dict.__getitem__(self, item)\n        elif item in self.fallback:\n            if item in self.fixed:\n                return self.fixed[item]\n            else:\n                return self.fallback[item]\n        raise KeyError(item)\n\n    def __contains__(self, item):\n        return dict.__contains__(self, item) or (item in self.fallback)\n\n\n\n# The code to be completed is:\n    def get(self, k, d=None):\n\n        \"\"\"\n        This function retrieves the value associated with the given key from the DogmaticDict instance. If the key is not found in the instance, it checks the fallback dictionary and returns the value associated with the key from the fallback dictionary if found, otherwise it returns the default value.\n        Input-Output Arguments\n        :param self: DogmaticDict. An instance of the DogmaticDict class.\n        :param k: The key to retrieve the value from the instance.\n        :param d: The default value to return if the key is not found in the instance or the fallback dictionary. Defaults to None.\n        :return: The value associated with the key, or the value associated with the key in the fallback dictionary, or the default value.\n        \"\"\""}
{"namespace": "sslyze.cli.server_string_parser.CommandLineServerStringParser.parse_server_string", "prompt": "# Please complete the parse_server_string function based on the contexts above the function.\n\n# The contexts above the function are:\nimport socket\nfrom dataclasses import dataclass\nfrom typing import Tuple, Optional\n\n\n@dataclass(frozen=True)\nclass InvalidServerStringError(Exception):\n    \"\"\"Exception raised when SSLyze was unable to parse a hostname:port string supplied via the command line.\"\"\"\n\n    server_string: str\n    error_message: str\n\n\nclass CommandLineServerStringParser:\n    \"\"\"Utility class to parse a 'host:port{ip}' string taken from the command line into a valid (host,ip, port) tuple.\n    Supports IPV6 addresses.\n    \"\"\"\n\n    SERVER_STRING_ERROR_BAD_PORT = \"Not a valid host:port\"\n\n    @classmethod\n\n\n# The code to be completed is:\n    def parse_server_string(cls, server_str: str) -> Tuple[str, Optional[str], Optional[int]]:\n        # Extract ip from target\n\n        \"\"\"\n        This function parses a server string and extracts the host, ip, and port information from it. It first checks if the server string contains curly braces, indicating the presence of an ip address. If so, it extracts the ip address and removes it from the server string. Then, it checks if the server string contains square brackets, indicating the presence of an ipv6 hint. If so, it calls a helper function to parse the ipv6 server string. If not, it checks if the extracted ip address contains square brackets, indicating the presence of an ipv6 hint. If so, it calls the helper function to parse the ipv6 ip address. Finally, if none of the above conditions are met, it calls the helper function to parse the ipv4 server string. The function returns the host, ip, and port extracted from the server string.\n        Input-Output Arguments\n        :param cls: The class object.\n        :param server_str: String. The server string to be parsed.\n        :return: Tuple. The host, ip, and port extracted from the server string.\n        \"\"\""}
{"namespace": "faker.utils.checksums.luhn_checksum", "prompt": "# Please complete the luhn_checksum function based on the contexts above the function.\n\n# The contexts above the function are:\nfrom typing import List\n\n\n\n\n# The code to be completed is:\ndef luhn_checksum(number: float) -> int:\n\n    \"\"\"\n    Calculate the Luhn checksum for the given number. The Luhn algorithm is used to validate a variety of identification numbers, such as credit card numbers, IMEI numbers, National Provider Identifier numbers in the United States, and Canadian Social Insurance Numbers.\n    Input-Output Arguments\n    :param number: float. The number for which the Luhn checksum needs to be calculated.\n    :return: int. The Luhn checksum for the given number.\n    \"\"\""}
{"namespace": "pyramid.util.InstancePropertyHelper.apply", "prompt": "# Please complete the apply function based on the contexts above the function.\n\n# The contexts above the function are:\nfrom contextlib import contextmanager\nimport functools\nfrom hmac import compare_digest\nimport inspect\nimport platform\nimport weakref\n\nfrom pyramid.path import DottedNameResolver as _DottedNameResolver\n\n_marker = object()\n\nWIN = platform.system() == 'Windows'\n\ntry:  # pragma: no cover\n    import __pypy__\n\n    PYPY = True\nexcept BaseException:  # pragma: no cover\n    __pypy__ = None\n    PYPY = False\n\n\nclass DottedNameResolver(_DottedNameResolver):\n    def __init__(\n        self, package=None\n    ):  # default to package = None for bw compat\n        _DottedNameResolver.__init__(self, package)\n\n\ndef text_(s, encoding='latin-1', errors='strict'):\n    \"\"\"If ``s`` is an instance of ``bytes``, return\n    ``s.decode(encoding, errors)``, otherwise return ``s``\"\"\"\n    if isinstance(s, bytes):\n        return s.decode(encoding, errors)\n    return s\n\n\ndef bytes_(s, encoding='latin-1', errors='strict'):\n    \"\"\"If ``s`` is an instance of ``str``, return\n    ``s.encode(encoding, errors)``, otherwise return ``s``\"\"\"\n    if isinstance(s, str):\n        return s.encode(encoding, errors)\n    return s\n\n\ndef ascii_(s):\n    \"\"\"\n    If ``s`` is an instance of ``str``, return\n    ``s.encode('ascii')``, otherwise return ``str(s, 'ascii', 'strict')``\n    \"\"\"\n    if isinstance(s, str):\n        s = s.encode('ascii')\n    return str(s, 'ascii', 'strict')\n\n\ndef is_nonstr_iter(v):\n    if isinstance(v, str):\n        return False\n    return hasattr(v, '__iter__')\n\n\ndef is_string_or_iterable(v):\n    if isinstance(v, str):\n        return True\n    if hasattr(v, '__iter__'):\n        return True\n\n\ndef as_sorted_tuple(val):\n    if not is_nonstr_iter(val):\n        val = (val,)\n    val = tuple(sorted(val))\n    return val\n\n\nclass SettableProperty:\n    # this is just like reify but does not store the computed result on\n    # the class such that subsequent invocations invoke the callable again\n    def __init__(self, wrapped):\n        self.wrapped = wrapped\n        functools.update_wrapper(self, wrapped)\n\n    def __get__(self, obj, type=None):\n        if obj is None:  # pragma: no cover\n            return self\n        return self.wrapped(obj)\n\n\nclass InstancePropertyHelper:\n    \"\"\"A helper object for assigning properties and descriptors to instances.\n    It is not normally possible to do this because descriptors must be\n    defined on the class itself.\n\n    This class is optimized for adding multiple properties at once to an\n    instance. This is done by calling :meth:`.add_property` once\n    per-property and then invoking :meth:`.apply` on target objects.\n\n    \"\"\"\n\n    def __init__(self):\n        self.properties = {}\n\n    @classmethod\n    def make_property(cls, callable, name=None, reify=False):\n        \"\"\"Convert a callable into one suitable for adding to the\n        instance. This will return a 2-tuple containing the computed\n        (name, property) pair.\n        \"\"\"\n\n        if name is None:\n            if not hasattr(callable, '__name__'):\n                raise ValueError(\n                    'missing __name__, must specify \"name\" for property'\n                )\n            name = callable.__name__\n        name = get_callable_name(name)\n        is_data_descriptor = inspect.isdatadescriptor(callable)\n        if reify and is_data_descriptor:\n            raise ValueError('cannot reify a data descriptor')\n        if is_data_descriptor:\n            fn = callable\n        else:\n            wrapped = lambda this: callable(this)\n            wrapped.__name__ = name\n            wrapped.__doc__ = callable.__doc__\n\n            if reify:\n                import pyramid.decorator  # avoid circular import\n\n                fn = pyramid.decorator.reify(wrapped)\n            else:\n                fn = SettableProperty(wrapped)\n\n        return name, fn\n\n    @classmethod\n    def apply_properties(cls, target, properties):\n        \"\"\"Accept a list or dict of ``properties`` generated from\n        :meth:`.make_property` and apply them to a ``target`` object.\n        \"\"\"\n        attrs = dict(properties)\n        if attrs:\n            parent = target.__class__\n            # fix the module name so it appears to still be the parent\n            # e.g. pyramid.request instead of pyramid.util\n            attrs.setdefault('__module__', parent.__module__)\n            newcls = type(parent.__name__, (parent, object), attrs)\n            # We assign __provides__ and __implemented__ below to prevent a\n            # memory leak that results from from the usage of this instance's\n            # eventual use in an adapter lookup.  Adapter lookup results in\n            # ``zope.interface.implementedBy`` being called with the\n            # newly-created class as an argument.  Because the newly-created\n            # class has no interface specification data of its own, lookup\n            # causes new ClassProvides and Implements instances related to our\n            # just-generated class to be created and set into the newly-created\n            # class' __dict__.  We don't want these instances to be created; we\n            # want this new class to behave exactly like it is the parent class\n            # instead.  See GitHub issues #1212, #1529 and #1568 for more\n            # information.\n            for name in ('__implemented__', '__provides__'):\n                # we assign these attributes conditionally to make it possible\n                # to test this class in isolation without having any interfaces\n                # attached to it\n                val = getattr(parent, name, _marker)\n                if val is not _marker:\n                    setattr(newcls, name, val)\n            target.__class__ = newcls\n\n    @classmethod\n    def set_property(cls, target, callable, name=None, reify=False):\n        \"\"\"A helper method to apply a single property to an instance.\"\"\"\n        prop = cls.make_property(callable, name=name, reify=reify)\n        cls.apply_properties(target, [prop])\n\n    def add_property(self, callable, name=None, reify=False):\n        \"\"\"Add a new property configuration.\n\n        This should be used in combination with :meth:`.apply` as a\n        more efficient version of :meth:`.set_property`.\n        \"\"\"\n        name, fn = self.make_property(callable, name=name, reify=reify)\n        self.properties[name] = fn\n\n\n\n# The code to be completed is:\n    def apply(self, target):\n\n        \"\"\"\n        This function applies all the configured properties to the target instance.\n        Input-Output Arguments\n        :param self: InstancePropertyHelper. An instance of the InstancePropertyHelper class.\n        :param target: The target instance to which the properties will be applied.\n        :return: No return values.\n        \"\"\""}
{"namespace": "boto.glacier.connect_to_region", "prompt": "# Please complete the connect_to_region function based on the contexts above the function.\n\n# The contexts above the function are:\n# Copyright (c) 2011 Mitch Garnaat http://garnaat.org/\n# Copyright (c) 2011 Amazon.com, Inc. or its affiliates.  All Rights Reserved\n#\n# Permission is hereby granted, free of charge, to any person obtaining a\n# copy of this software and associated documentation files (the\n# \"Software\"), to deal in the Software without restriction, including\n# without limitation the rights to use, copy, modify, merge, publish, dis-\n# tribute, sublicense, and/or sell copies of the Software, and to permit\n# persons to whom the Software is furnished to do so, subject to the fol-\n# lowing conditions:\n#\n# The above copyright notice and this permission notice shall be included\n# in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS\n# OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABIL-\n# ITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT\n# SHALL THE AUTHOR BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY,\n# WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS\n# IN THE SOFTWARE.\n#\n\nfrom boto.regioninfo import RegionInfo, get_regions\n\n\n\ndef regions():\n    \"\"\"\n    Get all available regions for the Amazon Glacier service.\n\n    :rtype: list\n    :return: A list of :class:`boto.regioninfo.RegionInfo`\n    \"\"\"\n    from boto.glacier.layer2 import Layer2\n    return get_regions('glacier', connection_cls=Layer2)\n\n\n\n\n# The code to be completed is:\ndef connect_to_region(region_name, **kw_params):\n\n    \"\"\"\n    Connect to a specific region in the AWS Glacier service. It creates a connection to the Glacier service using the provided region name and additional keyword parameters.\n    Input-Output Arguments\n    :param region_name: String. The name of the region to connect to.\n    :param **kw_params: Additional keyword parameters that can be passed to the connection.\n    :return: Connection. The connection object to the Glacier service in the specified region.\n    \"\"\""}
{"namespace": "aioxmpp.entitycaps.caps115.Key.path", "prompt": "# Please complete the path function based on the contexts above the function.\n\n# The contexts above the function are:\n########################################################################\n# File name: caps115.py\n# This file is part of: aioxmpp\n#\n# LICENSE\n#\n# This program is free software: you can redistribute it and/or modify\n# it under the terms of the GNU Lesser General Public License as\n# published by the Free Software Foundation, either version 3 of the\n# License, or (at your option) any later version.\n#\n# This program is distributed in the hope that it will be useful, but\n# WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\n# Lesser General Public License for more details.\n#\n# You should have received a copy of the GNU Lesser General Public\n# License along with this program.  If not, see\n# <http://www.gnu.org/licenses/>.\n#\n########################################################################\nimport base64\nimport collections\nimport hashlib\nimport pathlib\nimport urllib.parse\n\nfrom xml.sax.saxutils import escape\n\nfrom .common import AbstractKey, AbstractImplementation\nfrom . import xso as caps_xso\n\n\ndef build_identities_string(identities):\n    identities = [\n        b\"/\".join([\n            escape(identity.category).encode(\"utf-8\"),\n            escape(identity.type_).encode(\"utf-8\"),\n            escape(str(identity.lang or \"\")).encode(\"utf-8\"),\n            escape(identity.name or \"\").encode(\"utf-8\"),\n        ])\n        for identity in identities\n    ]\n\n    if len(set(identities)) != len(identities):\n        raise ValueError(\"duplicate identity\")\n\n    identities.sort()\n    identities.append(b\"\")\n    return b\"<\".join(identities)\n\n\ndef build_features_string(features):\n    features = list(escape(feature).encode(\"utf-8\") for feature in features)\n\n    if len(set(features)) != len(features):\n        raise ValueError(\"duplicate feature\")\n\n    features.sort()\n    features.append(b\"\")\n    return b\"<\".join(features)\n\n\ndef build_forms_string(forms):\n    types = set()\n    forms_list = []\n    for form in forms:\n        try:\n            form_types = set(\n                value\n                for field in form.fields.filter(attrs={\"var\": \"FORM_TYPE\"})\n                for value in field.values\n            )\n        except KeyError:\n            continue\n\n        if len(form_types) > 1:\n            raise ValueError(\"form with multiple types\")\n        elif not form_types:\n            continue\n\n        type_ = escape(next(iter(form_types))).encode(\"utf-8\")\n        if type_ in types:\n            raise ValueError(\"multiple forms of type {!r}\".format(type_))\n        types.add(type_)\n        forms_list.append((type_, form))\n    forms_list.sort()\n\n    parts = []\n\n    for type_, form in forms_list:\n        parts.append(type_)\n\n        field_list = sorted(\n            (\n                (escape(field.var).encode(\"utf-8\"), field.values)\n                for field in form.fields\n                if field.var != \"FORM_TYPE\"\n            ),\n            key=lambda x: x[0]\n        )\n\n        for var, values in field_list:\n            parts.append(var)\n            parts.extend(sorted(\n                escape(value).encode(\"utf-8\") for value in values\n            ))\n\n    parts.append(b\"\")\n    return b\"<\".join(parts)\n\n\ndef hash_query(query, algo):\n    hashimpl = hashlib.new(algo)\n    hashimpl.update(\n        build_identities_string(query.identities)\n    )\n    hashimpl.update(\n        build_features_string(query.features)\n    )\n    hashimpl.update(\n        build_forms_string(query.exts)\n    )\n\n    return base64.b64encode(hashimpl.digest()).decode(\"ascii\")\n\n\nKey = collections.namedtuple(\"Key\", [\"algo\", \"node\"])\n\n\nclass Key(Key, AbstractKey):\n    @property\n\n\n# The code to be completed is:\n    def path(self):\n\n        \"\"\"\n        Return the path of the key. It first quotes the node and then returns the path of the key based on the quoted node, algorithm, and the directory \"hashes\".\n        Input-Output Arguments\n        :param self: Key. An instance of the Key class.\n        :return: Path. The path of the key, which is a pathlib.Path object.\n        \"\"\""}
{"namespace": "pyt.__main__.discover_files", "prompt": "# Please complete the discover_files function based on the contexts above the function.\n\n# The contexts above the function are:\n\"\"\"The comand line module of PyT.\"\"\"\n\nimport logging\nimport os\nimport sys\nfrom collections import defaultdict\n\nfrom .analysis.constraint_table import initialize_constraint_table\nfrom .analysis.fixed_point import analyse\nfrom .cfg import make_cfg\nfrom .core.ast_helper import generate_ast\nfrom .core.project_handler import (\n    get_directory_modules,\n    get_modules\n)\nfrom .usage import parse_args\nfrom .vulnerabilities import (\n    find_vulnerabilities,\n    get_vulnerabilities_not_in_baseline\n)\nfrom .vulnerabilities.vulnerability_helper import SanitisedVulnerability\nfrom .web_frameworks import (\n    FrameworkAdaptor,\n    is_django_view_function,\n    is_flask_route_function,\n    is_function,\n    is_function_without_leading_\n)\n\nlog = logging.getLogger(__name__)\n\n\n\n\n# The code to be completed is:\ndef discover_files(targets, excluded_files, recursive=False):\n\n    \"\"\"\n    This function discovers files based on the given targets and excluded files. It searches for files with the extension \".py\" in the target directories and appends them to the included_files list. It also logs the discovered files debug mode ('Discovered file: %s').\n    Input-Output Arguments\n    :param targets: List of strings. The target directories or files to search for files.\n    :param excluded_files: String. A comma-separated list of files to exclude from the search.\n    :param recursive: Bool. Whether to search for files recursively in subdirectories. Defaults to False.\n    :return: List of strings. The list of discovered files.\n    \"\"\""}
{"namespace": "dash._grouping.flatten_grouping", "prompt": "# Please complete the flatten_grouping function based on the contexts above the function.\n\n# The contexts above the function are:\n\"\"\"\nThis module contains a collection of utility function for dealing with property\ngroupings.\n\nTerminology:\n\nFor the purpose of grouping and ungrouping, tuples/lists and dictionaries are considered\n\"composite values\" and all other values are considered \"scalar values\".\n\nA \"grouping value\" is either composite or scalar.\n\nA \"schema\" is a grouping value that can be used to encode an expected grouping\nstructure\n\n\"\"\"\nfrom dash.exceptions import InvalidCallbackReturnValue\nfrom ._utils import AttributeDict, stringify_id\n\n\n\n\n# The code to be completed is:\ndef flatten_grouping(grouping, schema=None):\n\n    \"\"\"\n    This function takes a grouping value and converts it into a list of scalar values. It recursively flattens the grouping value based on the provided schema.\n    Input-Output Arguments\n    :param grouping: The grouping value to flatten.\n    :param schema: Optional. A grouping value representing the expected structure of the input grouping value. If not provided, the grouping value is treated as its own schema. A schema is required to treat tuples and dicts in the input grouping as scalar values.\n    :return: A list of scalar values in the input grouping.\n    \"\"\""}
{"namespace": "mingus.core.intervals.major_second", "prompt": "# Please complete the major_second function based on the contexts above the function.\n\n# The contexts above the function are:\n# -*- coding: utf-8 -*-\n\n#    Music theory Python package, intervals module.\n#    Copyright (C) 2008-2009, Bart Spaans\n#\n#    This program is free software: you can redistribute it and/or modify\n#    it under the terms of the GNU General Public License as published by\n#    the Free Software Foundation, either version 3 of the License, or\n#    (at your option) any later version.\n#\n#    This program is distributed in the hope that it will be useful,\n#    but WITHOUT ANY WARRANTY; without even the implied warranty of\n#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#    GNU General Public License for more details.\n#\n#    You should have received a copy of the GNU General Public License\n#    along with this program.  If not, see <http://www.gnu.org/licenses/>.\n\n\"\"\"Module to create intervals from notes.\n\nWhen you are working in a key (for instance 'F'), you can use the functions\nsecond ('G'), third ('A'), fourth ('Bb'), fifth ('C'), sixth ('D') and\nseventh ('E') to get to the respective natural intervals of that note.\n\nWhen you want to get the absolute intervals you can use the minor and major\nfunctions. For example: minor_third('F') returns 'Ab' while major_third('F')\nreturns 'A'.\n\nThis modules also contains other useful helper functions like measure,\ndetermine, invert, is_consonant and is_dissonant.\n\"\"\"\nfrom __future__ import absolute_import\n\nfrom mingus.core import notes\nfrom mingus.core import keys\n\n\ndef interval(key, start_note, interval):\n    \"\"\"Return the note found at the interval starting from start_note in the\n    given key.\n\n    Raise a KeyError exception if start_note is not a valid note.\n\n    Example:\n    >>> interval('C', 'D', 1)\n    'E'\n    \"\"\"\n    if not notes.is_valid_note(start_note):\n        raise KeyError(\"The start note '%s' is not a valid note\" % start_note)\n    notes_in_key = keys.get_notes(key)\n    for n in notes_in_key:\n        if n[0] == start_note[0]:\n            index = notes_in_key.index(n)\n    return notes_in_key[(index + interval) % 7]\n\n\ndef unison(note, key=None):\n    \"\"\"Return the unison of note.\n\n    Raise a KeyError exception if the note is not found in the given key.\n\n    The key is not at all important, but is here for consistency reasons\n    only.\n\n    Example:\n    >>> unison('C')\n    'C'\n    \"\"\"\n    return interval(note, note, 0)\n\n\ndef second(note, key):\n    \"\"\"Take the diatonic second of note in key.\n\n    Raise a KeyError exception if the note is not found in the given key.\n\n    Examples:\n    >>> second('E', 'C')\n    'F'\n    >>> second('E', 'D')\n    'F#'\n    \"\"\"\n    return interval(key, note, 1)\n\n\ndef third(note, key):\n    \"\"\"Take the diatonic third of note in key.\n\n    Raise a KeyError exception if the note is not found in the given key.\n\n    Examples:\n    >>> third('E', 'C')\n    'G'\n    >>> third('E', 'E')\n    'G#'\n    \"\"\"\n    return interval(key, note, 2)\n\n\ndef fourth(note, key):\n    \"\"\"Take the diatonic fourth of note in key.\n\n    Raise a KeyError exception if the note is not found in the given key.\n\n    Examples:\n    >>> fourth('E', 'C')\n    'A'\n    >>> fourth('E', 'B')\n    'A#'\n    \"\"\"\n    return interval(key, note, 3)\n\n\ndef fifth(note, key):\n    \"\"\"Take the diatonic fifth of note in key.\n\n    Raise a KeyError exception if the note is not found in the given key.\n\n    Examples:\n    >>> fifth('E', 'C')\n    'B'\n    >>> fifth('E', 'F')\n    'Bb'\n    \"\"\"\n    return interval(key, note, 4)\n\n\ndef sixth(note, key):\n    \"\"\"Take the diatonic sixth of note in key.\n\n    Raise a KeyError exception if the note is not found in the given key.\n\n    Examples:\n    >>> sixth('E', 'C')\n    'C'\n    >>> sixth('E', 'B')\n    'C#'\n    \"\"\"\n    return interval(key, note, 5)\n\n\ndef seventh(note, key):\n    \"\"\"Take the diatonic seventh of note in key.\n\n    Raise a KeyError exception if the note is not found in the given key.\n\n    Examples:\n    >>> seventh('E', 'C')\n    'D'\n    >>> seventh('E', 'B')\n    'D#'\n    \"\"\"\n    return interval(key, note, 6)\n\n\ndef minor_unison(note):\n    return notes.diminish(note)\n\n\ndef major_unison(note):\n    return note\n\n\ndef augmented_unison(note):\n    return notes.augment(note)\n\n\ndef minor_second(note):\n    sec = second(note[0], \"C\")\n    return augment_or_diminish_until_the_interval_is_right(note, sec, 1)\n\n\n\n\n# The code to be completed is:\ndef major_second(note):\n\n    \"\"\"\n    This function calculates the major second interval for a given note. It first determines the second interval between the given note and \"C\". Then, it adjusts the interval by augmenting or diminishing it until it becomes a major second.\n    Input-Output Arguments\n    :param note: String. The note for which the major second interval is to be calculated.\n    :return: The adjusted major second interval for the given note.\n    \"\"\""}
{"namespace": "pymc.logprob.utils.walk_model", "prompt": "# Please complete the walk_model function based on the contexts above the function.\n\n# The contexts above the function are:\n#   Copyright 2023 The PyMC Developers\n#\n#   Licensed under the Apache License, Version 2.0 (the \"License\");\n#   you may not use this file except in compliance with the License.\n#   You may obtain a copy of the License at\n#\n#       http://www.apache.org/licenses/LICENSE-2.0\n#\n#   Unless required by applicable law or agreed to in writing, software\n#   distributed under the License is distributed on an \"AS IS\" BASIS,\n#   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#   See the License for the specific language governing permissions and\n#   limitations under the License.\n#\n#   MIT License\n#\n#   Copyright (c) 2021-2022 aesara-devs\n#\n#   Permission is hereby granted, free of charge, to any person obtaining a copy\n#   of this software and associated documentation files (the \"Software\"), to deal\n#   in the Software without restriction, including without limitation the rights\n#   to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n#   copies of the Software, and to permit persons to whom the Software is\n#   furnished to do so, subject to the following conditions:\n#\n#   The above copyright notice and this permission notice shall be included in all\n#   copies or substantial portions of the Software.\n#\n#   THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n#   IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n#   FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n#   AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n#   LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n#   OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n#   SOFTWARE.\n\nimport warnings\n\nfrom typing import (\n    Callable,\n    Container,\n    Dict,\n    Generator,\n    Iterable,\n    List,\n    Optional,\n    Sequence,\n    Set,\n    Tuple,\n    Union,\n)\n\nimport numpy as np\n\nfrom pytensor import Variable\nfrom pytensor import tensor as pt\nfrom pytensor.graph import Apply, Op\nfrom pytensor.graph.basic import Constant, clone_get_equiv, graph_inputs, walk\nfrom pytensor.graph.fg import FunctionGraph\nfrom pytensor.graph.op import HasInnerGraph\nfrom pytensor.link.c.type import CType\nfrom pytensor.raise_op import CheckAndRaise\nfrom pytensor.tensor.random.op import RandomVariable\nfrom pytensor.tensor.variable import TensorVariable\n\nfrom pymc.logprob.abstract import MeasurableVariable, _logprob\nfrom pymc.util import makeiter\n\n\n\n\n# The code to be completed is:\ndef walk_model(\n    graphs: Iterable[TensorVariable],\n    walk_past_rvs: bool = False,\n    stop_at_vars: Optional[Set[TensorVariable]] = None,\n    expand_fn: Callable[[TensorVariable], List[TensorVariable]] = lambda var: [],\n) -> Generator[TensorVariable, None, None]:\n\n    \"\"\"\n    This function walks through the model graphs and yields their nodes. It can be used to traverse the graph structure of a model and perform operations on the nodes.\n    Input-Output Arguments\n    :param graphs: Iterable of TensorVariable. The graphs to walk.\n    :param walk_past_rvs: Bool. If True, the walk will not terminate at MeasurableVariable nodes.\n    :param stop_at_vars: Optional set of TensorVariable. A list of variables at which the walk will terminate.\n    :param expand_fn: Callable function. A function that returns the next variable(s) to be traversed.\n    :return: Generator of TensorVariable. A generator that yields the nodes of the model graphs.\n    ```\n    \"\"\""}
{"namespace": "dash.fingerprint.build_fingerprint", "prompt": "# Please complete the build_fingerprint function based on the contexts above the function.\n\n# The contexts above the function are:\nimport re\n\ncache_regex = re.compile(r\"^v[\\w-]+m[0-9a-fA-F]+$\")\nversion_clean = re.compile(r\"[^\\w-]\")\n\n\n\n\n# The code to be completed is:\ndef build_fingerprint(path, version, hash_value):\n\n    \"\"\"\n    This function builds a fingerprint for a file based on the given path, version, and hash value. It extracts the filename and extension from the path, constructs a file path without the filename, replaces the version with underscores, and concatenates all the parts to form the fingerprint. The format of a fingerprint is \"{file_path}.v{v_str}m{hash_value}.{extension}\".\n    Input-Output Arguments\n    :param path: String. The path of the file.\n    :param version: Any data type. The version of the file.\n    :param hash_value: Any data type. The hash value of the file.\n    :return: String. The fingerprint of the file.\n    \"\"\""}
{"namespace": "sumy.summarizers.reduction.ReductionSummarizer.rate_sentences", "prompt": "# Please complete the rate_sentences function based on the contexts above the function.\n\n# The contexts above the function are:\n# -*- coding: utf-8 -*-\n\nfrom __future__ import absolute_import\nfrom __future__ import division, print_function, unicode_literals\n\nimport math\n\nfrom itertools import combinations\nfrom collections import defaultdict\nfrom ._summarizer import AbstractSummarizer\n\n\nclass ReductionSummarizer(AbstractSummarizer):\n    \"\"\"Source: https://github.com/adamfabish/Reduction\"\"\"\n\n    _stop_words = frozenset()\n\n    @property\n    def stop_words(self):\n        return self._stop_words\n\n    @stop_words.setter\n    def stop_words(self, words):\n        self._stop_words = frozenset(map(self.normalize_word, words))\n\n    def __call__(self, document, sentences_count):\n        ratings = self.rate_sentences(document)\n        return self._get_best_sentences(document.sentences, sentences_count, ratings)\n\n\n\n# The code to be completed is:\n    def rate_sentences(self, document):\n\n        \"\"\"\n        This function rates the sentences in a document based on their similarity. It calculates the similarity between each pair of sentences and assigns a rating to each sentence based on the similarity with other sentences.\n        Input-Output Arguments\n        :param self: ReductionSummarizer. An instance of the ReductionSummarizer class.\n        :param document: Document. The document containing the sentences to be rated.\n        :return: defaultdict. A dictionary containing the ratings for each sentence.\n        \"\"\""}
{"namespace": "pythonforandroid.androidndk.AndroidNDK.llvm_prebuilt_dir", "prompt": "# Please complete the llvm_prebuilt_dir function based on the contexts above the function.\n\n# The contexts above the function are:\nimport sys\nimport os\n\n\nclass AndroidNDK:\n    \"\"\"\n    This class is used to get the current NDK information.\n    \"\"\"\n\n    ndk_dir = \"\"\n\n    def __init__(self, ndk_dir):\n        self.ndk_dir = ndk_dir\n\n    @property\n    def host_tag(self):\n        \"\"\"\n        Returns the host tag for the current system.\n        Note: The host tag is ``darwin-x86_64`` even on Apple Silicon macs.\n        \"\"\"\n        return f\"{sys.platform}-x86_64\"\n\n    @property\n\n\n# The code to be completed is:\n    def llvm_prebuilt_dir(self):\n\n        \"\"\"\n        This function returns the directory path of the LLVM prebuilt files in the Android NDK. It constructs the directory path by joining the NDK directory path, \"toolchains\", \"llvm\", \"prebuilt\", and the host tag.\n        Input-Output Arguments\n        :param self: AndroidNDK. An instance of the AndroidNDK class.\n        :return: String. The directory path of the LLVM prebuilt files.\n        \"\"\""}
{"namespace": "hl7.client.MLLPClient.send", "prompt": "# Please complete the send function based on the contexts above the function.\n\n# The contexts above the function are:\nimport io\nimport os.path\nimport socket\nimport sys\nfrom optparse import OptionParser\n\nimport hl7\n\nSB = b\"\\x0b\"  # <SB>, vertical tab\nEB = b\"\\x1c\"  # <EB>, file separator\nCR = b\"\\x0d\"  # <CR>, \\r\n\nFF = b\"\\x0c\"  # <FF>, new page form feed\n\nRECV_BUFFER = 4096\n\n\nclass MLLPException(Exception):\n    pass\n\n\nclass MLLPClient(object):\n    \"\"\"\n    A basic, blocking, HL7 MLLP client based upon :py:mod:`socket`.\n\n    MLLPClient implements two methods for sending data to the server.\n\n    * :py:meth:`MLLPClient.send` for raw data that already is wrapped in the\n      appropriate MLLP container (e.g. *<SB>message<EB><CR>*).\n    * :py:meth:`MLLPClient.send_message` will wrap the message in the MLLP\n      container\n\n    Can be used by the ``with`` statement to ensure :py:meth:`MLLPClient.close`\n    is called::\n\n        with MLLPClient(host, port) as client:\n            client.send_message('MSH|...')\n\n    MLLPClient takes an optional ``encoding`` parameter, defaults to UTF-8,\n    for encoding unicode messages [#]_.\n\n    .. [#] http://wiki.hl7.org/index.php?title=Character_Set_used_in_v2_messages\n    \"\"\"\n\n    def __init__(self, host, port, encoding=\"utf-8\"):\n        self.socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        self.socket.connect((host, port))\n        self.encoding = encoding\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, exc_type, exc_val, trackeback):\n        self.close()\n\n    def close(self):\n        \"\"\"Release the socket connection\"\"\"\n        self.socket.close()\n\n    def send_message(self, message):\n        \"\"\"Wraps a byte string, unicode string, or :py:class:`hl7.Message`\n        in a MLLP container and send the message to the server\n\n        If message is a byte string, we assume it is already encoded properly.\n        If message is unicode or  :py:class:`hl7.Message`, it will be encoded\n        according to  :py:attr:`hl7.client.MLLPClient.encoding`\n\n        \"\"\"\n        if isinstance(message, bytes):\n            # Assume we have the correct encoding\n            binary = message\n        else:\n            # Encode the unicode message into a bytestring\n            if isinstance(message, hl7.Message):\n                message = str(message)\n            binary = message.encode(self.encoding)\n\n        # wrap in MLLP message container\n        data = SB + binary + EB + CR\n        return self.send(data)\n\n\n\n# The code to be completed is:\n    def send(self, data):\n\n        \"\"\"\n        This function sends data to the server using a low-level, direct access to the socket. It first sends the data to the server and then waits for the server to return a response.\n        Input-Output Arguments\n        :param self: MLLPClient. An instance of the MLLPClient class.\n        :param data: The data to be sent to the server. It should already be wrapped in an MLLP container.\n        :return: The response received from the server.\n        \"\"\""}
{"namespace": "kinto.plugins.accounts.utils.hash_password", "prompt": "# Please complete the hash_password function based on the contexts above the function.\n\n# The contexts above the function are:\nimport bcrypt\n\nfrom kinto.core import utils\n\nACCOUNT_CACHE_KEY = \"accounts:{}:verified\"\nACCOUNT_POLICY_NAME = \"account\"\nACCOUNT_RESET_PASSWORD_CACHE_KEY = \"accounts:{}:reset-password\"\nACCOUNT_VALIDATION_CACHE_KEY = \"accounts:{}:validation-key\"\nDEFAULT_RESET_PASSWORD_CACHE_TTL_SECONDS = 7 * 24 * 60 * 60\nDEFAULT_VALIDATION_KEY_CACHE_TTL_SECONDS = 7 * 24 * 60 * 60\n\n\n\n\n# The code to be completed is:\ndef hash_password(password):\n    # Store password safely in database as str\n    # (bcrypt.hashpw returns base64 bytes).\n\n    \"\"\"\n    This function takes a password as input, hashes it using bcrypt, and returns the hashed password as a string.\n    Input-Output Arguments\n    :param password: String. The password to be hashed.\n    :return: String. The hashed password.\n    \"\"\""}
{"namespace": "boltons.mathutils.ceil", "prompt": "# Please complete the ceil function based on the contexts above the function.\n\n# The contexts above the function are:\n# -*- coding: utf-8 -*-\n\n# Copyright (c) 2013, Mahmoud Hashemi\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are\n# met:\n#\n#    * Redistributions of source code must retain the above copyright\n#      notice, this list of conditions and the following disclaimer.\n#\n#    * Redistributions in binary form must reproduce the above\n#      copyright notice, this list of conditions and the following\n#      disclaimer in the documentation and/or other materials provided\n#      with the distribution.\n#\n#    * The names of the contributors may not be used to endorse or\n#      promote products derived from this software without specific\n#      prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\n# \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\n# LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR\n# A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT\n# OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,\n# SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT\n# LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,\n# DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY\n# THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n# (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\n\"\"\"This module provides useful math functions on top of Python's\nbuilt-in :mod:`math` module.\n\"\"\"\nfrom __future__ import division\n\nfrom math import ceil as _ceil, floor as _floor\nimport bisect\nimport binascii\n\n\ndef clamp(x, lower=float('-inf'), upper=float('inf')):\n    \"\"\"Limit a value to a given range.\n\n    Args:\n        x (int or float): Number to be clamped.\n        lower (int or float): Minimum value for x.\n        upper (int or float): Maximum value for x.\n\n    The returned value is guaranteed to be between *lower* and\n    *upper*. Integers, floats, and other comparable types can be\n    mixed.\n\n    >>> clamp(1.0, 0, 5)\n    1.0\n    >>> clamp(-1.0, 0, 5)\n    0\n    >>> clamp(101.0, 0, 5)\n    5\n    >>> clamp(123, upper=5)\n    5\n\n    Similar to `numpy's clip`_ function.\n\n    .. _numpy's clip: http://docs.scipy.org/doc/numpy/reference/generated/numpy.clip.html\n\n    \"\"\"\n    if upper < lower:\n        raise ValueError('expected upper bound (%r) >= lower bound (%r)'\n                         % (upper, lower))\n    return min(max(x, lower), upper)\n\n\n\n\n# The code to be completed is:\ndef ceil(x, options=None):\n\n    \"\"\"\n    Return the ceiling of the input number. If the options are set, return the smallest integer or float from the options that is greater than or equal to the input number.\n    Input-Output Arguments\n    :param x: int or float. Number to be tested.\n    :param options: iterable. Optional iterable of arbitrary numbers (ints or floats).\n    :return: int or float. The ceiling of x. If options is not None, return the smallest integer or float from the options that is greater than or equal to x.\n    \"\"\""}
{"namespace": "bentoml._internal.resource.CpuResource.validate", "prompt": "# Please complete the validate function based on the contexts above the function.\n\n# The contexts above the function are:\nfrom __future__ import annotations\n\nimport functools\nimport logging\nimport math\nimport os\nimport re\nimport typing as t\nfrom abc import ABC\nfrom abc import abstractmethod\n\nimport psutil\n\nfrom ..exceptions import BentoMLConfigException\n\nlogger = logging.getLogger(__name__)\n\n_RESOURCE_REGISTRY: dict[str, t.Type[Resource[t.Any]]] = {}\n\nT = t.TypeVar(\"T\")\n\n\ndef get_resource(\n    resources: dict[str, t.Any], resource_kind: str, validate: bool = True\n) -> t.Any:\n    if resource_kind not in _RESOURCE_REGISTRY:\n        raise BentoMLConfigException(f\"Unknown resource kind '{resource_kind}'.\")\n\n    resource: t.Type[Resource[t.Any]] = _RESOURCE_REGISTRY[resource_kind]\n\n    if resource_kind in resources:\n        if resources[resource_kind] == \"system\":\n            return resource.from_system()\n        else:\n            res = resource.from_spec(resources[resource_kind])\n            if validate:\n                resource.validate(res)\n            return res\n    else:\n        return None\n\n\ndef system_resources() -> dict[str, t.Any]:\n    res: dict[str, t.Any] = {}\n    for resource_kind, resource in _RESOURCE_REGISTRY.items():\n        res[resource_kind] = resource.from_system()\n    return res\n\n\nclass Resource(t.Generic[T], ABC):\n    def __init_subclass__(cls, *, resource_id: str):  # pylint: disable=arguments-differ\n        _RESOURCE_REGISTRY[resource_id] = cls\n\n    @classmethod\n    @abstractmethod\n    def from_spec(cls, spec: t.Any) -> T:\n        \"\"\"\n        Get an instance of this resource from user input. For example, a CPU resource\n        might parse \"10m\" and return a CPU resource with 0.01 CPUs.\n        \"\"\"\n\n    @classmethod\n    @abstractmethod\n    def from_system(cls) -> T:\n        \"\"\"\n        Infer resource value from the system.\n        \"\"\"\n\n    @classmethod\n    @abstractmethod\n    def validate(cls, val: T):\n        \"\"\"\n        Validate that the resources are available on the current system.\n        \"\"\"\n\n\nclass CpuResource(Resource[float], resource_id=\"cpu\"):\n    @classmethod\n    def from_spec(cls, spec: t.Any) -> float:\n        \"\"\"\n        Convert spec to CpuResource.\n\n        spec can be a float, int or string.\n        - 1.0 -> 1.0\n        - 1 -> 1.0\n        - \"1\" -> 1.0\n        - \"10m\" -> 0.01\n        \"\"\"\n        if not isinstance(spec, (int, float, str)):\n            raise TypeError(\"cpu must be int, float or str\")\n\n        if isinstance(spec, (int, float)):\n            return float(spec)\n\n        milli_match = re.match(\"([0-9]+)m\", spec)\n        if milli_match:\n            return float(milli_match[1]) / 1000.0\n\n        try:\n            return float(spec)\n        except ValueError:\n            raise BentoMLConfigException(f\"Invalid CPU resource limit '{spec}'. \")\n\n    @classmethod\n    def from_system(cls) -> float:\n        if psutil.POSIX:\n            return query_cgroup_cpu_count()\n        else:\n            return float(query_os_cpu_count())\n\n    @classmethod\n\n\n# The code to be completed is:\n    def validate(cls, val: float):\n\n        \"\"\"\n        This function validates a CPU resource limit value. It checks if the value is negative and raises an exception if it is. It also compares the value with the system's available CPU resources and raises an exception if the value is greater than the system's available resources.\n        Input-Output Arguments\n        :param cls: Class. The class itself.\n        :param val: Float. The CPU resource limit value to validate.\n        :return: No return values.\n        \"\"\""}
{"namespace": "sumy.summarizers.lsa.LsaSummarizer._create_dictionary", "prompt": "# Please complete the _create_dictionary function based on the contexts above the function.\n\n# The contexts above the function are:\n# -*- coding: utf-8 -*-\n\nfrom __future__ import absolute_import\nfrom __future__ import division, print_function, unicode_literals\n\nimport math\n\nfrom warnings import warn\n\ntry:\n    import numpy\nexcept ImportError:\n    numpy = None\n\ntry:\n    from numpy.linalg import svd as singular_value_decomposition\nexcept ImportError:\n    singular_value_decomposition = None\nfrom ._summarizer import AbstractSummarizer\n\n\nclass LsaSummarizer(AbstractSummarizer):\n    MIN_DIMENSIONS = 3\n    REDUCTION_RATIO = 1/1\n    _stop_words = frozenset()\n\n    @property\n    def stop_words(self):\n        return self._stop_words\n\n    @stop_words.setter\n    def stop_words(self, words):\n        self._stop_words = frozenset(map(self.normalize_word, words))\n\n    def __call__(self, document, sentences_count):\n        self._ensure_dependecies_installed()\n\n        dictionary = self._create_dictionary(document)\n        # empty document\n        if not dictionary:\n            return ()\n\n        matrix = self._create_matrix(document, dictionary)\n        matrix = self._compute_term_frequency(matrix)\n        u, sigma, v = singular_value_decomposition(matrix, full_matrices=False)\n\n        ranks = iter(self._compute_ranks(sigma, v))\n        return self._get_best_sentences(document.sentences, sentences_count,\n            lambda s: next(ranks))\n\n    def _ensure_dependecies_installed(self):\n        if numpy is None:\n            raise ValueError(\"LSA summarizer requires NumPy. Please, install it by command 'pip install numpy'.\")\n\n\n\n# The code to be completed is:\n    def _create_dictionary(self, document):\n\n        \"\"\"\n        This function creates a dictionary that maps each unique word in the document to its corresponding row index. It first normalizes each word in the document and removes any stop words. Then, it creates a dictionary where the keys are the unique words and the values are their respective row indices.\n        Input-Output Arguments\n        :param self: LsaSummarizer. An instance of the LsaSummarizer class.\n        :param document: Object. The document for which the dictionary is created.\n        :return: dict. A dictionary mapping unique words to their row indices.\n        \"\"\""}
{"namespace": "boto.s3.bucket.Bucket.get_key", "prompt": "# Please complete the get_key function based on the contexts above the function.\n\n# The contexts above the function are:\n# Copyright (c) 2006-2010 Mitch Garnaat http://garnaat.org/\n# Copyright (c) 2010, Eucalyptus Systems, Inc.\n# All rights reserved.\n#\n# Permission is hereby granted, free of charge, to any person obtaining a\n# copy of this software and associated documentation files (the\n# \"Software\"), to deal in the Software without restriction, including\n# without limitation the rights to use, copy, modify, merge, publish, dis-\n# tribute, sublicense, and/or sell copies of the Software, and to permit\n# persons to whom the Software is furnished to do so, subject to the fol-\n# lowing conditions:\n#\n# The above copyright notice and this permission notice shall be included\n# in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS\n# OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABIL-\n# ITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT\n# SHALL THE AUTHOR BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY,\n# WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS\n# IN THE SOFTWARE.\n\nimport boto\nfrom boto import handler\nfrom boto.resultset import ResultSet\n\nfrom boto.s3.acl import Policy, CannedACLStrings, Grant\nfrom boto.s3.key import Key\nfrom boto.s3.prefix import Prefix\nfrom boto.s3.deletemarker import DeleteMarker\nfrom boto.s3.multipart import MultiPartUpload\nfrom boto.s3.multipart import CompleteMultiPartUpload\nfrom boto.s3.multidelete import MultiDeleteResult\nfrom boto.s3.multidelete import Error\nfrom boto.s3.bucketlistresultset import BucketListResultSet\nfrom boto.s3.bucketlistresultset import VersionedBucketListResultSet\nfrom boto.s3.bucketlistresultset import MultiPartUploadListResultSet\nfrom boto.s3.lifecycle import Lifecycle\n\nfrom boto.s3.cors import CORSConfiguration\nfrom boto.s3.bucketlogging import BucketLogging\nfrom boto.s3 import website\nimport boto.jsonresponse\nimport boto.utils\nimport xml.sax\nimport xml.sax.saxutils\nimport re\nimport base64\nfrom collections import defaultdict\nfrom boto.compat import BytesIO, six, StringIO, urllib\n\n# as per http://goo.gl/BDuud (02/19/2011)\n\n\nclass S3WebsiteEndpointTranslate(object):\n\n    trans_region = defaultdict(lambda: 's3-website-us-east-1')\n    trans_region['eu-west-1'] = 's3-website-eu-west-1'\n    trans_region['eu-central-1'] = 's3-website.eu-central-1'\n    trans_region['us-west-1'] = 's3-website-us-west-1'\n    trans_region['us-west-2'] = 's3-website-us-west-2'\n    trans_region['sa-east-1'] = 's3-website-sa-east-1'\n    trans_region['ap-northeast-1'] = 's3-website-ap-northeast-1'\n    trans_region['ap-southeast-1'] = 's3-website-ap-southeast-1'\n    trans_region['ap-southeast-2'] = 's3-website-ap-southeast-2'\n    trans_region['cn-north-1'] = 's3-website.cn-north-1'\n\n    @classmethod\n    def translate_region(self, reg):\n        return self.trans_region[reg]\n\nS3Permissions = ['READ', 'WRITE', 'READ_ACP', 'WRITE_ACP', 'FULL_CONTROL']\n\n\nclass Bucket(object):\n\n    LoggingGroup = 'http://acs.amazonaws.com/groups/s3/LogDelivery'\n\n    BucketPaymentBody = \"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n       <RequestPaymentConfiguration xmlns=\"http://s3.amazonaws.com/doc/2006-03-01/\">\n         <Payer>%s</Payer>\n       </RequestPaymentConfiguration>\"\"\"\n\n    VersioningBody = \"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n       <VersioningConfiguration xmlns=\"http://s3.amazonaws.com/doc/2006-03-01/\">\n         <Status>%s</Status>\n         <MfaDelete>%s</MfaDelete>\n       </VersioningConfiguration>\"\"\"\n\n    VersionRE = '<Status>([A-Za-z]+)</Status>'\n    MFADeleteRE = '<MfaDelete>([A-Za-z]+)</MfaDelete>'\n\n    def __init__(self, connection=None, name=None, key_class=Key):\n        self.name = name\n        self.connection = connection\n        self.key_class = key_class\n\n    def __repr__(self):\n        return '<Bucket: %s>' % self.name\n\n    def __iter__(self):\n        return iter(BucketListResultSet(self))\n\n    def __contains__(self, key_name):\n        return not (self.get_key(key_name) is None)\n\n    def startElement(self, name, attrs, connection):\n        return None\n\n    def endElement(self, name, value, connection):\n        if name == 'Name':\n            self.name = value\n        elif name == 'CreationDate':\n            self.creation_date = value\n        else:\n            setattr(self, name, value)\n\n    def set_key_class(self, key_class):\n        \"\"\"\n        Set the Key class associated with this bucket.  By default, this\n        would be the boto.s3.key.Key class but if you want to subclass that\n        for some reason this allows you to associate your new class with a\n        bucket so that when you call bucket.new_key() or when you get a listing\n        of keys in the bucket you will get an instances of your key class\n        rather than the default.\n\n        :type key_class: class\n        :param key_class: A subclass of Key that can be more specific\n        \"\"\"\n        self.key_class = key_class\n\n    def lookup(self, key_name, headers=None):\n        \"\"\"\n        Deprecated: Please use get_key method.\n\n        :type key_name: string\n        :param key_name: The name of the key to retrieve\n\n        :rtype: :class:`boto.s3.key.Key`\n        :returns: A Key object from this bucket.\n        \"\"\"\n        return self.get_key(key_name, headers=headers)\n\n\n\n# The code to be completed is:\n    def get_key(self, key_name, headers=None, version_id=None,\n                response_headers=None, validate=True):\n\n        \"\"\"\n        This function checks if a specific key exists within a bucket. It sends a HEAD request to check for the existence of the key. If the key exists, it returns an instance of the Key object. Otherwise, it returns None.\n        Input-Output Arguments\n        :param self: Bucket. An instance of the Bucket class.\n        :param key_name: String. The name of the key to retrieve.\n        :param headers: Dictionary. The headers to send when retrieving the key.\n        :param version_id: String. The version ID of the key.\n        :param response_headers: Dictionary. A dictionary containing HTTP headers/values that will override any headers associated with the stored object in the response.\n        :param validate: Bool. Verifies whether the key exists. If False, this will not hit the service, constructing an in-memory object. Default is True.\n        :return: Key. An instance of a Key object or None\n        \"\"\""}
{"namespace": "aioxmpp.entitycaps.caps115.build_features_string", "prompt": "# Please complete the build_features_string function based on the contexts above the function.\n\n# The contexts above the function are:\n########################################################################\n# File name: caps115.py\n# This file is part of: aioxmpp\n#\n# LICENSE\n#\n# This program is free software: you can redistribute it and/or modify\n# it under the terms of the GNU Lesser General Public License as\n# published by the Free Software Foundation, either version 3 of the\n# License, or (at your option) any later version.\n#\n# This program is distributed in the hope that it will be useful, but\n# WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\n# Lesser General Public License for more details.\n#\n# You should have received a copy of the GNU Lesser General Public\n# License along with this program.  If not, see\n# <http://www.gnu.org/licenses/>.\n#\n########################################################################\nimport base64\nimport collections\nimport hashlib\nimport pathlib\nimport urllib.parse\n\nfrom xml.sax.saxutils import escape\n\nfrom .common import AbstractKey, AbstractImplementation\nfrom . import xso as caps_xso\n\n\ndef build_identities_string(identities):\n    identities = [\n        b\"/\".join([\n            escape(identity.category).encode(\"utf-8\"),\n            escape(identity.type_).encode(\"utf-8\"),\n            escape(str(identity.lang or \"\")).encode(\"utf-8\"),\n            escape(identity.name or \"\").encode(\"utf-8\"),\n        ])\n        for identity in identities\n    ]\n\n    if len(set(identities)) != len(identities):\n        raise ValueError(\"duplicate identity\")\n\n    identities.sort()\n    identities.append(b\"\")\n    return b\"<\".join(identities)\n\n\n\n\n# The code to be completed is:\ndef build_features_string(features):\n\n    \"\"\"\n    This function builds a string of features. It first escapes each feature and encodes it in utf-8. Then, it checks for duplicate features and raises a ValueError if found. Finally, it sorts the features and joins them with \"<\".\n    Input-Output Arguments\n    :param features: List. A list of features to be processed.\n    :return: Bytes. The built features string which is seperated by '<'.\n    \"\"\""}
{"namespace": "psd_tools.utils.pack", "prompt": "# Please complete the pack function based on the contexts above the function.\n\n# The contexts above the function are:\n\"\"\"\nVarious utility functions for low-level binary processing.\n\"\"\"\nfrom __future__ import unicode_literals, print_function, division\nimport logging\nimport sys\nimport struct\nimport array\n\ntry:\n    unichr = unichr\nexcept NameError:\n    unichr = chr\n\nlogger = logging.getLogger(__name__)\n\n\n\n\n# The code to be completed is:\ndef pack(fmt, *args):\n\n    \"\"\"\n    This function packs the input arguments into a binary string according to the given format like \">{format}\".\n    Input-Output Arguments\n    :param fmt: String. The format string that specifies the format of the returned string.\n    :param *args: Tuple. The input arguments to be packed.\n    :return: Binary string. The packed binary string.\n    \"\"\""}
{"namespace": "bentoml._internal.utils.uri.uri_to_path", "prompt": "# Please complete the uri_to_path function based on the contexts above the function.\n\n# The contexts above the function are:\nimport os\nimport pathlib\nfrom urllib.parse import quote\nfrom urllib.parse import unquote\nfrom urllib.parse import urlparse\nfrom urllib.request import url2pathname\n\nimport psutil\n\n\ndef path_to_uri(path: str) -> str:\n    \"\"\"\n    Convert a path to a URI.\n\n    Args:\n        path: Path to convert to URI.\n\n    Returns:\n        URI string. (quoted, absolute)\n    \"\"\"\n    path = os.path.abspath(path)\n    if psutil.WINDOWS:\n        return pathlib.PureWindowsPath(path).as_uri()\n    if psutil.POSIX:\n        return pathlib.PurePosixPath(path).as_uri()\n    raise ValueError(\"Unsupported OS\")\n\n\n\n\n# The code to be completed is:\ndef uri_to_path(uri: str) -> str:\n\n    \"\"\"\n    Convert a file URI to a path. It first parses the input URI and then checks if the scheme is supported. Then, it constructs the path string and returns it.\n    Input-Output Arguments\n    :param uri: String. The URI to convert to a path.\n    :return: String. The path string (unquoted).\n    \"\"\""}
{"namespace": "exodus_bundler.launchers.construct_bash_launcher", "prompt": "# Please complete the construct_bash_launcher function based on the contexts above the function.\n\n# The contexts above the function are:\n# -*- coding: utf-8 -*-\n\"\"\"Methods to produce launchers that will invoke the relocated executables with\nthe proper linker and library paths.\"\"\"\nimport os\nimport re\nimport tempfile\nfrom distutils.spawn import find_executable as find_executable_original\nfrom subprocess import PIPE\nfrom subprocess import Popen\n\nfrom exodus_bundler.templating import render_template_file\n\n\nparent_directory = os.path.dirname(os.path.realpath(__file__))\n\n\nclass CompilerNotFoundError(Exception):\n    pass\n\n\n# This is kind of a hack to find things in PATH inside of bundles.\ndef find_executable(binary_name, skip_original_for_testing=False):\n    # This won't be set on Alpine Linux, but it's required for the `find_executable()` calls.\n    if 'PATH' not in os.environ:\n        os.environ['PATH'] = '/bin/:/usr/bin/'\n    executable = find_executable_original(binary_name)\n    if executable and not skip_original_for_testing:\n        return executable\n    # Try to find it within the same bundle if it's not actually in the PATH.\n    directory = parent_directory\n    while True:\n        directory, basename = os.path.split(directory)\n        if not len(basename):\n            break\n        # The bundle directory.\n        if re.match('[A-Fa-f0-9]{64}', basename):\n            for bin_directory in os.environ['PATH'].split(':'):\n                if os.path.isabs(bin_directory):\n                    bin_directory = os.path.relpath(bin_directory, '/')\n                candidate_executable = os.path.join(directory, basename,\n                                                    bin_directory, binary_name)\n                if os.path.exists(candidate_executable):\n                    return candidate_executable\n\n\ndef compile(code):\n    try:\n        return compile_musl(code)\n    except CompilerNotFoundError:\n        try:\n            return compile_diet(code)\n        except CompilerNotFoundError:\n            raise CompilerNotFoundError('No suiteable C compiler was found.')\n\n\ndef compile_diet(code):\n    diet = find_executable('diet')\n    gcc = find_executable('gcc')\n    if diet is None or gcc is None:\n        raise CompilerNotFoundError('The diet compiler was not found.')\n    return compile_helper(code, [diet, 'gcc'])\n\n\ndef compile_helper(code, initial_args):\n    f, input_filename = tempfile.mkstemp(prefix='exodus-bundle-', suffix='.c')\n    os.close(f)\n    f, output_filename = tempfile.mkstemp(prefix='exodus-bundle-')\n    os.close(f)\n    try:\n        with open(input_filename, 'w') as input_file:\n            input_file.write(code)\n\n        args = initial_args + ['-static', '-O3', input_filename, '-o', output_filename]\n        process = Popen(args, stdout=PIPE, stderr=PIPE)\n        stdout, stderr = process.communicate()\n        assert process.returncode == 0, \\\n            'There was an error compiling: %s' % stderr.decode('utf-8')\n\n        with open(output_filename, 'rb') as output_file:\n            return output_file.read()\n    finally:\n        os.remove(input_filename)\n        os.remove(output_filename)\n\n\ndef compile_musl(code):\n    musl = find_executable('musl-gcc')\n    if musl is None:\n        raise CompilerNotFoundError('The musl compiler was not found.')\n    return compile_helper(code, [musl])\n\n\n\n\n# The code to be completed is:\ndef construct_bash_launcher(linker, library_path, executable, full_linker=True):\n\n    \"\"\"\n    Construct a bash launcher script based on the given parameters. It creates a bash launcher script by rendering a template file with the provided parameters.\n    Input-Output Arguments\n    :param linker: String. The path to the linker executable.\n    :param library_path: String. The path to the library.\n    :param executable: String. The path to the executable.\n    :param full_linker: Bool. Whether to use the full linker path. Defaults to True.\n    :return: String. The constructed bash launcher script.\n    \"\"\""}
{"namespace": "music_dl.source.MusicSource.search", "prompt": "# Please complete the search function based on the contexts above the function.\n\n# The contexts above the function are:\n#!/usr/bin/env python\n# -*- coding:utf-8 -*-\n\"\"\"\n@author: HJK\n@file: source.py\n@time: 2019-05-13\n\"\"\"\n\n\"\"\"\n    Music source proxy object\n\"\"\"\n\nimport re\nimport threading\nimport importlib\nimport traceback\nimport logging\nimport click\nfrom . import config\nfrom .utils import colorize\nfrom .exceptions import *\n\n\nclass MusicSource:\n    \"\"\"\n        Music source proxy object\n    \"\"\"\n\n    def __init__(self):\n        self.logger = logging.getLogger(__name__)\n\n\n\n# The code to be completed is:\n    def search(self, keyword, sources_list) -> list:\n\n        \"\"\"\n        This function searches for a keyword in a list of music sources. It creates multiple threads to search for the keyword in each source concurrently. It then sorts and removes duplicates from the search results based on song title, singer, and file size.\n        Input-Output Arguments\n        :param self: MusicSource. An instance of the MusicSource class.\n        :param keyword: String. The keyword to search for in the music sources.\n        :param sources_list: List of strings. The list of music sources to search in.\n        :return: List of songs. The search results containing songs that match the keyword.\n        \"\"\""}
{"namespace": "sumy._compat.to_unicode", "prompt": "# Please complete the to_unicode function based on the contexts above the function.\n\n# The contexts above the function are:\n# -*- coding: utf-8 -*-\n\nfrom __future__ import absolute_import\nfrom __future__ import division, print_function, unicode_literals\n\nfrom sys import version_info\n\n\nPY3 = version_info[0] == 3\n\n\nif PY3:\n    bytes = bytes\n    unicode = str\nelse:\n    bytes = str\n    unicode = unicode\nstring_types = (bytes, unicode,)\n\n\ntry:\n    from itertools import ifilterfalse as ffilter\nexcept ImportError:\n    from itertools import filterfalse as ffilter\n\ntry:\n    from collections.abc import Sequence\nexcept ImportError:\n    from collections import Sequence\n\n\ndef unicode_compatible(cls):\n    \"\"\"\n    Decorator for unicode compatible classes. Method ``__unicode__``\n    has to be implemented to work decorator as expected.\n    \"\"\"\n    if PY3:\n        cls.__str__ = cls.__unicode__\n        cls.__bytes__ = lambda self: self.__str__().encode(\"utf-8\")\n    else:\n        cls.__str__ = lambda self: self.__unicode__().encode(\"utf-8\")\n\n    return cls\n\n\ndef to_string(object):\n    return to_unicode(object) if PY3 else to_bytes(object)\n\n\ndef to_bytes(object):\n    if isinstance(object, bytes):\n        return object\n    elif isinstance(object, unicode):\n        return object.encode(\"utf-8\")\n    else:\n        # try encode instance to bytes\n        return instance_to_bytes(object)\n\n\n\n\n# The code to be completed is:\ndef to_unicode(object):\n\n    \"\"\"\n    This function converts the input object to a Unicode string. It first checks if the object is already a Unicode string, and if so, returns it as is. If the object is a byte string, it decodes it using the \"utf-8\" encoding and returns the resulting Unicode string. If the object is neither a Unicode string nor a byte string, it calls a custom function to decode it to a Unicode string.\n    Input-Output Arguments\n    :param object: Object. The object to be converted to a Unicode string.\n    :return: Unicode string. The converted Unicode string.\n    \"\"\""}
{"namespace": "bentoml._internal.configuration.helpers.is_valid_ip_address", "prompt": "# Please complete the is_valid_ip_address function based on the contexts above the function.\n\n# The contexts above the function are:\nfrom __future__ import annotations\n\nimport ipaddress\nimport logging\nimport os\nimport re\nimport typing as t\nfrom functools import singledispatch\nfrom typing import TYPE_CHECKING\n\nimport schema as s\nimport yaml\n\nfrom ...exceptions import BentoMLConfigException\nfrom ..utils import LazyLoader\n\nif TYPE_CHECKING:\n    from types import ModuleType\n\nlogger = logging.getLogger(__name__)\n\nTRACING_TYPE = [\"zipkin\", \"jaeger\", \"otlp\", \"in_memory\"]\n\n\ndef import_configuration_spec(version: int) -> ModuleType:  # pragma: no cover\n    return LazyLoader(\n        f\"v{version}\",\n        globals(),\n        f\"bentoml._internal.configuration.v{version}\",\n        exc_msg=f\"Configuration version {version} does not exist.\",\n    )\n\n\n@singledispatch\ndef depth(_: t.Any, _level: int = 0):  # pragma: no cover\n    return _level\n\n\n@depth.register(dict)\ndef _(d: dict[str, t.Any], level: int = 0, **kw: t.Any):\n    return max(depth(v, level + 1, **kw) for v in d.values())\n\n\ndef rename_fields(\n    d: dict[str, t.Any],\n    current: str,\n    replace_with: str | None = None,\n    *,\n    remove_only: bool = False,\n):\n    # We assume that the given dictionary is already flattened.\n    # This function will rename the keys in the dictionary.\n    # If `replace_with` is None, then the key will be removed.\n    if depth(d) != 1:\n        raise ValueError(\n            \"Given dictionary is not flattened. Use flatten_dict first.\"\n        ) from None\n    if current in d:\n        if remove_only:\n            logger.warning(\"Field '%s' is deprecated and will be removed.\" % current)\n            d.pop(current)\n        else:\n            assert replace_with, \"'replace_with' must be provided.\"\n            logger.warning(\n                \"Field '%s' is deprecated and has been renamed to '%s'\"\n                % (current, replace_with)\n            )\n            d[replace_with] = d.pop(current)\n\n\npunctuation = r\"\"\"!\"#$%&'()*+,-./:;<=>?@[\\]^`{|}~\"\"\"\n\n\ndef flatten_dict(\n    d: t.MutableMapping[str, t.Any],\n    parent: str = \"\",\n    sep: str = \".\",\n) -> t.Generator[tuple[str, t.Any], None, None]:\n    \"\"\"Flatten nested dictionary into a single level dictionary.\"\"\"\n    for k, v in d.items():\n        k = f'\"{k}\"' if any(i in punctuation for i in k) else k\n        nkey = parent + sep + k if parent else k\n        if isinstance(v, t.MutableMapping):\n            yield from flatten_dict(\n                t.cast(t.MutableMapping[str, t.Any], v), parent=nkey, sep=sep\n            )\n        else:\n            yield nkey, v\n\n\ndef load_config_file(path: str) -> dict[str, t.Any]:\n    \"\"\"Load configuration from given path.\"\"\"\n    if not os.path.exists(path):\n        raise BentoMLConfigException(\n            \"Configuration file %s not found.\" % path\n        ) from None\n    with open(path, \"rb\") as f:\n        config = yaml.safe_load(f)\n    return config\n\n\ndef get_default_config(version: int) -> dict[str, t.Any]:\n    config = load_config_file(\n        os.path.join(\n            os.path.dirname(__file__), f\"v{version}\", \"default_configuration.yaml\"\n        )\n    )\n    mod = import_configuration_spec(version)\n    assert hasattr(mod, \"SCHEMA\"), (\n        \"version %d does not have a validation schema\" % version\n    )\n    try:\n        mod.SCHEMA.validate(config)\n    except s.SchemaError as e:\n        raise BentoMLConfigException(\n            \"Default configuration for version %d does not conform to given schema:\\n%s\"\n            % (version, e)\n        ) from None\n    return config\n\n\ndef validate_tracing_type(tracing_type: str) -> bool:\n    return tracing_type in TRACING_TYPE\n\n\ndef validate_otlp_protocol(protocol: str) -> bool:\n    return protocol in [\"grpc\", \"http\"]\n\n\ndef ensure_larger_than(target: int | float) -> t.Callable[[int | float], bool]:\n    \"\"\"Ensure that given value is (lower, inf]\"\"\"\n\n    def v(value: int | float) -> bool:\n        return value > target\n\n    return v\n\n\nensure_larger_than_zero = ensure_larger_than(0)\n\n\ndef ensure_range(\n    lower: int | float, upper: int | float\n) -> t.Callable[[int | float], bool]:\n    \"\"\"Ensure that given value is within the range of [lower, upper].\"\"\"\n\n    def v(value: int | float) -> bool:\n        return lower <= value <= upper\n\n    return v\n\n\ndef ensure_iterable_type(typ_: type) -> t.Callable[[t.MutableSequence[t.Any]], bool]:\n    \"\"\"Ensure that given mutable sequence has all elements of given types.\"\"\"\n\n    def v(value: t.MutableSequence[t.Any]) -> bool:\n        return all(isinstance(i, typ_) for i in value)\n\n    return v\n\n\n\n\n# The code to be completed is:\ndef is_valid_ip_address(addr: str) -> bool:\n\n    \"\"\"\n    Check if the given string is a valid IP address. It uses the ipaddress module to check if the input string is a valid IP address.\n    Input-Output Arguments\n    :param addr: String. The input string to be checked.\n    :return: Bool. True if the input string is a valid IP address, False otherwise.\n    \"\"\""}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.probabilities.compute_values_probs", "prompt": "# Please complete the compute_values_probs function based on the contexts above the function.\n\n# The contexts above the function are:\n# -------------------------------------------------------------------------\n# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT License. See License.txt in the project root for\n# license information.\n# --------------------------------------------------------------------------\n\"\"\"Helper module for computing training probabilities when modelling sessions.\"\"\"\n\nfrom collections import defaultdict\nfrom typing import DefaultDict, Tuple, Union\n\nfrom ..utils.data_structures import StateMatrix\n\n\ndef compute_cmds_probs(  # nosec\n    seq1_counts: Union[StateMatrix, dict],\n    seq2_counts: Union[StateMatrix, dict],\n    unk_token: str,\n) -> Tuple[StateMatrix, StateMatrix]:\n    \"\"\"\n    Compute command related probabilities.\n\n    In particular, computes the probabilities for the individual commands,\n    and also the probabilities for the transitions of commands.\n\n    Parameters\n    ----------\n    seq1_counts: Union[StateMatrix, dict]\n        individual command counts\n    seq2_counts: Union[StateMatrix, dict]\n        sequence command (length 2) counts\n    unk_token: str\n        dummy command to signify an unseen command (e.g. \"##UNK##\")\n\n    Returns\n    -------\n    Tuple:\n        individual command probabilities,\n        sequence command (length 2) probabilities\n\n    \"\"\"\n    total_cmds = sum(seq1_counts.values())\n\n    prior_probs: DefaultDict[str, float] = defaultdict(lambda: 0)\n    trans_probs: DefaultDict[str, DefaultDict[str, float]] = defaultdict(\n        lambda: defaultdict(lambda: 0)\n    )\n\n    # compute prior probs\n    for cmd in seq1_counts:\n        prior_probs[cmd] = seq1_counts[cmd] / total_cmds\n\n    # compute trans probs\n    for prev, currents in seq2_counts.items():\n        for current in currents:\n            trans_probs[prev][current] = seq2_counts[prev][current] / sum(\n                seq2_counts[prev].values()\n            )\n\n    prior_probs_sm = StateMatrix(states=prior_probs, unk_token=unk_token)\n    trans_probs_sm = StateMatrix(states=trans_probs, unk_token=unk_token)\n\n    return prior_probs_sm, trans_probs_sm\n\n\ndef compute_params_probs(  # nosec\n    param_counts: Union[StateMatrix, dict],\n    cmd_param_counts: Union[StateMatrix, dict],\n    seq1_counts: Union[StateMatrix, dict],\n    unk_token: str,\n) -> Tuple[StateMatrix, StateMatrix]:\n    \"\"\"\n    Compute param related probabilities.\n\n    In particular, computes the probabilities of the individual params,\n    and also the probabilities of the params conditional on the command.\n\n    Note that we will be modelling whether a parameter is present or not\n    for each command. So we make the modelling assumption that the\n    parameters are independent Bernoulii random variables conditional\n    on the command.\n\n    Note also that because multiple parameters can appear at a time for\n    a command, and because we are computing the probability that each\n    parameter is present or not, we do NOT expect the probabilities to\n    sum to 1.\n\n    Note also that we use laplace smoothing in the counting\n    stage of the calculations. Therefore if you have parameter p which\n    appeared for every occurrence of command c, the resulting\n    probability for param p appearing conditional on command c would\n    NOT equal 1. It would be slightly less due to the laplace smoothing.\n\n    Parameters\n    ----------\n    param_counts: Union[StateMatrix, dict]\n        individual param counts\n    cmd_param_counts: Union[StateMatrix, dict]\n        param conditional on command counts\n    seq1_counts: Union[StateMatrix, dict]\n        individual command counts\n    unk_token: str\n        dummy command to signify an unseen command (e.g. \"##UNK##\")\n\n    Returns\n    -------\n    Tuple:\n        individual param probabilities,\n        param conditional on command probabilities\n\n    \"\"\"\n    param_probs: DefaultDict[str, float] = defaultdict(lambda: 0)\n    param_cond_cmd_probs: DefaultDict[str, DefaultDict[str, float]] = defaultdict(\n        lambda: defaultdict(lambda: 0)\n    )\n\n    for cmd, params in cmd_param_counts.items():\n        n_cmd = seq1_counts[cmd]\n        for param, count in params.items():\n            param_cond_cmd_probs[cmd][param] = count / n_cmd\n\n    tot_cmd = sum(seq1_counts.values())\n    for param, count in param_counts.items():\n        param_probs[param] = count / tot_cmd\n\n    param_probs_sm = StateMatrix(states=param_probs, unk_token=unk_token)\n    param_cond_cmd_probs_sm = StateMatrix(\n        states=param_cond_cmd_probs, unk_token=unk_token\n    )\n\n    return param_probs_sm, param_cond_cmd_probs_sm\n\n\n\n\n# The code to be completed is:\ndef compute_values_probs(  # nosec\n    value_counts: Union[StateMatrix, dict],\n    param_value_counts: Union[StateMatrix, dict],\n    unk_token: str,\n) -> Tuple[StateMatrix, StateMatrix]:\n\n    \"\"\"\n    This function computes the probabilities of individual values and the probabilities of values conditional on a parameter. It takes the counts of individual values and the counts of values conditional on the parameter as input and returns the corresponding probabilities.\n    Input-Output Arguments\n    :param value_counts: Union[StateMatrix, dict]. The counts of individual values.\n    :param param_value_counts: Union[StateMatrix, dict]. The counts of values conditional on the parameter.\n    :param unk_token: str. A dummy command to represent an unseen command.\n    :return: Tuple[StateMatrix, StateMatrix]. The probabilities of individual values and the probabilities of values conditional on the parameter.\n    \"\"\""}
{"namespace": "kinto.plugins.openid.OpenIDConnectPolicy._verify_token", "prompt": "# Please complete the _verify_token function based on the contexts above the function.\n\n# The contexts above the function are:\nimport requests\nfrom pyramid import authentication as base_auth\nfrom pyramid.interfaces import IAuthenticationPolicy\nfrom pyramid.settings import aslist\nfrom zope.interface import implementer\n\n\nfrom kinto.core import utils as core_utils\nfrom kinto.core.openapi import OpenAPI\n\nfrom .utils import fetch_openid_config\n\n\n@implementer(IAuthenticationPolicy)\nclass OpenIDConnectPolicy(base_auth.CallbackAuthenticationPolicy):\n    def __init__(self, issuer, client_id, realm=\"Realm\", **kwargs):\n        self.realm = realm\n        self.issuer = issuer\n        self.client_id = client_id\n        self.client_secret = kwargs.get(\"client_secret\", \"\")\n        self.header_type = kwargs.get(\"header_type\", \"Bearer\")\n        self.userid_field = kwargs.get(\"userid_field\", \"sub\")\n        self.verification_ttl = int(kwargs.get(\"verification_ttl_seconds\", 86400))\n\n        # Fetch OpenID config (at instantiation, ie. startup)\n        self.oid_config = fetch_openid_config(issuer)\n\n        self._jwt_keys = None\n\n    def unauthenticated_userid(self, request):\n        \"\"\"Return the userid or ``None`` if token could not be verified.\"\"\"\n        settings = request.registry.settings\n        hmac_secret = settings[\"userid_hmac_secret\"]\n\n        authorization = request.headers.get(\"Authorization\", \"\")\n        try:\n            authmeth, access_token = authorization.split(\" \", 1)\n        except ValueError:\n            return None\n\n        if authmeth.lower() != self.header_type.lower():\n            return None\n\n        # XXX JWT Access token\n        # https://auth0.com/docs/tokens/access-token#access-token-format\n\n        # Check cache if these tokens were already verified.\n        hmac_tokens = core_utils.hmac_digest(hmac_secret, access_token)\n        cache_key = f\"openid:verify:{hmac_tokens}\"\n        payload = request.registry.cache.get(cache_key)\n        if payload is None:\n            # This can take some time.\n            payload = self._verify_token(access_token)\n            if payload is None:\n                return None\n        # Save for next time / refresh ttl.\n        request.registry.cache.set(cache_key, payload, ttl=self.verification_ttl)\n        request.bound_data[\"user_profile\"] = payload\n        # Extract meaningful field from userinfo (eg. email or sub)\n        return payload.get(self.userid_field)\n\n    def forget(self, request):\n        \"\"\"A no-op. Credentials are sent on every request.\n        Return WWW-Authenticate Realm header for Bearer token.\n        \"\"\"\n        return [(\"WWW-Authenticate\", '%s realm=\"%s\"' % (self.header_type, self.realm))]\n\n\n\n# The code to be completed is:\n    def _verify_token(self, access_token):\n\n        \"\"\"\n        This function verifies the access token by fetching the user information from the profile endpoint. It sends a GET request to the userinfo endpoint with the access token in the Authorization header. If the request is successful, it returns the user profile information. If there is an error during the request or parsing the response, it logs a debug message and returns None.\n        Input-Output Arguments\n        :param self: OpenIDConnectPolicy. An instance of the OpenIDConnectPolicy class.\n        :param access_token: String. The access token to verify.\n        :return: Dictionary. The user profile information if the access token is valid, otherwise None.\n        \"\"\""}
{"namespace": "boto.cognito.identity.connect_to_region", "prompt": "# Please complete the connect_to_region function based on the contexts above the function.\n\n# The contexts above the function are:\n# Copyright (c) 2014 Amazon.com, Inc. or its affiliates.\n# All Rights Reserved\n#\n# Permission is hereby granted, free of charge, to any person obtaining a\n# copy of this software and associated documentation files (the\n# \"Software\"), to deal in the Software without restriction, including\n# without limitation the rights to use, copy, modify, merge, publish, dis-\n# tribute, sublicense, and/or sell copies of the Software, and to permit\n# persons to whom the Software is furnished to do so, subject to the fol-\n# lowing conditions:\n#\n# The above copyright notice and this permission notice shall be included\n# in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS\n# OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABIL-\n# ITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT\n# SHALL THE AUTHOR BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY,\n# WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS\n# IN THE SOFTWARE.\n#\nfrom boto.regioninfo import RegionInfo, get_regions\n\n\n\ndef regions():\n    \"\"\"\n    Get all available regions for the Amazon Cognito Identity service.\n\n    :rtype: list\n    :return: A list of :class:`boto.regioninfo.RegionInfo`\n    \"\"\"\n    from boto.cognito.identity.layer1 import CognitoIdentityConnection\n    return get_regions('cognito-identity',\n                       connection_cls=CognitoIdentityConnection)\n\n\n\n\n# The code to be completed is:\ndef connect_to_region(region_name, **kw_params):\n\n    \"\"\"\n    Connect to a specific region using the CognitoIdentityConnection class. It calls the connect function with the specified parameters and returns the connection object.\n    Input-Output Arguments\n    :param region_name: String. The name of the region to connect to.\n    :param **kw_params: Additional keyword arguments that can be passed to the connect function.\n    :return: CognitoIdentityConnection. The connection object to the specified region.\n    \"\"\""}
{"namespace": "pyt.vulnerabilities.vulnerabilities.label_contains", "prompt": "# Please complete the label_contains function based on the contexts above the function.\n\n# The contexts above the function are:\n\"\"\"Module for finding vulnerabilities based on a definitions file.\"\"\"\n\nimport ast\nimport json\nfrom collections import defaultdict\n\nfrom ..analysis.definition_chains import build_def_use_chain\nfrom ..analysis.lattice import Lattice\nfrom ..core.node_types import (\n    AssignmentNode,\n    BBorBInode,\n    IfNode,\n    TaintedNode\n)\nfrom ..helper_visitors import (\n    CallVisitor,\n    RHSVisitor,\n    VarsVisitor\n)\nfrom .trigger_definitions_parser import parse, Source\nfrom .vulnerability_helper import TriggerNode, Triggers, vuln_factory, VulnerabilityType\n\n\ndef identify_triggers(\n    cfg,\n    sources,\n    sinks,\n    lattice,\n    nosec_lines\n):\n    \"\"\"Identify sources, sinks and sanitisers in a CFG.\n\n    Args:\n        cfg(CFG): CFG to find sources, sinks and sanitisers in.\n        sources(tuple): list of sources, a source is a (source, sanitiser) tuple.\n        sinks(tuple): list of sources, a sink is a (sink, sanitiser) tuple.\n        nosec_lines(set): lines with # nosec whitelisting\n\n    Returns:\n        Triggers tuple with sink and source nodes and a sanitiser node dict.\n    \"\"\"\n    assignment_nodes = filter_cfg_nodes(cfg, AssignmentNode)\n    tainted_nodes = filter_cfg_nodes(cfg, TaintedNode)\n    tainted_trigger_nodes = [\n        TriggerNode(\n            Source('Framework function URL parameter'),\n            cfg_node=node\n        ) for node in tainted_nodes\n    ]\n    sources_in_file = find_triggers(assignment_nodes, sources, nosec_lines)\n    sources_in_file.extend(tainted_trigger_nodes)\n\n    find_secondary_sources(assignment_nodes, sources_in_file, lattice)\n\n    sinks_in_file = find_triggers(cfg.nodes, sinks, nosec_lines)\n\n    sanitiser_node_dict = build_sanitiser_node_dict(cfg, sinks_in_file)\n\n    return Triggers(sources_in_file, sinks_in_file, sanitiser_node_dict)\n\n\ndef filter_cfg_nodes(\n    cfg,\n    cfg_node_type\n):\n    return [node for node in cfg.nodes if isinstance(node, cfg_node_type)]\n\n\ndef find_secondary_sources(\n    assignment_nodes,\n    sources,\n    lattice\n):\n    \"\"\"\n        Sets the secondary_nodes attribute of each source in the sources list.\n\n        Args:\n            assignment_nodes([AssignmentNode])\n            sources([tuple])\n            lattice(Lattice): the lattice we're analysing.\n    \"\"\"\n    for source in sources:\n        source.secondary_nodes = find_assignments(assignment_nodes, source, lattice)\n\n\ndef find_assignments(\n    assignment_nodes,\n    source,\n    lattice\n):\n    old = list()\n    # propagate reassignments of the source node\n    new = [source.cfg_node]\n\n    while new != old:\n        update_assignments(new, assignment_nodes, source.cfg_node, lattice)\n        old = new\n\n    # remove source node from result\n    del new[0]\n\n    return new\n\n\ndef update_assignments(\n    assignment_list,\n    assignment_nodes,\n    source,\n    lattice\n):\n    for node in assignment_nodes:\n        for other in assignment_list:\n            if node not in assignment_list and lattice.in_constraint(other, node):\n                append_node_if_reassigned(assignment_list, other, node)\n\n\ndef append_node_if_reassigned(\n    assignment_list,\n    secondary,\n    node\n):\n    if (\n        secondary.left_hand_side in node.right_hand_side_variables or\n        secondary.left_hand_side == node.left_hand_side\n    ):\n        assignment_list.append(node)\n\n\ndef find_triggers(\n    nodes,\n    trigger_words,\n    nosec_lines\n):\n    \"\"\"Find triggers from the trigger_word_list in the nodes.\n\n    Args:\n        nodes(list[Node]): the nodes to find triggers in.\n        trigger_word_list(list[Union[Sink, Source]]): list of trigger words to look for.\n        nosec_lines(set): lines with # nosec whitelisting\n\n    Returns:\n        List of found TriggerNodes\n    \"\"\"\n    trigger_nodes = list()\n    for node in nodes:\n        if node.line_number not in nosec_lines:\n            trigger_nodes.extend(iter(label_contains(node, trigger_words)))\n    return trigger_nodes\n\n\n\n\n# The code to be completed is:\ndef label_contains(\n    node,\n    triggers\n):\n\n    \"\"\"\n    This function checks if a given node contains any of the trigger words provided. It iterates through the list of trigger words and checks if each trigger word is present in the label of the node. If a trigger word is found, it yields a TriggerNode object.\n    Input-Output Arguments\n    :param node: Node. The CFG node to check.\n    :param triggers: List of Union[Sink, Source]. The list of trigger words to look for.\n    :return: Iterable of TriggerNodes. It can contain multiple TriggerNodes if multiple trigger words are found in the node's label.\n    \"\"\""}
{"namespace": "pyramid.security.PermitsResult.__repr__", "prompt": "# Please complete the __repr__ function based on the contexts above the function.\n\n# The contexts above the function are:\nfrom zope.deprecation import deprecated\nfrom zope.interface import implementer, providedBy\n\nfrom pyramid.interfaces import (\n    IAuthenticationPolicy,\n    IAuthorizationPolicy,\n    ISecuredView,\n    ISecurityPolicy,\n    IView,\n    IViewClassifier,\n)\nfrom pyramid.threadlocal import get_current_registry\n\nNO_PERMISSION_REQUIRED = '__no_permission_required__'\n\n\ndef _get_security_policy(request):\n    return request.registry.queryUtility(ISecurityPolicy)\n\n\ndef remember(request, userid, **kw):\n    \"\"\"\n    Returns a sequence of header tuples (e.g. ``[('Set-Cookie', 'foo=abc')]``)\n    on this request's response.\n    These headers are suitable for 'remembering' a set of credentials\n    implied by the data passed as ``userid`` and ``*kw`` using the\n    current :term:`security policy`.  Common usage might look\n    like so within the body of a view function (``response`` is\n    assumed to be a :term:`WebOb` -style :term:`response` object\n    computed previously by the view code):\n\n    .. code-block:: python\n\n       from pyramid.security import remember\n       headers = remember(request, 'chrism', password='123', max_age='86400')\n       response = request.response\n       response.headerlist.extend(headers)\n       return response\n\n    If no :term:`security policy` is in use, this function will\n    always return an empty sequence. If used, the composition and\n    meaning of ``**kw`` must be agreed upon by the calling code and\n    the effective security policy.\n\n    .. versionchanged:: 1.6\n        Deprecated the ``principal`` argument in favor of ``userid`` to clarify\n        its relationship to the security policy.\n\n    .. versionchanged:: 1.10\n        Removed the deprecated ``principal`` argument.\n    \"\"\"\n    policy = _get_security_policy(request)\n    if policy is None:\n        return []\n    return policy.remember(request, userid, **kw)\n\n\ndef forget(request, **kw):\n    \"\"\"\n    Return a sequence of header tuples (e.g. ``[('Set-Cookie',\n    'foo=abc')]``) suitable for 'forgetting' the set of credentials\n    possessed by the currently authenticated user.  A common usage\n    might look like so within the body of a view function\n    (``response`` is assumed to be an :term:`WebOb` -style\n    :term:`response` object computed previously by the view code):\n\n    .. code-block:: python\n\n       from pyramid.security import forget\n       headers = forget(request)\n       response.headerlist.extend(headers)\n       return response\n\n    If no :term:`security policy` is in use, this function will\n    always return an empty sequence.\n    \"\"\"\n    policy = _get_security_policy(request)\n    if policy is None:\n        return []\n    return policy.forget(request, **kw)\n\n\ndef principals_allowed_by_permission(context, permission):\n    \"\"\"\n    .. deprecated:: 2.0\n\n        The new security policy has removed the concept of principals.  See\n        :ref:`upgrading_auth_20` for more information.\n\n    Provided a ``context`` (a resource object), and a ``permission``\n    string, if an :term:`authorization policy` is\n    in effect, return a sequence of :term:`principal` ids that possess\n    the permission in the ``context``.  If no authorization policy is\n    in effect, this will return a sequence with the single value\n    :mod:`pyramid.authorization.Everyone` (the special principal\n    identifier representing all principals).\n\n    .. note::\n\n       Even if an :term:`authorization policy` is in effect,\n       some (exotic) authorization policies may not implement the\n       required machinery for this function; those will cause a\n       :exc:`NotImplementedError` exception to be raised when this\n       function is invoked.\n\n    \"\"\"\n    reg = get_current_registry()\n    policy = reg.queryUtility(IAuthorizationPolicy)\n    if policy is None:\n        from pyramid.authorization import Everyone  # noqa: F811\n\n        return [Everyone]\n    return policy.principals_allowed_by_permission(context, permission)\n\n\ndeprecated(\n    'principals_allowed_by_permission',\n    'The new security policy has removed the concept of principals.  See '\n    '\"Upgrading Authentication/Authorization\" in \"What\\'s New in Pyramid 2.0\" '\n    'of the documentation for more information.',\n)\n\n\ndef view_execution_permitted(context, request, name=''):\n    \"\"\"If the view specified by ``context`` and ``name`` is protected\n    by a :term:`permission`, check the permission associated with the\n    view using the effective authentication/authorization policies and\n    the ``request``.  Return a boolean result.  If no\n    :term:`security policy` is in effect, or if the view is not\n    protected by a permission, return ``True``. If no view can view found,\n    an exception will be raised.\n\n    .. versionchanged:: 1.4a4\n       An exception is raised if no view is found.\n\n    \"\"\"\n    reg = request.registry\n    provides = [IViewClassifier] + [providedBy(x) for x in (request, context)]\n    # XXX not sure what to do here about using _find_views or analogue;\n    # for now let's just keep it as-is\n    view = reg.adapters.lookup(provides, ISecuredView, name=name)\n    if view is None:\n        view = reg.adapters.lookup(provides, IView, name=name)\n        if view is None:\n            raise TypeError(\n                'No registered view satisfies the constraints. '\n                'It would not make sense to claim that this view '\n                '\"is\" or \"is not\" permitted.'\n            )\n        return Allowed(\n            'Allowed: view name %r in context %r (no permission defined)'\n            % (name, context)\n        )\n    return view.__permitted__(context, request)\n\n\nclass PermitsResult(int):\n    def __new__(cls, s, *args):\n        \"\"\"\n        Create a new instance.\n\n        :param fmt: A format string explaining the reason for denial.\n        :param args: Arguments are stored and used with the format string\n                      to generate the ``msg``.\n\n        \"\"\"\n        inst = int.__new__(cls, cls.boolval)\n        inst.s = s\n        inst.args = args\n        return inst\n\n    @property\n    def msg(self):\n        \"\"\"A string indicating why the result was generated.\"\"\"\n        return self.s % self.args\n\n    def __str__(self):\n        return self.msg\n\n\n\n# The code to be completed is:\n    def __repr__(self):\n\n        \"\"\"\n        This function overrides the default \"__repr__\" method for the PermitsResult class. It returns a string representation of the instance, including the class name, instance id, and the message. The output format is \"<{class name} instance at {instance id} with msg {message}>\"\n        Input-Output Arguments\n        :param self: PermitsResult. An instance of the PermitsResult class.\n        :return: String. A string representation of the instance.\n        \"\"\""}
{"namespace": "pycoin.satoshi.stackops.do_OP_TUCK", "prompt": "# Please complete the do_OP_TUCK function based on the contexts above the function.\n\n# The contexts above the function are:\nimport hashlib\n\nfrom . import errno\nfrom pycoin.coins.SolutionChecker import ScriptError\n\n\n\n\ndef do_OP_NOP(s):\n    pass\n\n\nfor i in range(1, 11):\n    exec(\"def do_OP_NOP%d(s): pass\" % i)\n\n\ndef do_OP_VER(stack):\n    raise ScriptError(\"OP_VER encountered\", errno.BAD_OPCODE)\n\n\ndef do_OP_RESERVED1(stack):\n    raise ScriptError(\"OP_RESERVED1 encountered\", errno.BAD_OPCODE)\n\n\ndef do_OP_RESERVED2(stack):\n    raise ScriptError(\"OP_RESERVED2 encountered\", errno.BAD_OPCODE)\n\n\ndef do_OP_RETURN(stack):\n    raise ScriptError(\"OP_RETURN encountered\", errno.OP_RETURN)\n\n\ndef do_OP_2DROP(stack):\n    stack.pop()\n    stack.pop()\n\n\ndef do_OP_2DUP(stack):\n    #  (x1 x2 -- x1 x2 x1 x2)\n    stack.append(stack[-2])\n    stack.append(stack[-2])\n\n\ndef do_OP_3DUP(stack):\n    #  (x1 x2 x3 -- x1 x2 x3 x1 x2 x3)\n    stack.append(stack[-3])\n    stack.append(stack[-3])\n    stack.append(stack[-3])\n\n\ndef do_OP_2OVER(stack):\n    #  (x1 x2 x3 x4 -- x1 x2 x3 x4 x1 x2)\n    stack.append(stack[-4])\n    stack.append(stack[-4])\n\n\ndef do_OP_2ROT(stack):\n    # (1, 2, 3, 4, 5, 6 -- 3, 4, 5, 6, 1, 2)\n    stack.append(stack.pop(-6))\n    stack.append(stack.pop(-6))\n\n\ndef do_OP_2SWAP(stack):\n    stack.append(stack.pop(-4))\n    stack.append(stack.pop(-4))\n\n\ndef do_OP_IFDUP(stack):\n    if stack[-1]:\n        stack.append(stack[-1])\n\n\ndef do_OP_DROP(stack):\n    stack.pop()\n\n\ndef do_OP_DUP(stack):\n    stack.append(stack[-1])\n\n\ndef do_OP_NIP(stack):\n    v = stack.pop()\n    stack.pop()\n    stack.append(v)\n\n\ndef do_OP_OVER(stack):\n    stack.append(stack[-2])\n\n\ndef do_OP_ROT(stack):\n    stack.append(stack.pop(-3))\n\n\ndef do_OP_SWAP(stack):\n    stack.append(stack.pop(-2))\n\n\n\n\n# The code to be completed is:\ndef do_OP_TUCK(stack):\n\n    \"\"\"\n    This function performs the TUCK operation on the input stack. It pops the top two elements from the stack, and then pushes them back in the reverse order, followed by the original top element.\n    Input-Output Arguments\n    :param stack: List. The input stack on which the TUCK operation is to be performed.\n    :return: No return values.\n    \"\"\""}
{"namespace": "aioxmpp.xso.query.EvaluationContext.eval_bool", "prompt": "# Please complete the eval_bool function based on the contexts above the function.\n\n# The contexts above the function are:\n########################################################################\n# File name: query.py\n# This file is part of: aioxmpp\n#\n# LICENSE\n#\n# This program is free software: you can redistribute it and/or modify\n# it under the terms of the GNU Lesser General Public License as\n# published by the Free Software Foundation, either version 3 of the\n# License, or (at your option) any later version.\n#\n# This program is distributed in the hope that it will be useful, but\n# WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\n# Lesser General Public License for more details.\n#\n# You should have received a copy of the GNU Lesser General Public\n# License along with this program.  If not, see\n# <http://www.gnu.org/licenses/>.\n#\n########################################################################\nimport abc\nimport copy\nimport itertools\nimport inspect\nimport operator\n\n\nclass _SoftExprMixin:\n    \"\"\"\n    This mixin is used for metaclasses and descriptors.\n\n    It defines the operators ``/`` and ``[]``, which are rarely used for either\n    classes or descriptors.\n\n    .. seealso::\n\n       :class:`_ExprMixin`\n          which inherits from this class and defines more operators, some of\n          which would be unsafe to implement on classes or descriptors, such as\n          ``==``.\n\n    \"\"\"\n\n    def __truediv__(self, other):\n        if isinstance(other, PreExpr):\n            return as_expr(other, lhs=self)\n        elif isinstance(other, Expr):\n            return as_expr(other, lhs=self)\n        return NotImplemented\n\n    def __getitem__(self, index):\n        if isinstance(index, where):\n            return ExprFilter(self, as_expr(index.expr))\n        return Nth(self, as_expr(index))\n\n\nclass _ExprMixin(_SoftExprMixin):\n    \"\"\"\n    This mixin defines operators which are only \"safe\" to overload in\n    constrained situations. These operators often have meanings and may be\n    implicitly used by the python language; thus, they are only defined on\n    :class:`Expr` subclasses and some :class:`PreExpr` subclasses.\n\n    The defined operators currently are:\n\n    * Comparison: ``==``, ``<``, ``<=``, ``>=``, ``>``, ``!=``\n    \"\"\"\n\n    def __eq__(self, other):\n        return CmpOp(\n            as_expr(self),\n            as_expr(other),\n            operator.eq,\n        )\n\n    def __ne__(self, other):\n        return CmpOp(\n            as_expr(self),\n            as_expr(other),\n            operator.ne,\n        )\n\n    def __lt__(self, other):\n        return CmpOp(\n            as_expr(self),\n            as_expr(other),\n            operator.lt,\n        )\n\n    def __gt__(self, other):\n        return CmpOp(\n            as_expr(self),\n            as_expr(other),\n            operator.gt,\n        )\n\n    def __ge__(self, other):\n        return CmpOp(\n            as_expr(self),\n            as_expr(other),\n            operator.ge,\n        )\n\n    def __le__(self, other):\n        return CmpOp(\n            as_expr(self),\n            as_expr(other),\n            operator.le,\n        )\n\n\nclass EvaluationContext:\n    \"\"\"\n    The evaluation context holds contextual information for the evaluation of a\n    query expression.\n\n    Most notably, it provides the methods for acquiring and replacing the\n    toplevel objects of classes:\n\n    .. automethod:: get_toplevel_object()\n\n    .. automethod:: set_toplevel_object()\n\n    In addition, it provides shortcuts for evaluating expressions:\n\n    .. automethod:: eval\n\n    .. automethod:: eval_bool\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n        super().__init__()\n        self._toplevels = {}\n\n    def __copy__(self):\n        result = type(self).__new__(type(self))\n        result._toplevels = dict(self._toplevels)\n        return result\n\n    def get_toplevel_object(self, class_):\n        \"\"\"\n        Return the toplevel object for the given `class_`. Only exact matches\n        are returned.\n        \"\"\"\n        return self._toplevels[class_]\n\n    def set_toplevel_object(self, instance, class_=None):\n        \"\"\"\n        Set the toplevel object to return from :meth:`get_toplevel_object` when\n        asked for `class_` to `instance`.\n\n        If `class_` is :data:`None`, the :func:`type` of the `instance` is\n        used.\n        \"\"\"\n        if class_ is None:\n            class_ = type(instance)\n        self._toplevels[class_] = instance\n\n    def eval(self, expr):\n        \"\"\"\n        Evaluate the expression `expr` and return the result.\n\n        The result of an expression is always an iterable.\n        \"\"\"\n        return expr.eval(self)\n\n\n\n# The code to be completed is:\n    def eval_bool(self, expr):\n\n        \"\"\"\n        Evaluate the expression `expr` and return the truthness of its result. A result of an expression is said to be true if it contains at least one value. It has the same semantics as :func:`bool` on sequences.\n        Input-Output Arguments\n        :param self: EvaluationContext. An instance of the EvaluationContext class.\n        :param expr: The expression to be evaluated.\n        :return: Boolean. The truthness of the evaluated expression.\n        \"\"\""}
{"namespace": "bentoml._internal.runner.strategy.DefaultStrategy.get_worker_count", "prompt": "# Please complete the get_worker_count function based on the contexts above the function.\n\n# The contexts above the function are:\nfrom __future__ import annotations\n\nimport abc\nimport logging\nimport math\nimport typing as t\n\nfrom ..resource import get_resource\nfrom ..resource import system_resources\nfrom .runnable import Runnable\n\nlogger = logging.getLogger(__name__)\n\n\nclass Strategy(abc.ABC):\n    @classmethod\n    @abc.abstractmethod\n    def get_worker_count(\n        cls,\n        runnable_class: t.Type[Runnable],\n        resource_request: dict[str, t.Any] | None,\n        workers_per_resource: int | float,\n    ) -> int:\n        ...\n\n    @classmethod\n    @abc.abstractmethod\n    def get_worker_env(\n        cls,\n        runnable_class: t.Type[Runnable],\n        resource_request: dict[str, t.Any] | None,\n        workers_per_resource: int | float,\n        worker_index: int,\n    ) -> dict[str, t.Any]:\n        \"\"\"\n        Args:\n            runnable_class : The runnable class to be run.\n            resource_request : The resource request of the runnable.\n            worker_index : The index of the worker, start from 0.\n        \"\"\"\n        ...\n\n\nTHREAD_ENVS = [\n    \"BENTOML_NUM_THREAD\",  # For custom Runner code\n    \"OMP_NUM_THREADS\",  # openmp\n    \"OPENBLAS_NUM_THREADS\",  # openblas,\n    \"MKL_NUM_THREADS\",  # mkl,\n    \"VECLIB_MAXIMUM_THREADS\",  # accelerate,\n    \"NUMEXPR_NUM_THREADS\",  # numexpr\n    # For huggingface fast tokenizer\n    \"RAYON_RS_NUM_CPUS\",\n    # For Tensorflow\n    \"TF_NUM_INTEROP_THREADS\",\n    \"TF_NUM_INTRAOP_THREADS\",\n]  # TODO(jiang): make it configurable?\n\n\nclass DefaultStrategy(Strategy):\n    @classmethod\n\n\n# The code to be completed is:\n    def get_worker_count(\n        cls,\n        runnable_class: t.Type[Runnable],\n        resource_request: dict[str, t.Any] | None,\n        workers_per_resource: int | float,\n    ) -> int:\n\n        \"\"\"\n        This function calculates the number of workers needed based on the given parameters. It first checks if there is a resource request for Nvidia GPUs and if the runnable class supports Nvidia GPUs. If so, it calculates the number of workers based on the number of available Nvidia GPUs and the workers per resource value. If not, it checks if there are CPUs available and if the runnable class supports CPUs. If so, it calculates the number of workers based on the number of available CPUs and the workers per resource value. If none of the conditions are met, it raises a ValueError indicating that there are no known supported resources available for the runnable class.\n        Input-Output Arguments\n        :param cls: DefaultStrategy. The class itself.\n        :param runnable_class: Type[Runnable]. The class of the runnable object.\n        :param resource_request: Union[Dict[str, Any], None]. The resource request for the runnable object. Defaults to None.\n        :param workers_per_resource: Union[int, float]. The number of workers per resource.\n        :return: int. The number of workers needed based on the given parameters.\n        \"\"\""}
{"namespace": "twilio.base.deserialize.decimal", "prompt": "# Please complete the decimal function based on the contexts above the function.\n\n# The contexts above the function are:\nimport datetime\nfrom decimal import BasicContext, Decimal\nfrom email.utils import parsedate\nfrom typing import Optional, Union\n\nISO8601_DATE_FORMAT = \"%Y-%m-%d\"\nISO8601_DATETIME_FORMAT = \"%Y-%m-%dT%H:%M:%SZ\"\n\n\ndef iso8601_date(s: str) -> Union[datetime.date, str]:\n    \"\"\"\n    Parses an ISO 8601 date string and returns a UTC date object or the string\n    if the parsing failed.\n    :param s: ISO 8601-formatted date string (2015-01-25)\n    :return:\n    \"\"\"\n    try:\n        return (\n            datetime.datetime.strptime(s, ISO8601_DATE_FORMAT)\n            .replace(tzinfo=datetime.timezone.utc)\n            .date()\n        )\n    except (TypeError, ValueError):\n        return s\n\n\ndef iso8601_datetime(\n    s: str,\n) -> Union[datetime.datetime, str]:\n    \"\"\"\n    Parses an ISO 8601 datetime string and returns a UTC datetime object,\n    or the string if parsing failed.\n    :param s: ISO 8601-formatted datetime string (2015-01-25T12:34:56Z)\n    \"\"\"\n    try:\n        return datetime.datetime.strptime(s, ISO8601_DATETIME_FORMAT).replace(\n            tzinfo=datetime.timezone.utc\n        )\n    except (TypeError, ValueError):\n        return s\n\n\ndef rfc2822_datetime(s: str) -> Optional[datetime.datetime]:\n    \"\"\"\n    Parses an RFC 2822 date string and returns a UTC datetime object,\n    or the string if parsing failed.\n    :param s: RFC 2822-formatted string date\n    :return: datetime or str\n    \"\"\"\n    date_tuple = parsedate(s)\n    if date_tuple is None:\n        return None\n    return datetime.datetime(*date_tuple[:6]).replace(tzinfo=datetime.timezone.utc)\n\n\n\n\n# The code to be completed is:\ndef decimal(d: Optional[str]) -> Union[Decimal, str]:\n\n    \"\"\"\n    This function parses a decimal string into a Decimal object using the BasicContext.\n    Input-Output Arguments\n    :param d: Optional string. The decimal string to be parsed.\n    :return: Union[Decimal, str]. The parsed Decimal object or the original string if it is empty.\n    \"\"\""}
{"namespace": "gunicorn.http.body.EOFReader.read", "prompt": "# Please complete the read function based on the contexts above the function.\n\n# The contexts above the function are:\n# -*- coding: utf-8 -\n#\n# This file is part of gunicorn released under the MIT license.\n# See the NOTICE for more information.\n\nimport io\nimport sys\n\nfrom gunicorn.http.errors import (NoMoreData, ChunkMissingTerminator,\n                                  InvalidChunkSize)\n\n\nclass ChunkedReader(object):\n    def __init__(self, req, unreader):\n        self.req = req\n        self.parser = self.parse_chunked(unreader)\n        self.buf = io.BytesIO()\n\n    def read(self, size):\n        if not isinstance(size, int):\n            raise TypeError(\"size must be an integer type\")\n        if size < 0:\n            raise ValueError(\"Size must be positive.\")\n        if size == 0:\n            return b\"\"\n\n        if self.parser:\n            while self.buf.tell() < size:\n                try:\n                    self.buf.write(next(self.parser))\n                except StopIteration:\n                    self.parser = None\n                    break\n\n        data = self.buf.getvalue()\n        ret, rest = data[:size], data[size:]\n        self.buf = io.BytesIO()\n        self.buf.write(rest)\n        return ret\n\n    def parse_trailers(self, unreader, data):\n        buf = io.BytesIO()\n        buf.write(data)\n\n        idx = buf.getvalue().find(b\"\\r\\n\\r\\n\")\n        done = buf.getvalue()[:2] == b\"\\r\\n\"\n        while idx < 0 and not done:\n            self.get_data(unreader, buf)\n            idx = buf.getvalue().find(b\"\\r\\n\\r\\n\")\n            done = buf.getvalue()[:2] == b\"\\r\\n\"\n        if done:\n            unreader.unread(buf.getvalue()[2:])\n            return b\"\"\n        self.req.trailers = self.req.parse_headers(buf.getvalue()[:idx])\n        unreader.unread(buf.getvalue()[idx + 4:])\n\n    def parse_chunked(self, unreader):\n        (size, rest) = self.parse_chunk_size(unreader)\n        while size > 0:\n            while size > len(rest):\n                size -= len(rest)\n                yield rest\n                rest = unreader.read()\n                if not rest:\n                    raise NoMoreData()\n            yield rest[:size]\n            # Remove \\r\\n after chunk\n            rest = rest[size:]\n            while len(rest) < 2:\n                rest += unreader.read()\n            if rest[:2] != b'\\r\\n':\n                raise ChunkMissingTerminator(rest[:2])\n            (size, rest) = self.parse_chunk_size(unreader, data=rest[2:])\n\n    def parse_chunk_size(self, unreader, data=None):\n        buf = io.BytesIO()\n        if data is not None:\n            buf.write(data)\n\n        idx = buf.getvalue().find(b\"\\r\\n\")\n        while idx < 0:\n            self.get_data(unreader, buf)\n            idx = buf.getvalue().find(b\"\\r\\n\")\n\n        data = buf.getvalue()\n        line, rest_chunk = data[:idx], data[idx + 2:]\n\n        chunk_size = line.split(b\";\", 1)[0].strip()\n        try:\n            chunk_size = int(chunk_size, 16)\n        except ValueError:\n            raise InvalidChunkSize(chunk_size)\n\n        if chunk_size == 0:\n            try:\n                self.parse_trailers(unreader, rest_chunk)\n            except NoMoreData:\n                pass\n            return (0, None)\n        return (chunk_size, rest_chunk)\n\n    def get_data(self, unreader, buf):\n        data = unreader.read()\n        if not data:\n            raise NoMoreData()\n        buf.write(data)\n\n\nclass LengthReader(object):\n    def __init__(self, unreader, length):\n        self.unreader = unreader\n        self.length = length\n\n    def read(self, size):\n        if not isinstance(size, int):\n            raise TypeError(\"size must be an integral type\")\n\n        size = min(self.length, size)\n        if size < 0:\n            raise ValueError(\"Size must be positive.\")\n        if size == 0:\n            return b\"\"\n\n        buf = io.BytesIO()\n        data = self.unreader.read()\n        while data:\n            buf.write(data)\n            if buf.tell() >= size:\n                break\n            data = self.unreader.read()\n\n        buf = buf.getvalue()\n        ret, rest = buf[:size], buf[size:]\n        self.unreader.unread(rest)\n        self.length -= size\n        return ret\n\n\nclass EOFReader(object):\n    def __init__(self, unreader):\n        self.unreader = unreader\n        self.buf = io.BytesIO()\n        self.finished = False\n\n\n\n# The code to be completed is:\n    def read(self, size):\n\n        \"\"\"\n        Read a specified number of bytes from the input stream. It reads the data from the input stream and returns the requested number of bytes. If the end of the stream is reached, it returns an empty byte string.\n        The function first checks if the size parameter is an integer. If it's not, it raises a TypeError \"size must be an integral type\". Then it checks if the size is negative. If it is, it raises a ValueError \"Size must be positive.\". If the size is 0, the function returns an empty bytes string b\"\".\n        If all the data has been read from the stream, the function retrieves the data from the buffer, splits it into two parts - one with the requested size and the other with the rest of the data. It then resets the buffer, writes the remaining data to it, and returns the first part.\n        Else, it reads data from the unreader and writes it to the buffer until the buffer's size exceeds the requested size. Then it stops reading and checks if there's more data to read. If there isn't, it sets a finish flag to True.\n        Finally, the function retrieves the data from the buffer, splits it into two parts - one with the requested size and the other with the rest of the data. It then resets the buffer, writes the remaining data to it, and returns the first part.\n        Input-Output Arguments\n        :param self: EOFReader. An instance of the EOFReader class.\n        :param size: int. The number of bytes to read from the input stream.\n        :return: bytes. The data read from the input stream.\n        \"\"\""}
{"namespace": "chatette.parsing.UnitDefBuilder._build_modifiers_repr", "prompt": "# Please complete the _build_modifiers_repr function based on the contexts above the function.\n\n# The contexts above the function are:\n# coding: utf-8\n\"\"\"\nModule `chatette.parsing`\nContains everything that is related to the management and parsing\nof the template file(s).\nThe most important classes defined in this module are:\n- Parser, which runs the whole parsing of template files.\n- Lexer, in charge of \"lexing\" the information present in those files.\n- All the lexing rules the lexer will use.\n- InputFileManager, which manages the opening, closing and read of those files.\n- ItemBuilders that are used by the parser to create concrete items.\n\"\"\"\n# TODO Add LineCountFileWrapper in here\n\nfrom abc import ABCMeta, abstractmethod\nfrom future.utils import with_metaclass\n\n\n\n\n\n\n\nfrom chatette.modifiers.representation import \\\n    ModifiersRepresentation, RandgenRepresentation\n\nfrom chatette.units.ast import AST\nfrom chatette.utils import UnitType\n\n\nclass ItemBuilder(with_metaclass(ABCMeta, object)):\n    \"\"\"\n    An intermediate representation of generating items that are used by the\n    parser. It is able to construct the corresponding item once it has\n    all the required information.\n    NOTE: This does not correspond to the *Builder* design pattern.\n    \"\"\"\n    def __init__(self):\n        self.leading_space = False\n        self.casegen = False\n        self.randgen = False\n        self.randgen_name = None\n        self.randgen_opposite = False\n        self.randgen_percent = 50\n\n    def _check_information(self):\n        if not self.randgen and self.randgen_name is not None:  # Should never happen\n            raise ValueError(\n                \"There was a problem with some modifiers: detected \" + \\\n                \"a random generation modifier name but no \" + \\\n                \"random generation modifier.\"\n            )\n\n    def _build_modifiers_repr(self):\n        \"\"\"\n        Returns an instance of `ModifiersRepresentation` that corresponds\n        to the modifiers set in `self`.\n        \"\"\"\n        modifiers = ModifiersRepresentation()\n        modifiers.casegen = self.casegen\n\n        randgen = RandgenRepresentation()\n        randgen._present = self.randgen\n        randgen.name = self.randgen_name\n        randgen.opposite = self.randgen_opposite\n        randgen.percentage = self.randgen_percent\n        modifiers.randgen = randgen\n\n        return modifiers\n\n    @abstractmethod\n    def create_concrete(self):\n        raise NotImplementedError()\n\nclass ChoiceBuilder(ItemBuilder):\n    def __init__(self):\n        super(ChoiceBuilder, self).__init__()\n        self.rules = []\n\n    def create_concrete(self):\n        from chatette.units.modifiable.choice import Choice\n        self._check_information()\n        return Choice(\n            self.leading_space, self._build_modifiers_repr(),\n            self.rules\n        )\n\nclass UnitRefBuilder(ItemBuilder):\n    def __init__(self):\n        super(UnitRefBuilder, self).__init__()\n        self.type = None\n        self.identifier = None\n        self.variation = None\n        self.arg_value = None\n\n    def _check_information(self):\n        super(UnitRefBuilder, self)._check_information()\n        if self.type is None or self.identifier is None:  # Should never happen\n            raise ValueError(\n                \"Tried to create a concrete unit reference without setting \" + \\\n                \"its identifier or type.\"\n            )\n\n    def _build_modifiers_repr(self):\n        modifiers = super(UnitRefBuilder, self)._build_modifiers_repr()\n        modifiers.argument_value = self.arg_value\n        modifiers.variation_name = self.variation\n        return modifiers\n\n    def create_concrete(self):\n        from chatette.units.modifiable.unit_reference import UnitReference\n        self._check_information()\n        return UnitReference(\n            self.identifier, self.type,\n            self.leading_space, self._build_modifiers_repr()\n        )\n\nclass UnitDefBuilder(ItemBuilder):\n    def __init__(self):\n        super(UnitDefBuilder, self).__init__()\n        self.identifier = None\n        self.variation = None\n        self.arg_name = None\n\n\n\n# The code to be completed is:\n    def _build_modifiers_repr(self):\n\n        \"\"\"\n        This function builds the representation of modifiers for a UnitDefBuilder instance. It first gets the modifiers, then sets the argument name of the modifiers to the arg name of the UnitDefBuilder instance. Finally, it returns the modifiers.\n        Input-Output Arguments\n        :param self: UnitDefBuilder. An instance of the UnitDefBuilder class.\n        :return: The representation of modifiers for the UnitDefBuilder instance.\n        \"\"\""}
{"namespace": "mopidy.config.types.Secret.serialize", "prompt": "# Please complete the serialize function based on the contexts above the function.\n\n# The contexts above the function are:\nimport logging\nimport re\nimport socket\n\nfrom mopidy.config import validators\nfrom mopidy.internal import log, path\n\n\ndef decode(value):\n    if isinstance(value, bytes):\n        value = value.decode(errors=\"surrogateescape\")\n\n    for char in (\"\\\\\", \"\\n\", \"\\t\"):\n        value = value.replace(\n            char.encode(encoding=\"unicode-escape\").decode(), char\n        )\n\n    return value\n\n\ndef encode(value):\n    if isinstance(value, bytes):\n        value = value.decode(errors=\"surrogateescape\")\n\n    for char in (\"\\\\\", \"\\n\", \"\\t\"):\n        value = value.replace(\n            char, char.encode(encoding=\"unicode-escape\").decode()\n        )\n\n    return value\n\n\nclass DeprecatedValue:\n    pass\n\n\nclass _TransformedValue(str):\n    def __new__(cls, original, transformed):\n        return super().__new__(cls, transformed)\n\n    def __init__(self, original, transformed):\n        self.original = original\n\n\nclass ConfigValue:\n    \"\"\"Represents a config key's value and how to handle it.\n\n    Normally you will only be interacting with sub-classes for config values\n    that encode either deserialization behavior and/or validation.\n\n    Each config value should be used for the following actions:\n\n    1. Deserializing from a raw string and validating, raising ValueError on\n       failure.\n    2. Serializing a value back to a string that can be stored in a config.\n    3. Formatting a value to a printable form (useful for masking secrets).\n\n    :class:`None` values should not be deserialized, serialized or formatted,\n    the code interacting with the config should simply skip None config values.\n    \"\"\"\n\n    def deserialize(self, value):\n        \"\"\"Cast raw string to appropriate type.\"\"\"\n        return decode(value)\n\n    def serialize(self, value, display=False):\n        \"\"\"Convert value back to string for saving.\"\"\"\n        if value is None:\n            return \"\"\n        return str(value)\n\n\nclass Deprecated(ConfigValue):\n    \"\"\"Deprecated value.\n\n    Used for ignoring old config values that are no longer in use, but should\n    not cause the config parser to crash.\n    \"\"\"\n\n    def deserialize(self, value):\n        return DeprecatedValue()\n\n    def serialize(self, value, display=False):\n        return DeprecatedValue()\n\n\nclass String(ConfigValue):\n    \"\"\"String value.\n\n    Is decoded as utf-8 and \\\\n \\\\t escapes should work and be preserved.\n    \"\"\"\n\n    def __init__(self, optional=False, choices=None, transformer=None):\n        self._required = not optional\n        self._choices = choices\n        self._transformer = transformer\n\n    def deserialize(self, value):\n        value = decode(value).strip()\n        validators.validate_required(value, self._required)\n        if not value:\n            return None\n\n        # This is necessary for backwards-compatibility, in case subclasses\n        # aren't calling their parent constructor.\n        transformer = getattr(self, \"_transformer\", None)\n        if transformer:\n            transformed_value = transformer(value)\n            value = _TransformedValue(value, transformed_value)\n\n        validators.validate_choice(value, self._choices)\n        return value\n\n    def serialize(self, value, display=False):\n        if value is None:\n            return \"\"\n        if isinstance(value, _TransformedValue):\n            value = value.original\n        return encode(value)\n\n\nclass Secret(String):\n    \"\"\"Secret string value.\n\n    Is decoded as utf-8 and \\\\n \\\\t escapes should work and be preserved.\n\n    Should be used for passwords, auth tokens etc. Will mask value when being\n    displayed.\n    \"\"\"\n\n    def __init__(self, optional=False, choices=None, transformer=None):\n        super().__init__(\n            optional=optional,\n            choices=None,  # Choices doesn't make sense for secrets\n            transformer=transformer,\n        )\n\n\n\n# The code to be completed is:\n    def serialize(self, value, display=False):\n\n        \"\"\"\n        Serialize a value based on the given condition. If the value is not None and the display flag is set to True, it returns \"********\". Otherwise, it makes the superclass to serialize that and returns the result.\n        Input-Output Arguments\n        :param self: Secret. An instance of the Secret class.\n        :param value: The value to be serialized.\n        :param display: Bool. Whether to display the serialized value. Defaults to False.\n        :return: The serialized value.\n        \"\"\""}
{"namespace": "aioxmpp.testutils.make_listener", "prompt": "# Please complete the make_listener function based on the contexts above the function.\n\n# The contexts above the function are:\n########################################################################\n# File name: testutils.py\n# This file is part of: aioxmpp\n#\n# LICENSE\n#\n# This program is free software: you can redistribute it and/or modify\n# it under the terms of the GNU Lesser General Public License as\n# published by the Free Software Foundation, either version 3 of the\n# License, or (at your option) any later version.\n#\n# This program is distributed in the hope that it will be useful, but\n# WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\n# Lesser General Public License for more details.\n#\n# You should have received a copy of the GNU Lesser General Public\n# License along with this program.  If not, see\n# <http://www.gnu.org/licenses/>.\n#\n########################################################################\n\"\"\"\nThis module contains utilities used for testing aioxmpp code. These\nutilities themselves are tested, which is meta, but cool.\n\"\"\"\nimport asyncio\nimport collections\nimport contextlib\nimport functools\nimport logging\nimport os\nimport time\nimport unittest\nimport unittest.mock\n\nfrom datetime import timedelta\n\nimport aioxmpp.callbacks as callbacks\nimport aioxmpp.xso as xso\nimport aioxmpp.nonza as nonza\n\nfrom aioxmpp.utils import etree\n\n\nlogger = logging.getLogger(__name__)\n\n\nGLOBAL_TIMEOUT_FACTOR = 1.0\n\n_monotonic_info = time.get_clock_info(\"monotonic\")\n# this is a fun hack to make things work on windows, where the monotonic\n# resolution isnt that great\nGLOBAL_TIMEOUT_FACTOR *= max(_monotonic_info.resolution, 0.0015) / 0.0015\n\n# and now we slap on some extra for travis CI\nif os.environ.get(\"CI\") == \"true\":\n    GLOBAL_TIMEOUT_FACTOR *= 4\n    logger.debug(\"increasing GLOBAL_TIMEOUT_FACTOR for CI\")\n\n\nlogger.debug(\"using GLOBAL_TIMEOUT_FACTOR = %.3f\", GLOBAL_TIMEOUT_FACTOR)\n\n\ndef get_timeout(base):\n    return base * GLOBAL_TIMEOUT_FACTOR\n\n\nDEFAULT_TIMEOUT = get_timeout(1.0)\n\n\ndef make_protocol_mock():\n    return unittest.mock.Mock([\n        \"connection_made\",\n        \"eof_received\",\n        \"connection_lost\",\n        \"data_received\",\n        \"pause_writing\",\n        \"resume_writing\",\n    ])\n\n\ndef run_coroutine(coroutine, timeout=DEFAULT_TIMEOUT, loop=None):\n    if not loop:\n        loop = asyncio.get_event_loop()\n    return loop.run_until_complete(\n        asyncio.wait_for(\n            coroutine,\n            timeout=timeout))\n\n\ndef run_coroutine_with_peer(\n        coroutine,\n        peer_coroutine,\n        timeout=1.0,\n        loop=None):\n    loop = loop or asyncio.get_event_loop()\n\n    local_future = asyncio.ensure_future(coroutine, loop=loop)\n    remote_future = asyncio.ensure_future(peer_coroutine, loop=loop)\n\n    done, pending = loop.run_until_complete(\n        asyncio.wait(\n            [\n                local_future,\n                remote_future,\n            ],\n            timeout=timeout,\n            return_when=asyncio.FIRST_EXCEPTION)\n    )\n    if not done:\n        raise asyncio.TimeoutError(\"Test timed out\")\n\n    if pending:\n        pending_fut = next(iter(pending))\n        pending_fut.cancel()\n        fut = next(iter(done))\n        try:\n            fut.result()\n        except:  # NOQA: E722\n            # everything is fine, the other one failed\n            raise\n        else:\n            if pending_fut == remote_future:\n                raise asyncio.TimeoutError(\n                    \"Peer coroutine did not return in time\")\n            else:\n                raise asyncio.TimeoutError(\n                    \"Coroutine under test did not return in time\")\n\n    if local_future.exception():\n        # re-throw the error properly\n        local_future.result()\n\n    remote_future.result()\n    return local_future.result()\n\n\n\n\n# The code to be completed is:\ndef make_listener(instance):\n\n    \"\"\"\n    This function returns a unittest.mock.Mock object which has children connected to each aioxmpp.callbacks.Signal of the given instance. The children are named exactly like the signals.\n    Input-Output Arguments\n    :param instance: The instance for which the listener is to be created.\n    :return: unittest.mock.Mock. The created mock object with children connected to each signal of the instance.\n    \"\"\""}
{"namespace": "gunicorn.http.unreader.Unreader.unread", "prompt": "# Please complete the unread function based on the contexts above the function.\n\n# The contexts above the function are:\n# -*- coding: utf-8 -\n#\n# This file is part of gunicorn released under the MIT license.\n# See the NOTICE for more information.\n\nimport io\nimport os\n\n# Classes that can undo reading data from\n# a given type of data source.\n\n\nclass Unreader(object):\n    def __init__(self):\n        self.buf = io.BytesIO()\n\n    def chunk(self):\n        raise NotImplementedError()\n\n    def read(self, size=None):\n        if size is not None and not isinstance(size, int):\n            raise TypeError(\"size parameter must be an int or long.\")\n\n        if size is not None:\n            if size == 0:\n                return b\"\"\n            if size < 0:\n                size = None\n\n        self.buf.seek(0, os.SEEK_END)\n\n        if size is None and self.buf.tell():\n            ret = self.buf.getvalue()\n            self.buf = io.BytesIO()\n            return ret\n        if size is None:\n            d = self.chunk()\n            return d\n\n        while self.buf.tell() < size:\n            chunk = self.chunk()\n            if not chunk:\n                ret = self.buf.getvalue()\n                self.buf = io.BytesIO()\n                return ret\n            self.buf.write(chunk)\n        data = self.buf.getvalue()\n        self.buf = io.BytesIO()\n        self.buf.write(data[size:])\n        return data[:size]\n\n\n\n# The code to be completed is:\n    def unread(self, data):\n\n        \"\"\"\n        This function appends the given data to the end of the buffer in the Unreader instance.\n        Input-Output Arguments\n        :param self: Unreader. An instance of the Unreader class.\n        :param data: The data to be appended to the buffer.\n        :return: No return values.\n        \"\"\""}
{"namespace": "pyinfra.api.facts.get_facts", "prompt": "# Please complete the get_facts function based on the contexts above the function.\n\n# The contexts above the function are:\n\"\"\"\nThe pyinfra facts API. Facts enable pyinfra to collect remote server state which\nis used to \"diff\" with the desired state, producing the final commands required\nfor a deploy.\n\nNote that the facts API does *not* use the global currently in context host so\nit's possible to call facts on hosts out of context (ie give me the IP of this\nother host B while I operate on this host A).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport re\nfrom inspect import getcallargs\nfrom socket import error as socket_error, timeout as timeout_error\nfrom typing import TYPE_CHECKING, Any, Callable, Iterable, Optional, Union\n\nimport click\nimport gevent\nfrom paramiko import SSHException\n\nfrom pyinfra import logger\nfrom pyinfra.api import StringCommand\nfrom pyinfra.api.arguments import pop_global_arguments\nfrom pyinfra.api.util import (\n    get_kwargs_str,\n    log_error_or_warning,\n    log_host_command_error,\n    make_hash,\n    print_host_combined_output,\n)\nfrom pyinfra.connectors.util import CommandOutput\nfrom pyinfra.context import ctx_host, ctx_state\n\n\nfrom .arguments import get_connector_argument_keys\n\nif TYPE_CHECKING:\n    from pyinfra.api.host import Host\n    from pyinfra.api.state import State\n\nSUDO_REGEX = r\"^sudo: unknown user:\"\nSU_REGEXES = (\n    r\"^su: user .+ does not exist\",\n    r\"^su: unknown login\",\n)\n\n\nclass FactNameMeta(type):\n    def __init__(cls, name: str, bases, attrs, **kwargs):\n        super().__init__(name, bases, attrs, **kwargs)\n        module_name = cls.__module__.replace(\"pyinfra.facts.\", \"\")\n        cls.name = f\"{module_name}.{cls.__name__}\"\n\n\nclass FactBase(metaclass=FactNameMeta):\n    name: str\n\n    abstract: bool = True\n\n    shell_executable: Optional[str] = None\n\n    requires_command: Optional[str] = None\n\n    command: Union[str, Callable]\n\n    @staticmethod\n    def default():\n        \"\"\"\n        Set the default attribute to be a type (eg list/dict).\n        \"\"\"\n\n    @staticmethod\n    def process(output):\n        return \"\\n\".join(output)\n\n    def process_pipeline(self, args, output):\n        return {arg: self.process([output[i]]) for i, arg in enumerate(args)}\n\n\nclass ShortFactBase(metaclass=FactNameMeta):\n    fact: type[FactBase]\n\n    @staticmethod\n    def process_data(data):\n        return data\n\n\ndef get_short_facts(state: \"State\", host: \"Host\", short_fact, **kwargs):\n    fact_data = get_fact(state, host, short_fact.fact, **kwargs)\n    return short_fact().process_data(fact_data)\n\n\ndef _make_command(command_attribute, host_args):\n    if callable(command_attribute):\n        host_args.pop(\"self\", None)\n        return command_attribute(**host_args)\n    return command_attribute\n\n\ndef _get_executor_kwargs(\n    state: \"State\",\n    host: \"Host\",\n    override_kwargs: Optional[dict[str, Any]] = None,\n    override_kwarg_keys: Optional[list[str]] = None,\n):\n    if override_kwargs is None:\n        override_kwargs = {}\n    if override_kwarg_keys is None:\n        override_kwarg_keys = []\n\n    # Use the current operation global kwargs, or generate defaults\n    global_kwargs = host.current_op_global_arguments\n    if not global_kwargs:\n        global_kwargs, _ = pop_global_arguments({}, state, host)\n\n    # Apply any current op kwargs that *weren't* found in the overrides\n    override_kwargs.update(\n        {key: value for key, value in global_kwargs.items() if key not in override_kwarg_keys},\n    )\n\n    return {\n        key: value for key, value in override_kwargs.items() if key in get_connector_argument_keys()\n    }\n\n\ndef _handle_fact_kwargs(state, host, cls, args, kwargs):\n    args = args or []\n    kwargs = kwargs or {}\n\n    # TODO: this is here to avoid popping stuff accidentally, this is horrible! Change the\n    # pop function to return the clean kwargs to avoid the indirect mutation.\n    kwargs = kwargs.copy()\n\n    # Get the defaults *and* overrides by popping from kwargs, executor kwargs passed\n    # into get_fact override everything else (applied below).\n    override_kwargs, override_kwarg_keys = pop_global_arguments(\n        kwargs,\n        state=state,\n        host=host,\n        keys_to_check=get_connector_argument_keys(),\n    )\n\n    executor_kwargs = _get_executor_kwargs(\n        state,\n        host,\n        override_kwargs=override_kwargs,  # type: ignore[arg-type]\n        override_kwarg_keys=override_kwarg_keys,\n    )\n\n    fact_kwargs = {}\n\n    if args or kwargs:\n        assert not isinstance(cls.command, str)\n        # Merges args & kwargs into a single kwargs dictionary\n        fact_kwargs = getcallargs(cls().command, *args, **kwargs)\n\n    return fact_kwargs, executor_kwargs\n\n\n\n\n# The code to be completed is:\ndef get_facts(state: \"State\", *args, **kwargs):\n\n    \"\"\"\n    This function retrieves facts for a given state. It iterates over the active hosts in the state's inventory and spawns a greenlet for each host to retrieve the facts. It then waits for the greenlets to complete and stores the results in a dictionary.\n    Input-Output Arguments\n    :param state: State. An instance of the State class. The state for which to retrieve the facts.\n    :param *args: Variable length argument list. Additional arguments to pass to the get_fact function.\n    :param **kwargs: Arbitrary keyword arguments. Additional keyword arguments to pass to the get_fact function.\n    :return: dict. A dictionary containing the retrieved facts, with the host as the key and the facts as the value.\n    \"\"\""}
{"namespace": "boto.dynamodb2.fields.BaseIndexField.schema", "prompt": "# Please complete the schema function based on the contexts above the function.\n\n# The contexts above the function are:\nfrom boto.dynamodb2.types import STRING\n\n\nclass BaseSchemaField(object):\n    \"\"\"\n    An abstract class for defining schema fields.\n\n    Contains most of the core functionality for the field. Subclasses must\n    define an ``attr_type`` to pass to DynamoDB.\n    \"\"\"\n    attr_type = None\n\n    def __init__(self, name, data_type=STRING):\n        \"\"\"\n        Creates a Python schema field, to represent the data to pass to\n        DynamoDB.\n\n        Requires a ``name`` parameter, which should be a string name of the\n        field.\n\n        Optionally accepts a ``data_type`` parameter, which should be a\n        constant from ``boto.dynamodb2.types``. (Default: ``STRING``)\n        \"\"\"\n        self.name = name\n        self.data_type = data_type\n\n    def definition(self):\n        \"\"\"\n        Returns the attribute definition structure DynamoDB expects.\n\n        Example::\n\n            >>> field.definition()\n            {\n                'AttributeName': 'username',\n                'AttributeType': 'S',\n            }\n\n        \"\"\"\n        return {\n            'AttributeName': self.name,\n            'AttributeType': self.data_type,\n        }\n\n    def schema(self):\n        \"\"\"\n        Returns the schema structure DynamoDB expects.\n\n        Example::\n\n            >>> field.schema()\n            {\n                'AttributeName': 'username',\n                'KeyType': 'HASH',\n            }\n\n        \"\"\"\n        return {\n            'AttributeName': self.name,\n            'KeyType': self.attr_type,\n        }\n\n\nclass HashKey(BaseSchemaField):\n    \"\"\"\n    An field representing a hash key.\n\n    Example::\n\n        >>> from boto.dynamodb2.types import NUMBER\n        >>> HashKey('username')\n        >>> HashKey('date_joined', data_type=NUMBER)\n\n    \"\"\"\n    attr_type = 'HASH'\n\n\nclass RangeKey(BaseSchemaField):\n    \"\"\"\n    An field representing a range key.\n\n    Example::\n\n        >>> from boto.dynamodb2.types import NUMBER\n        >>> HashKey('username')\n        >>> HashKey('date_joined', data_type=NUMBER)\n\n    \"\"\"\n    attr_type = 'RANGE'\n\n\nclass BaseIndexField(object):\n    \"\"\"\n    An abstract class for defining schema indexes.\n\n    Contains most of the core functionality for the index. Subclasses must\n    define a ``projection_type`` to pass to DynamoDB.\n    \"\"\"\n    def __init__(self, name, parts):\n        self.name = name\n        self.parts = parts\n\n    def definition(self):\n        \"\"\"\n        Returns the attribute definition structure DynamoDB expects.\n\n        Example::\n\n            >>> index.definition()\n            {\n                'AttributeName': 'username',\n                'AttributeType': 'S',\n            }\n\n        \"\"\"\n        definition = []\n\n        for part in self.parts:\n            definition.append({\n                'AttributeName': part.name,\n                'AttributeType': part.data_type,\n            })\n\n        return definition\n\n\n\n# The code to be completed is:\n    def schema(self):\n\n        \"\"\"\n        This function returns the schema structure that DynamoDB expects for the given index field. It constructs the schema by iterating over the parts of the index field and appending their schemas to the key schema.\n        Input-Output Arguments\n        :param self: BaseIndexField. An instance of the BaseIndexField class.\n        :return: Dict. The schema structure that DynamoDB expects for the index field. The structure includes the index name, key schema, and projection type.\n        \"\"\""}
{"namespace": "pyinfra.api.operation.add_op", "prompt": "# Please complete the add_op function based on the contexts above the function.\n\n# The contexts above the function are:\n\"\"\"\nOperations are the core of pyinfra. The ``@operation`` wrapper intercepts calls\nto the function and instead diff against the remote server, outputting commands\nto the deploy state. This is then run later by pyinfra's ``__main__`` or the\n:doc:`./pyinfra.api.operations` module.\n\"\"\"\n\nfrom __future__ import annotations\n\nfrom functools import wraps\nfrom io import StringIO\nfrom types import FunctionType\nfrom typing import Any, Iterator, Optional, Set, Tuple\n\nimport pyinfra\nfrom pyinfra import context, logger\n\n\nfrom .arguments import AllArguments, get_execution_kwarg_keys, pop_global_arguments\nfrom .command import PyinfraCommand, StringCommand\nfrom .exceptions import OperationValueError, PyinfraError\nfrom .host import Host\nfrom .operations import run_host_op\nfrom .state import State, StateOperationHostData, StateOperationMeta\nfrom .util import (\n    get_call_location,\n    get_file_sha1,\n    get_operation_order_from_stack,\n    log_operation_start,\n    make_hash,\n)\n\nop_meta_default = object()\n\n\nclass OperationMeta:\n    combined_output_lines = None\n    commands: Optional[list[Any]] = None\n    changed: bool = False\n    success: Optional[bool] = None\n\n    def __init__(self, hash=None, is_change=False):\n        self.hash = hash\n        self.changed = is_change\n\n    def __repr__(self) -> str:\n        \"\"\"\n        Return Operation object as a string.\n        \"\"\"\n\n        return f\"OperationMeta(changed={self.changed}, hash={self.hash})\"\n\n    def set_combined_output_lines(self, combined_output_lines):\n        self.combined_output_lines = combined_output_lines\n\n    def set_commands(self, commands) -> None:\n        self.commands = commands\n\n    def set_result(self, success: bool) -> None:\n        self.success = success\n\n    def _get_lines(self, types=(\"stdout\", \"stderr\")):\n        if self.combined_output_lines is None:\n            raise AttributeError(\"Output is not available until operations have been executed\")\n\n        return [line for type_, line in self.combined_output_lines if type_ in types]\n\n    @property\n    def stdout_lines(self):\n        return self._get_lines(types=(\"stdout\",))\n\n    @property\n    def stderr_lines(self):\n        return self._get_lines(types=(\"stderr\",))\n\n    @property\n    def stdout(self):\n        return \"\\n\".join(self.stdout_lines)\n\n    @property\n    def stderr(self):\n        return \"\\n\".join(self.stderr_lines)\n\n\n\n\n# The code to be completed is:\ndef add_op(state: State, op_func, *args, **kwargs):\n\n    \"\"\"\n    This function prepares and adds an operation to the input `pyinfra.State` instance by executing it on all hosts. It takes the operation function and its arguments as input and executes the function on each host.\n    Input-Output Arguments\n    :param state: State. An instance of the pyinfra.State class. The deploy state to add the operation to.\n    :param op_func: function. The operation function from one of the modules, such as `server.user`.\n    :param args/kwargs: Additional arguments passed to the operation function.\n    :return: No return values.\n    \"\"\""}
{"namespace": "boto.route53.domains.connect_to_region", "prompt": "# Please complete the connect_to_region function based on the contexts above the function.\n\n# The contexts above the function are:\n# Copyright (c) 2014 Amazon.com, Inc. or its affiliates.  All Rights Reserved\n#\n# Permission is hereby granted, free of charge, to any person obtaining a\n# copy of this software and associated documentation files (the\n# \"Software\"), to deal in the Software without restriction, including\n# without limitation the rights to use, copy, modify, merge, publish, dis-\n# tribute, sublicense, and/or sell copies of the Software, and to permit\n# persons to whom the Software is furnished to do so, subject to the fol-\n# lowing conditions:\n#\n# The above copyright notice and this permission notice shall be included\n# in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS\n# OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABIL-\n# ITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT\n# SHALL THE AUTHOR BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY,\n# WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS\n# IN THE SOFTWARE.\n#\nfrom boto.regioninfo import RegionInfo, get_regions\n\n\n\ndef regions():\n    \"\"\"\n    Get all available regions for the Amazon Route 53 Domains service.\n    :rtype: list\n    :return: A list of :class:`boto.regioninfo.RegionInfo`\n    \"\"\"\n    from boto.route53.domains.layer1 import Route53DomainsConnection\n    return get_regions('route53domains',\n                       connection_cls=Route53DomainsConnection)\n\n\n\n\n# The code to be completed is:\ndef connect_to_region(region_name, **kw_params):\n\n    \"\"\"\n    Connect to a specific region in the AWS Route53 service. It creates a connection to the Route53Domains service in the specified region using the provided parameters.\n    Input-Output Arguments\n    :param region_name: String. The name of the region to connect to.\n    :param **kw_params: Additional keyword arguments that can be passed to the connection.\n    :return: Route53DomainsConnection. The connection object to the Route53Domains service in the specified region.\n    \"\"\""}
{"namespace": "mssqlcli.packages.parseutils.ctes.extract_ctes", "prompt": "# Please complete the extract_ctes function based on the contexts above the function.\n\n# The contexts above the function are:\nfrom __future__ import unicode_literals\nfrom collections import namedtuple\nfrom sqlparse import parse\nfrom sqlparse.tokens import Keyword, CTE, DML\nfrom sqlparse.sql import Identifier, IdentifierList, Parenthesis\nfrom .meta import TableMetadata, ColumnMetadata\n\n\n# TableExpression is a namedtuple representing a CTE, used internally\n# name: cte alias assigned in the query\n# columns: list of column names\n# start: index into the original string of the left parens starting the CTE\n# stop: index into the original string of the right parens ending the CTE\nTableExpression = namedtuple('TableExpression', 'name columns start stop')\n\n\ndef isolate_query_ctes(full_text, text_before_cursor):\n    \"\"\"Simplify a query by converting CTEs into table metadata objects\n    \"\"\"\n\n    if not full_text:\n        return full_text, text_before_cursor, tuple()\n\n    ctes, _ = extract_ctes(full_text)\n    if not ctes:\n        return full_text, text_before_cursor, ()\n\n    current_position = len(text_before_cursor)\n    meta = []\n\n    for cte in ctes:\n        if cte.start < current_position < cte.stop:\n            # Currently editing a cte - treat its body as the current full_text\n            text_before_cursor = full_text[cte.start:current_position]\n            full_text = full_text[cte.start:cte.stop]\n            return full_text, text_before_cursor, meta\n\n        # Append this cte to the list of available table metadata\n        cols = (ColumnMetadata(name, None, ()) for name in cte.columns)\n        meta.append(TableMetadata(cte.name, cols))\n\n    # Editing past the last cte (ie the main body of the query)\n    full_text = full_text[ctes[-1].stop:]\n    text_before_cursor = text_before_cursor[ctes[-1].stop:current_position]\n\n    return full_text, text_before_cursor, tuple(meta)\n\n\n\n\n# The code to be completed is:\ndef extract_ctes(sql):\n\n    \"\"\"\n    This function extracts constant table expressions (CTEs) from a given SQL query. It parses the query using a parser and checks if the first meaningful token is \"WITH\", which indicates the presence of CTEs. It then extracts the CTEs from the query and returns them as a list of TableExpression namedtuples. The function also returns the remaining SQL text after the CTEs have been stripped.\n    Input-Output Arguments\n    :param sql: String. The SQL query from which to extract CTEs.\n    :return: Tuple. The first element is a list of TableExpression namedtuples representing the extracted CTEs. The second element is the remaining SQL text after the CTEs have been stripped.\n    \"\"\""}
{"namespace": "pysimplesoap.simplexml.SimpleXMLElement.as_xml", "prompt": "# Please complete the as_xml function based on the contexts above the function.\n\n# The contexts above the function are:\n#!/usr/bin/python\n# -*- coding: utf-8 -*-\n# This program is free software; you can redistribute it and/or modify\n# it under the terms of the GNU Lesser General Public License as published by the\n# Free Software Foundation; either version 3, or (at your option) any later\n# version.\n#\n# This program is distributed in the hope that it will be useful, but\n# WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTIBILITY\n# or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n# for more details.\n\n\"\"\"Simple XML manipulation\"\"\"\n\n\nfrom __future__ import unicode_literals\nimport sys\nif sys.version > '3':\n    basestring = str\n    unicode = str\n\nimport logging\nimport re\nimport time\nimport xml.dom.minidom\n\nfrom . import __author__, __copyright__, __license__, __version__\n\n# Utility functions used for marshalling, moved aside for readability\nfrom .helpers import TYPE_MAP, TYPE_MARSHAL_FN, TYPE_UNMARSHAL_FN, \\\n                     REVERSE_TYPE_MAP, Struct, Date, Decimal\n\nlog = logging.getLogger(__name__)\n\n\nclass SimpleXMLElement(object):\n    \"\"\"Simple XML manipulation (simil PHP)\"\"\"\n\n    def __init__(self, text=None, elements=None, document=None,\n                 namespace=None, prefix=None, namespaces_map={}, jetty=False):\n        \"\"\"\n        :param namespaces_map: How to map our namespace prefix to that given by the client;\n          {prefix: received_prefix}\n        \"\"\"\n        self.__namespaces_map = namespaces_map\n        _rx = \"|\".join(namespaces_map.keys())  # {'external': 'ext', 'model': 'mod'} -> 'external|model'\n        self.__ns_rx = re.compile(r\"^(%s):.*$\" % _rx)  # And now we build an expression ^(external|model):.*$\n                                                       # to find prefixes in all xml nodes i.e.: <model:code>1</model:code>\n                                                       # and later change that to <mod:code>1</mod:code>\n        self.__ns = namespace\n        self.__prefix = prefix\n        self.__jetty = jetty                           # special list support\n\n        if text is not None:\n            try:\n                self.__document = xml.dom.minidom.parseString(text)\n            except:\n                log.error(text)\n                raise\n            self.__elements = [self.__document.documentElement]\n        else:\n            self.__elements = elements\n            self.__document = document\n\n    def add_child(self, name, text=None, ns=True):\n        \"\"\"Adding a child tag to a node\"\"\"\n        if not ns or self.__ns is False:\n            ##log.debug('adding %s without namespace', name)\n            element = self.__document.createElement(name)\n        else:\n            ##log.debug('adding %s ns \"%s\" %s', name, self.__ns, ns)\n            if isinstance(ns, basestring):\n                element = self.__document.createElement(name)\n                if ns:\n                    element.setAttribute(\"xmlns\", ns)\n            elif self.__prefix:\n                element = self.__document.createElementNS(self.__ns, \"%s:%s\" % (self.__prefix, name))\n            else:\n                element = self.__document.createElementNS(self.__ns, name)\n        # don't append null tags!\n        if text is not None:\n            if isinstance(text, xml.dom.minidom.CDATASection):\n                element.appendChild(self.__document.createCDATASection(text.data))\n            else:\n                element.appendChild(self.__document.createTextNode(text))\n        self._element.appendChild(element)\n        return SimpleXMLElement(\n            elements=[element],\n            document=self.__document,\n            namespace=self.__ns,\n            prefix=self.__prefix,\n            jetty=self.__jetty,\n            namespaces_map=self.__namespaces_map\n        )\n\n    def __setattr__(self, tag, text):\n        \"\"\"Add text child tag node (short form)\"\"\"\n        if tag.startswith(\"_\"):\n            object.__setattr__(self, tag, text)\n        else:\n            ##log.debug('__setattr__(%s, %s)', tag, text)\n            self.add_child(tag, text)\n\n    def __delattr__(self, tag):\n        \"\"\"Remove a child tag (non recursive!)\"\"\"\n        elements = [__element for __element in self._element.childNodes\n                    if __element.nodeType == __element.ELEMENT_NODE]\n        for element in elements:\n            self._element.removeChild(element)\n\n    def add_comment(self, data):\n        \"\"\"Add an xml comment to this child\"\"\"\n        comment = self.__document.createComment(data)\n        self._element.appendChild(comment)\n\n\n\n# The code to be completed is:\n    def as_xml(self, filename=None, pretty=False):\n\n        \"\"\"\n        This function returns the XML representation of the document. If the \"pretty\" parameter is set to False, it returns the XML representation without any formatting. If \"pretty\" is set to True, it returns the XML representation with indentation and line breaks for better readability.\n        Input-Output Arguments\n        :param self: SimpleXMLElement. An instance of the SimpleXMLElement class.\n        :param filename: String [optional]. The name of the file to save the XML representation. Defaults to None.\n        :param pretty: Bool. Whether to format the XML representation with indentation and line breaks. Defaults to False.\n        :return: String. The XML representation of the document.\n        \"\"\""}
{"namespace": "trailscraper.s3_download._s3_key_prefixes", "prompt": "# Please complete the _s3_key_prefixes function based on the contexts above the function.\n\n# The contexts above the function are:\n\"\"\"Functions to download CloudTrail Logs from S3\"\"\"\nimport concurrent.futures\nimport datetime\nimport logging\nimport os\nimport threading\n\nimport boto3\nimport pytz\n\nfrom trailscraper.collection_utils import consume\n\n\ndef _s3_key_prefix(prefix, date, account_id, region):\n    return f\"{prefix}AWSLogs/{account_id}/CloudTrail/{region}/{date.year}/{date.month:02d}/{date.day:02d}/\"\n\ndef _s3_key_prefix_for_org_trails(prefix, date, org_id, account_id, region):\n    return f\"{prefix}AWSLogs/{org_id}/{account_id}/CloudTrail/{region}/{date.year}/{date.month:02d}/{date.day:02d}/\"\n\n\n# pylint: disable=too-many-arguments\n\n\n# The code to be completed is:\ndef _s3_key_prefixes(prefix, org_ids, account_ids, regions, from_date, to_date):\n\n    \"\"\"\n    This function generates a list of S3 key prefixes based on the given parameters. It first calculates the delta between the two dates, then generates a list of dates based on the delta. It then creates a list of S3 key prefixes based on the organization IDs, account IDs, regions, and dates.\n    Input-Output Arguments\n    :param prefix: String. The prefix for the S3 key.\n    :param org_ids: List of Strings. The organization IDs.\n    :param account_ids: List of Strings. The account IDs.\n    :param regions: List of Strings. The regions.\n    :param from_date: Datetime. The start date.\n    :param to_date: Datetime. The end date.\n    :return: List of Strings. The list of S3 key prefixes.\n    \"\"\""}
{"namespace": "alembic.testing.fixtures.capture_engine_context_buffer", "prompt": "# Please complete the capture_engine_context_buffer function based on the contexts above the function.\n\n# The contexts above the function are:\nfrom __future__ import annotations\n\nimport configparser\nfrom contextlib import contextmanager\nimport io\nimport re\nfrom typing import Any\nfrom typing import Dict\n\nfrom sqlalchemy import Column\nfrom sqlalchemy import inspect\nfrom sqlalchemy import MetaData\nfrom sqlalchemy import String\nfrom sqlalchemy import Table\nfrom sqlalchemy import testing\nfrom sqlalchemy import text\nfrom sqlalchemy.testing import config\nfrom sqlalchemy.testing import mock\nfrom sqlalchemy.testing.assertions import eq_\nfrom sqlalchemy.testing.fixtures import TablesTest as SQLAlchemyTablesTest\nfrom sqlalchemy.testing.fixtures import TestBase as SQLAlchemyTestBase\n\nimport alembic\nfrom .assertions import _get_dialect\nfrom ..environment import EnvironmentContext\nfrom ..migration import MigrationContext\nfrom ..operations import Operations\nfrom ..util import sqla_compat\n\nfrom ..util.sqla_compat import sqla_14\nfrom ..util.sqla_compat import sqla_2\n\n\ntesting_config = configparser.ConfigParser()\ntesting_config.read([\"test.cfg\"])\n\n\nclass TestBase(SQLAlchemyTestBase):\n    is_sqlalchemy_future = sqla_2\n\n    @testing.fixture()\n    def ops_context(self, migration_context):\n        with migration_context.begin_transaction(_per_migration=True):\n            yield Operations(migration_context)\n\n    @testing.fixture\n    def migration_context(self, connection):\n        return MigrationContext.configure(\n            connection, opts=dict(transaction_per_migration=True)\n        )\n\n    @testing.fixture\n    def connection(self):\n        with config.db.connect() as conn:\n            yield conn\n\n\nclass TablesTest(TestBase, SQLAlchemyTablesTest):\n    pass\n\n\nif sqla_14:\n    from sqlalchemy.testing.fixtures import FutureEngineMixin\nelse:\n\n    class FutureEngineMixin:  # type:ignore[no-redef]\n        __requires__ = (\"sqlalchemy_14\",)\n\n\nFutureEngineMixin.is_sqlalchemy_future = True\n\n\ndef capture_db(dialect=\"postgresql://\"):\n    from ..util.sqla_compat import create_mock_engine\n    buf = []\n\n    def dump(sql, *multiparams, **params):\n        buf.append(str(sql.compile(dialect=engine.dialect)))\n\n    engine = create_mock_engine(dialect, dump)\n    return engine, buf\n\n\n_engs: Dict[Any, Any] = {}\n\n\n@contextmanager\ndef capture_context_buffer(**kw):\n    if kw.pop(\"bytes_io\", False):\n        buf = io.BytesIO()\n    else:\n        buf = io.StringIO()\n\n    kw.update({\"dialect_name\": \"sqlite\", \"output_buffer\": buf})\n    conf = EnvironmentContext.configure\n\n    def configure(*arg, **opt):\n        opt.update(**kw)\n        return conf(*arg, **opt)\n\n    with mock.patch.object(EnvironmentContext, \"configure\", configure):\n        yield buf\n\n\n@contextmanager\n\n\n# The code to be completed is:\ndef capture_engine_context_buffer(**kw):\n\n    \"\"\"\n    This function captures the engine context buffer by writing the executed SQL statements into a buffer. It creates a SQLite database engine, connects to it, and sets up a listener to write the executed statements into the buffer. It also updates the input parameters and configures the environment context. Finally, it yields the buffer.\n    Input-Output Arguments\n    :param **kw: Keyword arguments. Additional parameters that can be passed to the function.\n    :return: A buffer object that contains the executed SQL statements.\n    \"\"\""}
{"namespace": "kinto.core.storage.generators.Generator.match", "prompt": "# Please complete the match function based on the contexts above the function.\n\n# The contexts above the function are:\nimport re\nfrom uuid import uuid4\n\n\nclass Generator:\n    \"\"\"Base generator for objects ids.\n\n    Id generators are used by storage backend during object creation, and at\n    resource level to validate object id in requests paths.\n    \"\"\"\n\n    regexp = r\"^[a-zA-Z0-9][a-zA-Z0-9_-]*$\"\n    \"\"\"Default object id pattern. Can be changed to comply with custom ids.\"\"\"\n\n    def __init__(self, config=None):\n        self.config = config\n        self._regexp = None\n\n        if not self.match(self()):\n            error_msg = \"Generated object id does comply with regexp.\"\n            raise ValueError(error_msg)\n\n\n\n# The code to be completed is:\n    def match(self, object_id):\n\n        \"\"\"\n        This function validates whether the given object id matches the expected format defined by the generator. It checks if the object id matches the regular expression pattern defined by the generator.\n        Input-Output Arguments\n        :param self: Generator. An instance of the Generator class.\n        :param object_id: The object id to be validated.\n        :return: bool. Returns True if the object id matches the expected format, otherwise False.\n        \"\"\""}
{"namespace": "mopidy.config.types.encode", "prompt": "# Please complete the encode function based on the contexts above the function.\n\n# The contexts above the function are:\nimport logging\nimport re\nimport socket\n\nfrom mopidy.config import validators\nfrom mopidy.internal import log, path\n\n\ndef decode(value):\n    if isinstance(value, bytes):\n        value = value.decode(errors=\"surrogateescape\")\n\n    for char in (\"\\\\\", \"\\n\", \"\\t\"):\n        value = value.replace(\n            char.encode(encoding=\"unicode-escape\").decode(), char\n        )\n\n    return value\n\n\n\n\n# The code to be completed is:\ndef encode(value):\n\n    \"\"\"\n    This function encodes the given value. If the value is of type bytes, it decodes it using the \"surrogateescape\" error handler. Then, it replaces the characters \"\\\" with \"\\\\n\" and \"\\t\" with \"\\\\t\" and returns the encoded value.\n    Input-Output Arguments\n    :param value: The value to be encoded.\n    :return: The encoded value.\n    \"\"\""}
{"namespace": "praw.util.token_manager.FileTokenManager.pre_refresh_callback", "prompt": "# Please complete the pre_refresh_callback function based on the contexts above the function.\n\n# The contexts above the function are:\n\"\"\"Token Manager classes.\n\nThere should be a 1-to-1 mapping between an instance of a subclass of\n:class:`.BaseTokenManager` and a :class:`.Reddit` instance.\n\nA few proof of concept token manager classes are provided here, but it is expected that\nPRAW users will create their own token manager classes suitable for their needs.\n\n.. deprecated:: 7.4.0\n\n    Tokens managers have been deprecated and will be removed in the near future.\n\n\"\"\"\nfrom abc import ABC, abstractmethod\n\nfrom . import _deprecate_args\n\n\nclass BaseTokenManager(ABC):\n    \"\"\"An abstract class for all token managers.\"\"\"\n\n    @abstractmethod\n    def post_refresh_callback(self, authorizer):\n        \"\"\"Handle callback that is invoked after a refresh token is used.\n\n        :param authorizer: The ``prawcore.Authorizer`` instance used containing\n            ``access_token`` and ``refresh_token`` attributes.\n\n        This function will be called after refreshing the access and refresh tokens.\n        This callback can be used for saving the updated ``refresh_token``.\n\n        \"\"\"\n\n    @abstractmethod\n    def pre_refresh_callback(self, authorizer):\n        \"\"\"Handle callback that is invoked before refreshing PRAW's authorization.\n\n        :param authorizer: The ``prawcore.Authorizer`` instance used containing\n            ``access_token`` and ``refresh_token`` attributes.\n\n        This callback can be used to inspect and modify the attributes of the\n        ``prawcore.Authorizer`` instance, such as setting the ``refresh_token``.\n\n        \"\"\"\n\n    @property\n    def reddit(self):\n        \"\"\"Return the :class:`.Reddit` instance bound to the token manager.\"\"\"\n        return self._reddit\n\n    @reddit.setter\n    def reddit(self, value):\n        if self._reddit is not None:\n            raise RuntimeError(\n                \"'reddit' can only be set once and is done automatically\"\n            )\n        self._reddit = value\n\n    def __init__(self):\n        \"\"\"Initialize a :class:`.BaseTokenManager` instance.\"\"\"\n        self._reddit = None\n\n\nclass FileTokenManager(BaseTokenManager):\n    \"\"\"Provides a single-file based token manager.\n\n    It is expected that the file with the initial ``refresh_token`` is created prior to\n    use.\n\n    .. warning::\n\n        The same ``file`` should not be used by more than one instance of this class\n        concurrently. Doing so may result in data corruption. Consider using\n        :class:`.SQLiteTokenManager` if you want more than one instance of PRAW to\n        concurrently manage a specific ``refresh_token`` chain.\n\n    \"\"\"\n\n    def __init__(self, filename):\n        \"\"\"Initialize a :class:`.FileTokenManager` instance.\n\n        :param filename: The file the contains the refresh token.\n\n        \"\"\"\n        super().__init__()\n        self._filename = filename\n\n    def post_refresh_callback(self, authorizer):\n        \"\"\"Update the saved copy of the refresh token.\"\"\"\n        with open(self._filename, \"w\") as fp:\n            fp.write(authorizer.refresh_token)\n\n\n\n# The code to be completed is:\n    def pre_refresh_callback(self, authorizer):\n\n        \"\"\"\n        This function loads the refresh token from the file and assigns it to the authorizer if it is not already set.\n        Input-Output Arguments\n        :param self: FileTokenManager. An instance of the FileTokenManager class.\n        :param authorizer: The authorizer object that needs to be refreshed.\n        :return: No return values.\n        \"\"\""}
{"namespace": "mssqlcli.util.is_command_valid", "prompt": "# Please complete the is_command_valid function based on the contexts above the function.\n\n# The contexts above the function are:\nfrom os import devnull\nimport subprocess\n\ndef encode(s):\n    try:\n        return s.encode('utf-8')\n    except (AttributeError, SyntaxError):\n        pass\n    return s\n\n# In Python 3, all strings are sequences of Unicode characters.\n# There is a bytes type that holds raw bytes.\n# In Python 2, a string may be of type str or of type unicode.\ndef decode(s):\n    try:\n        return s.decode('utf-8')\n    except (AttributeError, SyntaxError, UnicodeEncodeError):\n        pass\n    return s\n\n\n\n# The code to be completed is:\ndef is_command_valid(command):\n\n    \"\"\"\n    Check if the command is recognized on the machine. It is used to determine the installation of the 'less' pager. If the command is empty or if calling the command silently throws an OSError, the function returns False. Otherwise, it returns True.\n    Input-Output Arguments\n    :param command: String. The command to be checked.\n    :return: Bool. True if the command is recognized, False otherwise.\n    \"\"\""}
{"namespace": "mingus.core.notes.diminish", "prompt": "# Please complete the diminish function based on the contexts above the function.\n\n# The contexts above the function are:\n# -*- coding: utf-8 -*-\n\n#    mingus - Music theory Python package, notes module.\n#    Copyright (C) 2008-2009, Bart Spaans\n#    Copyright (C) 2011, Carlo Stemberger\n#\n#    This program is free software: you can redistribute it and/or modify\n#    it under the terms of the GNU General Public License as published by\n#    the Free Software Foundation, either version 3 of the License, or\n#    (at your option) any later version.\n#\n#    This program is distributed in the hope that it will be useful,\n#    but WITHOUT ANY WARRANTY; without even the implied warranty of\n#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#    GNU General Public License for more details.\n#\n#    You should have received a copy of the GNU General Public License\n#    along with this program.  If not, see <http://www.gnu.org/licenses/>.\n\n\"\"\"Basic module for notes.\n\nThis module is the foundation of the music theory package.\n\nIt handles conversions from integers to notes and vice versa and thus\nenables simple calculations.\n\"\"\"\nfrom __future__ import absolute_import\n\nfrom mingus.core.mt_exceptions import NoteFormatError\nfrom six.moves import range\n\n_note_dict = {\"C\": 0, \"D\": 2, \"E\": 4, \"F\": 5, \"G\": 7, \"A\": 9, \"B\": 11}\nfifths = [\"F\", \"C\", \"G\", \"D\", \"A\", \"E\", \"B\"]\n\n\ndef int_to_note(note_int, accidentals=\"#\"):\n    \"\"\"Convert integers in the range of 0-11 to notes in the form of C or C#\n    or Db.\n\n    Throw a RangeError exception if the note_int is not in the range 0-11.\n\n    If not specified, sharps will be used.\n\n    Examples:\n    >>> int_to_note(0)\n    'C'\n    >>> int_to_note(3)\n    'D#'\n    >>> int_to_note(3, 'b')\n    'Eb'\n    \"\"\"\n    from mingus.core.mt_exceptions import RangeError\n    from mingus.core.mt_exceptions import FormatError\n    if note_int not in range(12):\n        raise RangeError(\"int out of bounds (0-11): %d\" % note_int)\n    ns = [\"C\", \"C#\", \"D\", \"D#\", \"E\", \"F\", \"F#\", \"G\", \"G#\", \"A\", \"A#\", \"B\"]\n    nf = [\"C\", \"Db\", \"D\", \"Eb\", \"E\", \"F\", \"Gb\", \"G\", \"Ab\", \"A\", \"Bb\", \"B\"]\n    if accidentals == \"#\":\n        return ns[note_int]\n    elif accidentals == \"b\":\n        return nf[note_int]\n    else:\n        raise FormatError(\"'%s' not valid as accidental\" % accidentals)\n\n\ndef is_enharmonic(note1, note2):\n    \"\"\"Test whether note1 and note2 are enharmonic, i.e. they sound the same.\"\"\"\n    return note_to_int(note1) == note_to_int(note2)\n\n\ndef is_valid_note(note):\n    \"\"\"Return True if note is in a recognised format. False if not.\"\"\"\n    if note[0] not in _note_dict:\n        return False\n    for post in note[1:]:\n        if post != \"b\" and post != \"#\":\n            return False\n    return True\n\n\ndef note_to_int(note):\n    \"\"\"Convert notes in the form of C, C#, Cb, C##, etc. to an integer in the\n    range of 0-11.\n\n    Throw a NoteFormatError exception if the note format is not recognised.\n    \"\"\"\n    if is_valid_note(note):\n        val = _note_dict[note[0]]\n    else:\n        raise NoteFormatError(\"Unknown note format '%s'\" % note)\n\n    # Check for '#' and 'b' postfixes\n    for post in note[1:]:\n        if post == \"b\":\n            val -= 1\n        elif post == \"#\":\n            val += 1\n    return val % 12\n\n\ndef reduce_accidentals(note):\n    \"\"\"Reduce any extra accidentals to proper notes.\n\n    Example:\n    >>> reduce_accidentals('C####')\n    'E'\n    \"\"\"\n    val = note_to_int(note[0])\n    for token in note[1:]:\n        if token == \"b\":\n            val -= 1\n        elif token == \"#\":\n            val += 1\n        else:\n            raise NoteFormatError(\"Unknown note format '%s'\" % note)\n    if val >= note_to_int(note[0]):\n        return int_to_note(val % 12)\n    else:\n        return int_to_note(val % 12, \"b\")\n\n\ndef remove_redundant_accidentals(note):\n    \"\"\"Remove redundant sharps and flats from the given note.\n\n    Examples:\n    >>> remove_redundant_accidentals('C##b')\n    'C#'\n    >>> remove_redundant_accidentals('Eb##b')\n    'E'\n    \"\"\"\n    val = 0\n    for token in note[1:]:\n        if token == \"b\":\n            val -= 1\n        elif token == \"#\":\n            val += 1\n    result = note[0]\n    while val > 0:\n        result = augment(result)\n        val -= 1\n    while val < 0:\n        result = diminish(result)\n        val += 1\n    return result\n\n\ndef augment(note):\n    \"\"\"Augment a given note.\n\n    Examples:\n    >>> augment('C')\n    'C#'\n    >>> augment('Cb')\n    'C'\n    \"\"\"\n    if note[-1] != \"b\":\n        return note + \"#\"\n    else:\n        return note[:-1]\n\n\n\n\n# The code to be completed is:\ndef diminish(note):\n\n    \"\"\"\n    Diminish a given note. If the note does not end with \"#\", it adds \"b\" to the note. Otherwise, it removes the last character \"#\" from the note.\n    Input-Output Arguments\n    :param note: String. The note to be diminished.\n    :return: String. The diminished note.\n    \"\"\""}
{"namespace": "asyncssh.packet.String", "prompt": "# Please complete the String function based on the contexts above the function.\n\n# The contexts above the function are:\n# Copyright (c) 2013-2021 by Ron Frederick <ronf@timeheart.net> and others.\n#\n# This program and the accompanying materials are made available under\n# the terms of the Eclipse Public License v2.0 which accompanies this\n# distribution and is available at:\n#\n#     http://www.eclipse.org/legal/epl-2.0/\n#\n# This program may also be made available under the following secondary\n# licenses when the conditions for such availability set forth in the\n# Eclipse Public License v2.0 are satisfied:\n#\n#    GNU General Public License, Version 2.0, or any later versions of\n#    that license\n#\n# SPDX-License-Identifier: EPL-2.0 OR GPL-2.0-or-later\n#\n# Contributors:\n#     Ron Frederick - initial implementation, API, and documentation\n\n\"\"\"SSH packet encoding and decoding functions\"\"\"\n\nfrom typing import Any, Callable, Iterable, Mapping, Optional, Sequence, Union\n\nfrom .logging import SSHLogger\nfrom .misc import plural\n\n\n_LoggedPacket = Union[bytes, 'SSHPacket']\n_PacketHandler = Callable[[Any, int, int, 'SSHPacket'], None]\n\n\nclass PacketDecodeError(ValueError):\n    \"\"\"Packet decoding error\"\"\"\n\n\ndef Byte(value: int) -> bytes:\n    \"\"\"Encode a single byte\"\"\"\n\n    return bytes((value,))\n\n\ndef Boolean(value: bool) -> bytes:\n    \"\"\"Encode a boolean value\"\"\"\n\n    return Byte(bool(value))\n\n\ndef UInt16(value: int) -> bytes:\n    \"\"\"Encode a 16-bit integer value\"\"\"\n\n    return value.to_bytes(2, 'big')\n\n\ndef UInt32(value: int) -> bytes:\n    \"\"\"Encode a 32-bit integer value\"\"\"\n\n    return value.to_bytes(4, 'big')\n\n\ndef UInt64(value: int) -> bytes:\n    \"\"\"Encode a 64-bit integer value\"\"\"\n\n    return value.to_bytes(8, 'big')\n\n\n\n\n# The code to be completed is:\ndef String(value: Union[bytes, str]) -> bytes:\n\n    \"\"\"\n    This function encodes a byte string or UTF-8 string value. It first checks if the input value is a string, then encodes it to UTF-8. It then returns the length of the value in bytes followed by the value itself.\n    Input-Output Arguments\n    :param value: Union[bytes, str]. The input value to be encoded, which can be either a byte string or a UTF-8 string.\n    :return: bytes. The encoded byte string value.\n    \"\"\""}
{"namespace": "prometheus_client.mmap_dict.MmapedDict.read_value", "prompt": "# Please complete the read_value function based on the contexts above the function.\n\n# The contexts above the function are:\nimport json\nimport mmap\nimport os\nimport struct\nfrom typing import List\n\n_INITIAL_MMAP_SIZE = 1 << 16\n_pack_integer_func = struct.Struct(b'i').pack\n_pack_two_doubles_func = struct.Struct(b'dd').pack\n_unpack_integer = struct.Struct(b'i').unpack_from\n_unpack_two_doubles = struct.Struct(b'dd').unpack_from\n\n\n# struct.pack_into has atomicity issues because it will temporarily write 0 into\n# the mmap, resulting in false reads to 0 when experiencing a lot of writes.\n# Using direct assignment solves this issue.\n\n\ndef _pack_two_doubles(data, pos, value, timestamp):\n    data[pos:pos + 16] = _pack_two_doubles_func(value, timestamp)\n\n\ndef _pack_integer(data, pos, value):\n    data[pos:pos + 4] = _pack_integer_func(value)\n\n\ndef _read_all_values(data, used=0):\n    \"\"\"Yield (key, value, timestamp, pos). No locking is performed.\"\"\"\n\n    if used <= 0:\n        # If not valid `used` value is passed in, read it from the file.\n        used = _unpack_integer(data, 0)[0]\n\n    pos = 8\n\n    while pos < used:\n        encoded_len = _unpack_integer(data, pos)[0]\n        # check we are not reading beyond bounds\n        if encoded_len + pos > used:\n            raise RuntimeError('Read beyond file size detected, file is corrupted.')\n        pos += 4\n        encoded_key = data[pos:pos + encoded_len]\n        padded_len = encoded_len + (8 - (encoded_len + 4) % 8)\n        pos += padded_len\n        value, timestamp = _unpack_two_doubles(data, pos)\n        yield encoded_key.decode('utf-8'), value, timestamp, pos\n        pos += 16\n\n\nclass MmapedDict:\n    \"\"\"A dict of doubles, backed by an mmapped file.\n\n    The file starts with a 4 byte int, indicating how much of it is used.\n    Then 4 bytes of padding.\n    There's then a number of entries, consisting of a 4 byte int which is the\n    size of the next field, a utf-8 encoded string key, padding to a 8 byte\n    alignment, and then a 8 byte float which is the value and a 8 byte float\n    which is a UNIX timestamp in seconds.\n\n    Not thread safe.\n    \"\"\"\n\n    def __init__(self, filename, read_mode=False):\n        self._f = open(filename, 'rb' if read_mode else 'a+b')\n        self._fname = filename\n        capacity = os.fstat(self._f.fileno()).st_size\n        if capacity == 0:\n            self._f.truncate(_INITIAL_MMAP_SIZE)\n            capacity = _INITIAL_MMAP_SIZE\n        self._capacity = capacity\n        self._m = mmap.mmap(self._f.fileno(), self._capacity,\n                            access=mmap.ACCESS_READ if read_mode else mmap.ACCESS_WRITE)\n\n        self._positions = {}\n        self._used = _unpack_integer(self._m, 0)[0]\n        if self._used == 0:\n            self._used = 8\n            _pack_integer(self._m, 0, self._used)\n        else:\n            if not read_mode:\n                for key, _, _, pos in self._read_all_values():\n                    self._positions[key] = pos\n\n    @staticmethod\n    def read_all_values_from_file(filename):\n        with open(filename, 'rb') as infp:\n            # Read the first block of data, including the first 4 bytes which tell us\n            # how much of the file (which is preallocated to _INITIAL_MMAP_SIZE bytes) is occupied.\n            data = infp.read(mmap.PAGESIZE)\n            used = _unpack_integer(data, 0)[0]\n            if used > len(data):  # Then read in the rest, if needed.\n                data += infp.read(used - len(data))\n        return _read_all_values(data, used)\n\n    def _init_value(self, key):\n        \"\"\"Initialize a value. Lock must be held by caller.\"\"\"\n        encoded = key.encode('utf-8')\n        # Pad to be 8-byte aligned.\n        padded = encoded + (b' ' * (8 - (len(encoded) + 4) % 8))\n        value = struct.pack(f'i{len(padded)}sdd'.encode(), len(encoded), padded, 0.0, 0.0)\n        while self._used + len(value) > self._capacity:\n            self._capacity *= 2\n            self._f.truncate(self._capacity)\n            self._m = mmap.mmap(self._f.fileno(), self._capacity)\n        self._m[self._used:self._used + len(value)] = value\n\n        # Update how much space we've used.\n        self._used += len(value)\n        _pack_integer(self._m, 0, self._used)\n        self._positions[key] = self._used - 16\n\n    def _read_all_values(self):\n        \"\"\"Yield (key, value, pos). No locking is performed.\"\"\"\n        return _read_all_values(data=self._m, used=self._used)\n\n    def read_all_values(self):\n        \"\"\"Yield (key, value, timestamp). No locking is performed.\"\"\"\n        for k, v, ts, _ in self._read_all_values():\n            yield k, v, ts\n\n\n\n# The code to be completed is:\n    def read_value(self, key):\n\n        \"\"\"\n        Read the value corresponding to the given key from the MmapedDict instance. If the key is not found in the instance, it initializes the value and then returns it.\n        Input-Output Arguments\n        :param self: MmapedDict. An instance of the MmapedDict class.\n        :param key: The key to read the value from the instance.\n        :return: The value corresponding to the key.\n        \"\"\""}
{"namespace": "pycorrector.en_spell.EnSpell.check_init", "prompt": "# Please complete the check_init function based on the contexts above the function.\n\n# The contexts above the function are:\n# -*- coding: utf-8 -*-\n\"\"\"\n@author:XuMing(xuming624@qq.com)\n@description: english correction\nrefer: http://norvig.com/spell-correct.html\n\"\"\"\n\nimport gzip\nimport json\nimport operator\nimport os\nfrom codecs import open\nfrom collections import Counter\nfrom loguru import logger\nfrom pycorrector import config\n\n\n\n\ndef get_word_freq_dict_from_text(text):\n    from pycorrector.utils.tokenizer import whitespace_tokenize\n    return Counter(whitespace_tokenize(text))\n\n\nclass EnSpell(object):\n    def __init__(self, word_freq_dict={}):\n        # Word freq dict, k=word, v=int(freq)\n        self.word_freq_dict = word_freq_dict\n        self.custom_confusion = {}\n\n    def _init(self):\n        with gzip.open(config.en_dict_path, \"rb\") as f:\n            all_word_freq_dict = json.loads(f.read())\n            word_freq = {}\n            for k, v in all_word_freq_dict.items():\n                # 3400\n                if v > 400:\n                    word_freq[k] = v\n            self.word_freq_dict = word_freq\n            logger.debug(\"load en spell data: %s, size: %d\" % (config.en_dict_path,\n                                                               len(self.word_freq_dict)))\n\n\n\n# The code to be completed is:\n    def check_init(self):\n\n        \"\"\"\n        Check if the EnSpell instance has been initialized. If not, it initializes the instance.\n        Input-Output Arguments\n        :param self: EnSpell. An instance of the EnSpell class.\n        :return: No return values.\n        \"\"\""}
{"namespace": "faker.utils.checksums.calculate_luhn", "prompt": "# Please complete the calculate_luhn function based on the contexts above the function.\n\n# The contexts above the function are:\nfrom typing import List\n\n\ndef luhn_checksum(number: float) -> int:\n    def digits_of(n: float) -> List[int]:\n        return [int(d) for d in str(n)]\n\n    digits = digits_of(number)\n    odd_digits = digits[-1::-2]\n    even_digits = digits[-2::-2]\n    checksum = 0\n    checksum += sum(odd_digits)\n    for d in even_digits:\n        checksum += sum(digits_of(d * 2))\n    return checksum % 10\n\n\n\n\n# The code to be completed is:\ndef calculate_luhn(partial_number: float) -> int:\n\n    \"\"\"\n    This function calculates the checksum using Luhn's algorithm for a given partial number. It multiplies the partial number by 10, calculates the checksum, and returns the check digit. If the check digit is 0, it returns the check digit itself. Otherwise, it returns 10 minus the check digit.\n    Input-Output Arguments\n    :param partial_number: float. The partial number for which the checksum needs to be calculated.\n    :return: int. The calculated check digit using Luhn's algorithm.\n    \"\"\""}
{"namespace": "check_dummies.find_backend", "prompt": "# Please complete the find_backend function based on the contexts above the function.\n\n# The contexts above the function are:\n# coding=utf-8\n# Copyright 2023 The HuggingFace Inc. team.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport argparse\nimport os\nimport re\n\n\n# All paths are set with the intent you should run this script from the root of the repo with the command\n# python utils/check_dummies.py\nPATH_TO_DIFFUSERS = \"src/diffusers\"\n\n# Matches is_xxx_available()\n_re_backend = re.compile(r\"is\\_([a-z_]*)_available\\(\\)\")\n# Matches from xxx import bla\n_re_single_line_import = re.compile(r\"\\s+from\\s+\\S*\\s+import\\s+([^\\(\\s].*)\\n\")\n\n\nDUMMY_CONSTANT = \"\"\"\n{0} = None\n\"\"\"\n\nDUMMY_CLASS = \"\"\"\nclass {0}(metaclass=DummyObject):\n    _backends = {1}\n\n    def __init__(self, *args, **kwargs):\n        requires_backends(self, {1})\n\n    @classmethod\n    def from_config(cls, *args, **kwargs):\n        requires_backends(cls, {1})\n\n    @classmethod\n    def from_pretrained(cls, *args, **kwargs):\n        requires_backends(cls, {1})\n\"\"\"\n\n\nDUMMY_FUNCTION = \"\"\"\ndef {0}(*args, **kwargs):\n    requires_backends({0}, {1})\n\"\"\"\n\n\n\n\n# The code to be completed is:\ndef find_backend(line):\n\n    \"\"\"\n    This function searches for one or multiple backends in a code line of the init and returns them as a string joined by \"_and_\" if found.\n    Input-Output Arguments\n    :param line: str. The code line to search for backends.\n    :return: str or None. The backends found in the code line joined by \"_and_\" if found, otherwise None.\n    \"\"\""}
{"namespace": "mrjob.hadoop.fully_qualify_hdfs_path", "prompt": "# Please complete the fully_qualify_hdfs_path function based on the contexts above the function.\n\n# The contexts above the function are:\n# Copyright 2009-2016 Yelp and Contributors\n# Copyright 2017 Yelp\n# Copyright 2018 Yelp and Google, Inc.\n# Copyright 2019 Yelp\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport getpass\nimport logging\nimport os\nimport posixpath\nimport re\nfrom subprocess import CalledProcessError\nfrom subprocess import Popen\nfrom subprocess import PIPE\n\ntry:\n    import pty\n    pty  # quiet \"redefinition of unused ...\" warning from pyflakes\nexcept ImportError:\n    pty = None\n\nfrom mrjob.bin import MRJobBinRunner\nfrom mrjob.compat import uses_yarn\nfrom mrjob.conf import combine_dicts\nfrom mrjob.fs.composite import CompositeFilesystem\n\n\nfrom mrjob.logs.counters import _pick_counters\nfrom mrjob.logs.errors import _log_probable_cause_of_failure\nfrom mrjob.logs.mixin import LogInterpretationMixin\nfrom mrjob.logs.step import _eio_to_eof\nfrom mrjob.logs.step import _interpret_hadoop_jar_command_stderr\nfrom mrjob.logs.step import _is_counter_log4j_record\nfrom mrjob.logs.step import _log_line_from_driver\nfrom mrjob.logs.step import _log_log4j_record\nfrom mrjob.logs.wrap import _logs_exist\n\nfrom mrjob.py2 import to_unicode\nfrom mrjob.runner import _fix_env\nfrom mrjob.setup import UploadDirManager\nfrom mrjob.step import StepFailedException\nfrom mrjob.step import _is_spark_step_type\nfrom mrjob.util import cmd_line\nfrom mrjob.util import unique\nfrom mrjob.util import which\n\n\nlog = logging.getLogger(__name__)\n\n# don't look for the hadoop streaming jar here!\n_BAD_HADOOP_HOMES = ['/', '/usr', '/usr/local']\n\n# where YARN stores history logs, etc. on HDFS by default\n_DEFAULT_YARN_HDFS_LOG_DIR = 'hdfs:///tmp/hadoop-yarn/staging'\n\n# places to look for the Hadoop streaming jar if we're inside EMR\n_EMR_HADOOP_STREAMING_JAR_DIRS = [\n    # for the 2.x and 3.x AMIs (the 2.x AMIs also set $HADOOP_HOME properly)\n    '/home/hadoop/contrib',\n    # for the 4.x AMIs\n    '/usr/lib/hadoop-mapreduce',\n]\n\n# these are fairly standard places to keep Hadoop logs\n_FALLBACK_HADOOP_LOG_DIRS = [\n    '/var/log/hadoop',\n    '/mnt/var/log/hadoop',  # EMR's 2.x and 3.x AMIs use this\n]\n\n# fairly standard places to keep YARN logs (see #1339)\n_FALLBACK_HADOOP_YARN_LOG_DIRS = [\n    '/var/log/hadoop-yarn',\n    '/mnt/var/log/hadoop-yarn',\n]\n\n# start of Counters printed by Hadoop\n_HADOOP_COUNTERS_START_RE = re.compile(br'^Counters: (?P<amount>\\d+)\\s*$')\n\n# header for a group of counters\n_HADOOP_COUNTER_GROUP_RE = re.compile(br'^(?P<indent>\\s+)(?P<group>.*)$')\n\n# line for a counter\n_HADOOP_COUNTER_RE = re.compile(\n    br'^(?P<indent>\\s+)(?P<counter>.*)=(?P<amount>\\d+)\\s*$')\n\n# the one thing Hadoop streaming prints to stderr not in log format\n_HADOOP_NON_LOG_LINE_RE = re.compile(r'^Streaming Command Failed!')\n\n# if we see this from Hadoop, it actually came from stdout and shouldn't\n# be logged\n_HADOOP_STDOUT_RE = re.compile(br'^packageJobJar: ')\n\n# match the filename of a hadoop streaming jar\n_HADOOP_STREAMING_JAR_RE = re.compile(\n    r'^hadoop.*streaming.*(?<!-sources)\\.jar$')\n\n\n\n\n# The code to be completed is:\ndef fully_qualify_hdfs_path(path):\n\n    \"\"\"\n    This function takes a path as input and returns a fully qualified HDFS path. If the input path is already an \"hdfs://\" URL, it is returned as is. If the input path starts with a \"/\", it is converted into an \"hdfs://\" URL by appending \"hdfs://\" to the beginning. If the input path does not start with a \"/\", it is converted into an \"hdfs://\" URL by appending \"hdfs:///user/{username}/{path}\" where {username} is the current user's username and {path} is the input path.\n    Input-Output Arguments\n    :param path: str. The input path that needs to be converted into a fully qualified HDFS path.\n    :return: str. The fully qualified HDFS path.\n    \"\"\""}
{"namespace": "bplustree.utils.iter_slice", "prompt": "# Please complete the iter_slice function based on the contexts above the function.\n\n# The contexts above the function are:\nimport itertools\nfrom typing import Iterable\n\n\ndef pairwise(iterable: Iterable):\n    \"\"\"Iterate over elements two by two.\n\n    s -> (s0,s1), (s1,s2), (s2, s3), ...\n    \"\"\"\n    a, b = itertools.tee(iterable)\n    next(b, None)\n    return zip(a, b)\n\n\n\n\n# The code to be completed is:\ndef iter_slice(iterable: bytes, n: int):\n\n    \"\"\"\n    This function yields slices of the given size from the input iterable and indicates if each slice is the last one.\n    Input-Output Arguments\n    :param iterable: bytes. The input iterable to be sliced.\n    :param n: int. The size of each slice.\n    :return: Yields a tuple containing the slice and a boolean indicating if it is the last slice.\n    \"\"\""}
{"namespace": "boto.ec2.ec2object.TaggedEC2Object.remove_tags", "prompt": "# Please complete the remove_tags function based on the contexts above the function.\n\n# The contexts above the function are:\n# Copyright (c) 2006-2010 Mitch Garnaat http://garnaat.org/\n# Copyright (c) 2010, Eucalyptus Systems, Inc.\n#\n# Permission is hereby granted, free of charge, to any person obtaining a\n# copy of this software and associated documentation files (the\n# \"Software\"), to deal in the Software without restriction, including\n# without limitation the rights to use, copy, modify, merge, publish, dis-\n# tribute, sublicense, and/or sell copies of the Software, and to permit\n# persons to whom the Software is furnished to do so, subject to the fol-\n# lowing conditions:\n#\n# The above copyright notice and this permission notice shall be included\n# in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS\n# OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABIL-\n# ITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT\n# SHALL THE AUTHOR BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY,\n# WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS\n# IN THE SOFTWARE.\n\n\"\"\"\nRepresents an EC2 Object\n\"\"\"\nfrom boto.ec2.tag import TagSet\n\n\nclass EC2Object(object):\n\n    def __init__(self, connection=None):\n        self.connection = connection\n        if self.connection and hasattr(self.connection, 'region'):\n            self.region = connection.region\n        else:\n            self.region = None\n\n    def startElement(self, name, attrs, connection):\n        return None\n\n    def endElement(self, name, value, connection):\n        setattr(self, name, value)\n\n\nclass TaggedEC2Object(EC2Object):\n    \"\"\"\n    Any EC2 resource that can be tagged should be represented\n    by a Python object that subclasses this class.  This class\n    has the mechanism in place to handle the tagSet element in\n    the Describe* responses.  If tags are found, it will create\n    a TagSet object and allow it to parse and collect the tags\n    into a dict that is stored in the \"tags\" attribute of the\n    object.\n    \"\"\"\n\n    def __init__(self, connection=None):\n        super(TaggedEC2Object, self).__init__(connection)\n        self.tags = TagSet()\n\n    def startElement(self, name, attrs, connection):\n        if name == 'tagSet':\n            return self.tags\n        else:\n            return None\n\n    def add_tag(self, key, value='', dry_run=False):\n        \"\"\"\n        Add a tag to this object.  Tags are stored by AWS and can be used\n        to organize and filter resources.  Adding a tag involves a round-trip\n        to the EC2 service.\n\n        :type key: str\n        :param key: The key or name of the tag being stored.\n\n        :type value: str\n        :param value: An optional value that can be stored with the tag.\n                      If you want only the tag name and no value, the\n                      value should be the empty string.\n        \"\"\"\n        self.add_tags({key: value}, dry_run)\n\n    def add_tags(self, tags, dry_run=False):\n        \"\"\"\n        Add tags to this object.  Tags are stored by AWS and can be used\n        to organize and filter resources.  Adding tags involves a round-trip\n        to the EC2 service.\n\n        :type tags: dict\n        :param tags: A dictionary of key-value pairs for the tags being stored.\n                     If for some tags you want only the name and no value, the\n                     corresponding value for that tag name should be an empty\n                     string.\n        \"\"\"\n        status = self.connection.create_tags(\n            [self.id],\n            tags,\n            dry_run=dry_run\n        )\n        if self.tags is None:\n            self.tags = TagSet()\n        self.tags.update(tags)\n\n    def remove_tag(self, key, value=None, dry_run=False):\n        \"\"\"\n        Remove a tag from this object.  Removing a tag involves a round-trip\n        to the EC2 service.\n\n        :type key: str\n        :param key: The key or name of the tag being stored.\n\n        :type value: str\n        :param value: An optional value that can be stored with the tag.\n                      If a value is provided, it must match the value currently\n                      stored in EC2.  If not, the tag will not be removed.  If\n                      a value of None is provided, the tag will be\n                      unconditionally deleted.\n                      NOTE: There is an important distinction between a value\n                      of '' and a value of None.\n        \"\"\"\n        self.remove_tags({key: value}, dry_run)\n\n\n\n# The code to be completed is:\n    def remove_tags(self, tags, dry_run=False):\n\n        \"\"\"\n        This function removes tags from a TaggedEC2Object instance.. It sends a request to the EC2 service to remove the specified tags.\n        Input-Output Arguments\n        :param self: TaggedEC2Object. An instance of the TaggedEC2Object class.\n        :param tags: dict. A dictionary of key-value pairs for the tags being removed.\n        :param dry_run: bool. Whether to perform a dry run, which means the tags will not actually be removed. Defaults to False.\n        :return: None.\n        \"\"\""}
{"namespace": "boto.ec2.address.Address.release", "prompt": "# Please complete the release function based on the contexts above the function.\n\n# The contexts above the function are:\n# Copyright (c) 2006-2009 Mitch Garnaat http://garnaat.org/\n#\n# Permission is hereby granted, free of charge, to any person obtaining a\n# copy of this software and associated documentation files (the\n# \"Software\"), to deal in the Software without restriction, including\n# without limitation the rights to use, copy, modify, merge, publish, dis-\n# tribute, sublicense, and/or sell copies of the Software, and to permit\n# persons to whom the Software is furnished to do so, subject to the fol-\n# lowing conditions:\n#\n# The above copyright notice and this permission notice shall be included\n# in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS\n# OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABIL-\n# ITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT\n# SHALL THE AUTHOR BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY,\n# WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS\n# IN THE SOFTWARE.\n\n\nfrom boto.ec2.ec2object import EC2Object\n\n\nclass Address(EC2Object):\n    \"\"\"\n    Represents an EC2 Elastic IP Address\n\n    :ivar public_ip: The Elastic IP address.\n    :ivar instance_id: The instance the address is associated with (if any).\n    :ivar domain: Indicates whether the address is a EC2 address or a VPC address (standard|vpc).\n    :ivar allocation_id: The allocation ID for the address (VPC addresses only).\n    :ivar association_id: The association ID for the address (VPC addresses only).\n    :ivar network_interface_id: The network interface (if any) that the address is associated with (VPC addresses only).\n    :ivar network_interface_owner_id: The owner IID (VPC addresses only).\n    :ivar private_ip_address: The private IP address associated with the Elastic IP address (VPC addresses only).\n    \"\"\"\n\n    def __init__(self, connection=None, public_ip=None, instance_id=None):\n        super(Address, self).__init__(connection)\n        self.connection = connection\n        self.public_ip = public_ip\n        self.instance_id = instance_id\n        self.domain = None\n        self.allocation_id = None\n        self.association_id = None\n        self.network_interface_id = None\n        self.network_interface_owner_id = None\n        self.private_ip_address = None\n\n    def __repr__(self):\n        return 'Address:%s' % self.public_ip\n\n    def endElement(self, name, value, connection):\n        if name == 'publicIp':\n            self.public_ip = value\n        elif name == 'instanceId':\n            self.instance_id = value\n        elif name == 'domain':\n            self.domain = value\n        elif name == 'allocationId':\n            self.allocation_id = value\n        elif name == 'associationId':\n            self.association_id = value\n        elif name == 'networkInterfaceId':\n            self.network_interface_id = value\n        elif name == 'networkInterfaceOwnerId':\n            self.network_interface_owner_id = value\n        elif name == 'privateIpAddress':\n            self.private_ip_address = value\n        else:\n            setattr(self, name, value)\n\n\n\n# The code to be completed is:\n    def release(self, dry_run=False):\n\n        \"\"\"\n        Free up this Elastic IP address. If the address has an allocation ID, it releases the address using the allocation ID. Otherwise, it releases the address using the public IP.\n        Input-Output Arguments\n        :param self: Address. An instance of the Address class.\n        :param dry_run: Bool. Whether to perform a dry run (no changes are made). Defaults to False.\n        :return: The result of the release operation.\n        \"\"\""}
{"namespace": "praw.util.token_manager.FileTokenManager.post_refresh_callback", "prompt": "# Please complete the post_refresh_callback function based on the contexts above the function.\n\n# The contexts above the function are:\n\"\"\"Token Manager classes.\n\nThere should be a 1-to-1 mapping between an instance of a subclass of\n:class:`.BaseTokenManager` and a :class:`.Reddit` instance.\n\nA few proof of concept token manager classes are provided here, but it is expected that\nPRAW users will create their own token manager classes suitable for their needs.\n\n.. deprecated:: 7.4.0\n\n    Tokens managers have been deprecated and will be removed in the near future.\n\n\"\"\"\nfrom abc import ABC, abstractmethod\n\nfrom . import _deprecate_args\n\n\nclass BaseTokenManager(ABC):\n    \"\"\"An abstract class for all token managers.\"\"\"\n\n    @abstractmethod\n    def post_refresh_callback(self, authorizer):\n        \"\"\"Handle callback that is invoked after a refresh token is used.\n\n        :param authorizer: The ``prawcore.Authorizer`` instance used containing\n            ``access_token`` and ``refresh_token`` attributes.\n\n        This function will be called after refreshing the access and refresh tokens.\n        This callback can be used for saving the updated ``refresh_token``.\n\n        \"\"\"\n\n    @abstractmethod\n    def pre_refresh_callback(self, authorizer):\n        \"\"\"Handle callback that is invoked before refreshing PRAW's authorization.\n\n        :param authorizer: The ``prawcore.Authorizer`` instance used containing\n            ``access_token`` and ``refresh_token`` attributes.\n\n        This callback can be used to inspect and modify the attributes of the\n        ``prawcore.Authorizer`` instance, such as setting the ``refresh_token``.\n\n        \"\"\"\n\n    @property\n    def reddit(self):\n        \"\"\"Return the :class:`.Reddit` instance bound to the token manager.\"\"\"\n        return self._reddit\n\n    @reddit.setter\n    def reddit(self, value):\n        if self._reddit is not None:\n            raise RuntimeError(\n                \"'reddit' can only be set once and is done automatically\"\n            )\n        self._reddit = value\n\n    def __init__(self):\n        \"\"\"Initialize a :class:`.BaseTokenManager` instance.\"\"\"\n        self._reddit = None\n\n\nclass FileTokenManager(BaseTokenManager):\n    \"\"\"Provides a single-file based token manager.\n\n    It is expected that the file with the initial ``refresh_token`` is created prior to\n    use.\n\n    .. warning::\n\n        The same ``file`` should not be used by more than one instance of this class\n        concurrently. Doing so may result in data corruption. Consider using\n        :class:`.SQLiteTokenManager` if you want more than one instance of PRAW to\n        concurrently manage a specific ``refresh_token`` chain.\n\n    \"\"\"\n\n    def __init__(self, filename):\n        \"\"\"Initialize a :class:`.FileTokenManager` instance.\n\n        :param filename: The file the contains the refresh token.\n\n        \"\"\"\n        super().__init__()\n        self._filename = filename\n\n\n\n# The code to be completed is:\n    def post_refresh_callback(self, authorizer):\n\n        \"\"\"\n        This function updates the saved copy of the refresh token by writing it to the file of the instance.\n        Input-Output Arguments\n        :param self: FileTokenManager. An instance of the FileTokenManager class.\n        :param authorizer: The authorizer object containing the refresh token.\n        :return: No return values.\n        \"\"\""}
{"namespace": "pyramid.registry.Introspector.get", "prompt": "# Please complete the get function based on the contexts above the function.\n\n# The contexts above the function are:\nimport operator\nimport threading\nfrom zope.interface import implementer\nfrom zope.interface.registry import Components\n\nfrom pyramid.decorator import reify\nfrom pyramid.interfaces import IIntrospectable, IIntrospector, ISettings\nfrom pyramid.path import CALLER_PACKAGE, caller_package\n\n\nclass Registry(Components, dict):\n    \"\"\"A registry object is an :term:`application registry`.\n\n    It is used by the framework itself to perform mappings of URLs to view\n    callables, as well as servicing other various framework duties. A registry\n    has its own internal API, but this API is rarely used by Pyramid\n    application developers (it's usually only used by developers of the\n    Pyramid framework and Pyramid addons).  But it has a number of attributes\n    that may be useful to application developers within application code,\n    such as ``settings``, which is a dictionary containing application\n    deployment settings.\n\n    For information about the purpose and usage of the application registry,\n    see :ref:`zca_chapter`.\n\n    The registry may be used both as an :class:`pyramid.interfaces.IDict` and\n    as a Zope component registry.\n    These two ways of storing configuration are independent.\n    Applications will tend to prefer to store information as key-values\n    whereas addons may prefer to use the component registry to avoid naming\n    conflicts and to provide more complex lookup mechanisms.\n\n    The application registry is usually accessed as ``request.registry`` in\n    application code. By the time a registry is used to handle requests it\n    should be considered frozen and read-only. Any changes to its internal\n    state should be done with caution and concern for thread-safety.\n\n    \"\"\"\n\n    # for optimization purposes, if no listeners are listening, don't try\n    # to notify them\n    has_listeners = False\n\n    _settings = None\n\n    def __init__(self, package_name=CALLER_PACKAGE, *args, **kw):\n        # add a registry-instance-specific lock, which is used when the lookup\n        # cache is mutated\n        self._lock = threading.Lock()\n        # add a view lookup cache\n        self._clear_view_lookup_cache()\n        if package_name is CALLER_PACKAGE:\n            package_name = caller_package().__name__\n        Components.__init__(self, package_name, *args, **kw)\n        dict.__init__(self)\n\n    def _clear_view_lookup_cache(self):\n        self._view_lookup_cache = {}\n\n    def __bool__(self):\n        # defeat bool determination via dict.__len__\n        return True\n\n    @reify\n    def package_name(self):\n        return self.__name__\n\n    def registerSubscriptionAdapter(self, *arg, **kw):\n        result = Components.registerSubscriptionAdapter(self, *arg, **kw)\n        self.has_listeners = True\n        return result\n\n    def registerSelfAdapter(\n        self, required=None, provided=None, name='', info='', event=True\n    ):\n        # registerAdapter analogue which always returns the object itself\n        # when required is matched\n        return self.registerAdapter(\n            lambda x: x,\n            required=required,\n            provided=provided,\n            name=name,\n            info=info,\n            event=event,\n        )\n\n    def queryAdapterOrSelf(self, object, interface, default=None):\n        # queryAdapter analogue which returns the object if it implements\n        # the interface, otherwise it will return an adaptation to the\n        # interface\n        if not interface.providedBy(object):\n            return self.queryAdapter(object, interface, default=default)\n        return object\n\n    def registerHandler(self, *arg, **kw):\n        result = Components.registerHandler(self, *arg, **kw)\n        self.has_listeners = True\n        return result\n\n    def notify(self, *events):\n        if self.has_listeners:\n            # iterating over subscribers assures they get executed\n            [_ for _ in self.subscribers(events, None)]\n\n    # backwards compatibility for code that wants to look up a settings\n    # object via ``registry.getUtility(ISettings)``\n    def _get_settings(self):\n        return self._settings\n\n    def _set_settings(self, settings):\n        self.registerUtility(settings, ISettings)\n        self._settings = settings\n\n    settings = property(_get_settings, _set_settings)\n\n\n@implementer(IIntrospector)\nclass Introspector:\n    def __init__(self):\n        self._refs = {}\n        self._categories = {}\n        self._counter = 0\n\n    def add(self, intr):\n        category = self._categories.setdefault(intr.category_name, {})\n        category[intr.discriminator] = intr\n        category[intr.discriminator_hash] = intr\n        intr.order = self._counter\n        self._counter += 1\n\n\n\n# The code to be completed is:\n    def get(self, category_name, discriminator, default=None):\n\n        \"\"\"\n        This function retrieves an item from the Introspector instance based on the given category name and discriminator. If the item is not found, it returns the default value.\n        Input-Output Arguments\n        :param self: Introspector. An instance of the Introspector class.\n        :param category_name: str. The name of the category to retrieve the item from.\n        :param discriminator: The discriminator of the item to retrieve.\n        :param default: Any data type. The value to return if the item is not found. Defaults to None.\n        :return: Any data type. The retrieved item or the default value if the item is not found.\n        \"\"\""}
{"namespace": "mopidy.internal.xdg.get_dirs", "prompt": "# Please complete the get_dirs function based on the contexts above the function.\n\n# The contexts above the function are:\nimport configparser\nimport os\nimport pathlib\n\n\n\n\n# The code to be completed is:\ndef get_dirs():\n\n    \"\"\"\n    This function returns a dictionary containing all the known XDG Base Directories for the current user. It retrieves the values of the environment variables related to XDG Base Directories and expands the paths using `pathlib.Path.expanduser()`. It also updates the dictionary with additional directories if the `user-dirs.dirs` file exists and is parseable.\n    Input-Output Arguments\n    :param: No input parameters.\n    :return: dict. A dictionary containing the XDG Base Directories for the current user. The keys are the names of the directories (e.g., \"XDG_CACHE_DIR\", \"XDG_CONFIG_DIR\") and the values are `pathlib.Path` objects representing the expanded paths.\n    \"\"\""}
{"namespace": "chatette.utils.sample_indulgent", "prompt": "# Please complete the sample_indulgent function based on the contexts above the function.\n\n# The contexts above the function are:\n#!/usr/bin/env python3\n# coding: utf-8\n\"\"\"\nModule `chatette.utils`\nContains utility functions and classes used everywhere in the project.\n\"\"\"\n\nfrom __future__ import print_function\nimport sys\nfrom random import sample, choice\nfrom copy import deepcopy\n\nfrom string import ascii_letters\n\nfrom enum import Enum\n\n\nclass UnitType(Enum):\n    alias = \"alias\"\n    slot = \"slot\"\n    intent = \"intent\"\n\n\nclass Singleton(object):\n    \"\"\"\n    The base class for all singleton objects.\n    Every class that subclasses this class will have the behavior\n    of a singleton: their constructor will always return the same instance.\n    @pre: In order to work, a sub-class needs to have an `_instance` class\n          variable.\n    \"\"\"\n    _instance = None\n    def __new__(cls, *args, **kwargs):\n        if cls._instance is None:\n            cls._instance = super(Singleton, cls).__new__(cls)\n        return cls._instance\n\n    @classmethod\n    def get_or_create(cls, *args, **kwargs):\n        if cls._instance is None:\n            cls._instance = cls(*args, **kwargs)\n        return cls._instance\n\n    @classmethod\n    def reset_instance(cls, *args, **kwargs):\n        \"\"\"\n        Completely resets the instance of the class\n        (representing the singleton), makes a new one and\n        returns this instance.\n        \"\"\"\n        cls._instance = None\n        cls._instance = cls(*args, **kwargs)\n        return cls._instance\n\n    @classmethod\n    def was_instantiated(cls):\n        return (cls._instance is not None)\n\n\ndef cast_to_unicode(anything):\n    \"\"\"\n    If executed with Python 2.7, cast any string that is in `anything`\n    to unicode.\n    If executed with Python 3, returns `anything`.\n    `anything` can be a string, an array, a dict,...\n    \"\"\"\n    if sys.version_info[0] == 3:\n        return anything\n\n    if isinstance(anything, str):\n        return unicode(anything, \"utf-8\")\n    if isinstance(anything, dict):\n        cast_dict = dict()\n        for key in anything:\n            cast_key = cast_to_unicode(key)\n            cast_value = cast_to_unicode(anything[key])\n            cast_dict[cast_key] = cast_value\n        return cast_dict\n    if isinstance(anything, list):\n        cast_list = []\n        for e in anything:\n            cast_list.append(cast_to_unicode(e))\n        return cast_list\n    return anything\n\n\n\n\n# The code to be completed is:\ndef sample_indulgent(array, nb_items):\n\n    \"\"\"\n    This function is similar to the random.sample function but does not raise an error if the number of items to be sampled is larger than the length of the array. In that case, it simply returns a copy of the whole array.\n    Input-Output Arguments\n    :param array: List. The input array from which items are to be sampled.\n    :param nb_items: Integer. The number of items to be sampled from the array.\n    :return: List. The sampled items from the array or a copy of the whole array if nb_items is larger than the length of the array.\n    \"\"\""}
{"namespace": "mingus.core.notes.is_valid_note", "prompt": "# Please complete the is_valid_note function based on the contexts above the function.\n\n# The contexts above the function are:\n# -*- coding: utf-8 -*-\n\n#    mingus - Music theory Python package, notes module.\n#    Copyright (C) 2008-2009, Bart Spaans\n#    Copyright (C) 2011, Carlo Stemberger\n#\n#    This program is free software: you can redistribute it and/or modify\n#    it under the terms of the GNU General Public License as published by\n#    the Free Software Foundation, either version 3 of the License, or\n#    (at your option) any later version.\n#\n#    This program is distributed in the hope that it will be useful,\n#    but WITHOUT ANY WARRANTY; without even the implied warranty of\n#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#    GNU General Public License for more details.\n#\n#    You should have received a copy of the GNU General Public License\n#    along with this program.  If not, see <http://www.gnu.org/licenses/>.\n\n\"\"\"Basic module for notes.\n\nThis module is the foundation of the music theory package.\n\nIt handles conversions from integers to notes and vice versa and thus\nenables simple calculations.\n\"\"\"\nfrom __future__ import absolute_import\n\nfrom mingus.core.mt_exceptions import NoteFormatError\nfrom six.moves import range\n\n_note_dict = {\"C\": 0, \"D\": 2, \"E\": 4, \"F\": 5, \"G\": 7, \"A\": 9, \"B\": 11}\nfifths = [\"F\", \"C\", \"G\", \"D\", \"A\", \"E\", \"B\"]\n\n\ndef int_to_note(note_int, accidentals=\"#\"):\n    \"\"\"Convert integers in the range of 0-11 to notes in the form of C or C#\n    or Db.\n\n    Throw a RangeError exception if the note_int is not in the range 0-11.\n\n    If not specified, sharps will be used.\n\n    Examples:\n    >>> int_to_note(0)\n    'C'\n    >>> int_to_note(3)\n    'D#'\n    >>> int_to_note(3, 'b')\n    'Eb'\n    \"\"\"\n    from mingus.core.mt_exceptions import RangeError\n    from mingus.core.mt_exceptions import FormatError\n    if note_int not in range(12):\n        raise RangeError(\"int out of bounds (0-11): %d\" % note_int)\n    ns = [\"C\", \"C#\", \"D\", \"D#\", \"E\", \"F\", \"F#\", \"G\", \"G#\", \"A\", \"A#\", \"B\"]\n    nf = [\"C\", \"Db\", \"D\", \"Eb\", \"E\", \"F\", \"Gb\", \"G\", \"Ab\", \"A\", \"Bb\", \"B\"]\n    if accidentals == \"#\":\n        return ns[note_int]\n    elif accidentals == \"b\":\n        return nf[note_int]\n    else:\n        raise FormatError(\"'%s' not valid as accidental\" % accidentals)\n\n\ndef is_enharmonic(note1, note2):\n    \"\"\"Test whether note1 and note2 are enharmonic, i.e. they sound the same.\"\"\"\n    return note_to_int(note1) == note_to_int(note2)\n\n\n\n\n# The code to be completed is:\ndef is_valid_note(note):\n\n    \"\"\"\n    This function checks if a given note is in a recognized format. It returns True if the note is in the recognized format, and False otherwise.\n\n    Input-Output Arguments\n    :param note: str. The note to be checked for recognition.\n    :return: bool. Returns True if the note is in a recognized format, and False otherwise.\n\n    \"\"\""}
{"namespace": "rows.fields.Field.serialize", "prompt": "# Please complete the serialize function based on the contexts above the function.\n\n# The contexts above the function are:\n# coding: utf-8\n\n# Copyright 2014-2019 lvaro Justen <https://github.com/turicas/rows/>\n\n#    This program is free software: you can redistribute it and/or modify\n#    it under the terms of the GNU Lesser General Public License as published by\n#    the Free Software Foundation, either version 3 of the License, or\n#    (at your option) any later version.\n\n#    This program is distributed in the hope that it will be useful,\n#    but WITHOUT ANY WARRANTY; without even the implied warranty of\n#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#    GNU Lesser General Public License for more details.\n\n#    You should have received a copy of the GNU Lesser General Public License\n#    along with this program.  If not, see <http://www.gnu.org/licenses/>.\n\nfrom __future__ import unicode_literals\n\nimport binascii\nimport datetime\nimport json\nimport locale\nimport re\nfrom base64 import b64decode, b64encode\nfrom collections import OrderedDict, defaultdict\nfrom decimal import Decimal, InvalidOperation\nfrom unicodedata import normalize\n\nimport six\n\nif six.PY2:\n    from itertools import izip_longest as zip_longest\nelse:\n    from itertools import zip_longest\n\n\n# Order matters here\n__all__ = [\n    \"BoolField\",\n    \"IntegerField\",\n    \"FloatField\",\n    \"DatetimeField\",\n    \"DateField\",\n    \"DecimalField\",\n    \"PercentField\",\n    \"JSONField\",\n    \"EmailField\",\n    \"TextField\",\n    \"BinaryField\",\n    \"Field\",\n]\nNULL = (\"-\", \"null\", \"none\", \"nil\", \"n/a\", \"na\")\nNULL_BYTES = (b\"-\", b\"null\", b\"none\", b\"nil\", b\"n/a\", b\"na\")\nREGEXP_ONLY_NUMBERS = re.compile(\"[^0-9\\-]\")\nSHOULD_NOT_USE_LOCALE = True  # This variable is changed by rows.locale_manager\nSLUG_CHARS = \"ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789_\"\n\n\ndef value_error(value, cls):\n    value = repr(value)\n    if len(value) > 50:\n        value = value[:50] + \"...\"\n    raise ValueError(\"Value '{}' can't be {}\".format(value, cls.__name__))\n\n\nclass Field(object):\n    \"\"\"Base Field class - all fields should inherit from this\n\n    As the fallback for all other field types are the BinaryField, this Field\n    actually implements what is expected in the BinaryField\n    \"\"\"\n\n    TYPE = (type(None),)\n\n    @classmethod\n\n\n# The code to be completed is:\n    def serialize(cls, value, *args, **kwargs):\n\n        \"\"\"\n        This function serializes a value to be exported. It should always return a unicode value, except for BinaryField.\n        Input-Output Arguments\n        :param cls: Class. The class instance.\n        :param value: Any. The value to be serialized.\n        :param *args: Tuple. Additional positional arguments.\n        :param **kwargs: Dictionary. Additional keyword arguments.\n        :return: Any. The serialized value.\n        \"\"\""}
{"namespace": "praw.util.token_manager.SQLiteTokenManager._get", "prompt": "# Please complete the _get function based on the contexts above the function.\n\n# The contexts above the function are:\n\"\"\"Token Manager classes.\n\nThere should be a 1-to-1 mapping between an instance of a subclass of\n:class:`.BaseTokenManager` and a :class:`.Reddit` instance.\n\nA few proof of concept token manager classes are provided here, but it is expected that\nPRAW users will create their own token manager classes suitable for their needs.\n\n.. deprecated:: 7.4.0\n\n    Tokens managers have been deprecated and will be removed in the near future.\n\n\"\"\"\nfrom abc import ABC, abstractmethod\n\nfrom . import _deprecate_args\n\n\nclass BaseTokenManager(ABC):\n    \"\"\"An abstract class for all token managers.\"\"\"\n\n    @abstractmethod\n    def post_refresh_callback(self, authorizer):\n        \"\"\"Handle callback that is invoked after a refresh token is used.\n\n        :param authorizer: The ``prawcore.Authorizer`` instance used containing\n            ``access_token`` and ``refresh_token`` attributes.\n\n        This function will be called after refreshing the access and refresh tokens.\n        This callback can be used for saving the updated ``refresh_token``.\n\n        \"\"\"\n\n    @abstractmethod\n    def pre_refresh_callback(self, authorizer):\n        \"\"\"Handle callback that is invoked before refreshing PRAW's authorization.\n\n        :param authorizer: The ``prawcore.Authorizer`` instance used containing\n            ``access_token`` and ``refresh_token`` attributes.\n\n        This callback can be used to inspect and modify the attributes of the\n        ``prawcore.Authorizer`` instance, such as setting the ``refresh_token``.\n\n        \"\"\"\n\n    @property\n    def reddit(self):\n        \"\"\"Return the :class:`.Reddit` instance bound to the token manager.\"\"\"\n        return self._reddit\n\n    @reddit.setter\n    def reddit(self, value):\n        if self._reddit is not None:\n            raise RuntimeError(\n                \"'reddit' can only be set once and is done automatically\"\n            )\n        self._reddit = value\n\n    def __init__(self):\n        \"\"\"Initialize a :class:`.BaseTokenManager` instance.\"\"\"\n        self._reddit = None\n\n\nclass FileTokenManager(BaseTokenManager):\n    \"\"\"Provides a single-file based token manager.\n\n    It is expected that the file with the initial ``refresh_token`` is created prior to\n    use.\n\n    .. warning::\n\n        The same ``file`` should not be used by more than one instance of this class\n        concurrently. Doing so may result in data corruption. Consider using\n        :class:`.SQLiteTokenManager` if you want more than one instance of PRAW to\n        concurrently manage a specific ``refresh_token`` chain.\n\n    \"\"\"\n\n    def __init__(self, filename):\n        \"\"\"Initialize a :class:`.FileTokenManager` instance.\n\n        :param filename: The file the contains the refresh token.\n\n        \"\"\"\n        super().__init__()\n        self._filename = filename\n\n    def post_refresh_callback(self, authorizer):\n        \"\"\"Update the saved copy of the refresh token.\"\"\"\n        with open(self._filename, \"w\") as fp:\n            fp.write(authorizer.refresh_token)\n\n    def pre_refresh_callback(self, authorizer):\n        \"\"\"Load the refresh token from the file.\"\"\"\n        if authorizer.refresh_token is None:\n            with open(self._filename) as fp:\n                authorizer.refresh_token = fp.read().strip()\n\n\nclass SQLiteTokenManager(BaseTokenManager):\n    \"\"\"Provides a SQLite3 based token manager.\n\n    Unlike, :class:`.FileTokenManager`, the initial database need not be created ahead\n    of time, as it'll automatically be created on first use. However, initial refresh\n    tokens will need to be registered via :meth:`.register` prior to use.\n\n    .. warning::\n\n        This class is untested on Windows because we encountered file locking issues in\n        the test environment.\n\n    \"\"\"\n\n    @_deprecate_args(\"database\", \"key\")\n    def __init__(self, *, database, key):\n        \"\"\"Initialize a :class:`.SQLiteTokenManager` instance.\n\n        :param database: The path to the SQLite database.\n        :param key: The key used to locate the refresh token. This ``key`` can be\n            anything. You might use the ``client_id`` if you expect to have unique a\n            refresh token for each ``client_id``, or you might use a redditor's\n            ``username`` if you're managing multiple users' authentications.\n\n        \"\"\"\n        super().__init__()\n        import sqlite3\n\n        self._connection = sqlite3.connect(database)\n        self._connection.execute(\n            \"CREATE TABLE IF NOT EXISTS tokens (id, refresh_token, updated_at)\"\n        )\n        self._connection.execute(\n            \"CREATE UNIQUE INDEX IF NOT EXISTS ux_tokens_id on tokens(id)\"\n        )\n        self._connection.commit()\n        self.key = key\n\n\n\n# The code to be completed is:\n    def _get(self):\n\n        \"\"\"\n        This function retrieves the refresh token from the SQLite database based on the provided key. It executes a SQL query to fetch the refresh token from the \"tokens\" table using the given key. If the result is None, it raises a KeyError. Otherwise, it returns the first refresh token.\n        Input-Output Arguments\n        :param self: SQLiteTokenManager. An instance of the SQLiteTokenManager class.\n        :return: String. The refresh token retrieved from the database.\n        \"\"\""}
{"namespace": "sacred.experiment.Experiment.main", "prompt": "# Please complete the main function based on the contexts above the function.\n\n# The contexts above the function are:\n\"\"\"The Experiment class, which is central to sacred.\"\"\"\nimport inspect\nimport os.path\nimport sys\nimport warnings\nfrom collections import OrderedDict\nfrom typing import Sequence, Optional, List\n\nfrom docopt import docopt, printable_usage\n\nfrom sacred import SETTINGS\nfrom sacred.arg_parser import format_usage, get_config_updates\nfrom sacred import commandline_options\nfrom sacred.commandline_options import CLIOption\nfrom sacred.commands import (\n    help_for_command,\n    print_config,\n    print_dependencies,\n    save_config,\n    print_named_configs,\n)\nfrom sacred.observers.file_storage import file_storage_option\nfrom sacred.observers.s3_observer import s3_option\nfrom sacred.config.signature import Signature\nfrom sacred.ingredient import Ingredient\nfrom sacred.initialize import create_run\nfrom sacred.observers.sql import sql_option\nfrom sacred.observers.tinydb_hashfs import tiny_db_option\nfrom sacred.run import Run\nfrom sacred.host_info import check_additional_host_info, HostInfoGetter\nfrom sacred.utils import (\n    print_filtered_stacktrace,\n    ensure_wellformed_argv,\n    SacredError,\n    format_sacred_error,\n    PathType,\n    get_inheritors,\n)\nfrom sacred.observers.mongo import mongo_db_option\n\n__all__ = (\"Experiment\",)\n\n\nclass Experiment(Ingredient):\n    \"\"\"\n    The central class for each experiment in Sacred.\n\n    It manages the configuration, the main function, captured methods,\n    observers, commands, and further ingredients.\n\n    An Experiment instance should be created as one of the first\n    things in any experiment-file.\n    \"\"\"\n\n    def __init__(\n        self,\n        name: Optional[str] = None,\n        ingredients: Sequence[Ingredient] = (),\n        interactive: bool = False,\n        base_dir: Optional[PathType] = None,\n        additional_host_info: Optional[List[HostInfoGetter]] = None,\n        additional_cli_options: Optional[Sequence[CLIOption]] = None,\n        save_git_info: bool = True,\n    ):\n        \"\"\"\n        Create a new experiment with the given name and optional ingredients.\n\n        Parameters\n        ----------\n        name\n            Optional name of this experiment, defaults to the filename.\n            (Required in interactive mode)\n\n        ingredients : list[sacred.Ingredient], optional\n            A list of ingredients to be used with this experiment.\n\n        interactive\n            If set to True will allow the experiment to be run in interactive\n            mode (e.g. IPython or Jupyter notebooks).\n            However, this mode is discouraged since it won't allow storing the\n            source-code or reliable reproduction of the runs.\n\n        base_dir\n            Optional full path to the base directory of this experiment. This\n            will set the scope for automatic source file discovery.\n\n        additional_host_info\n            Optional dictionary containing as keys the names of the pieces of\n            host info you want to collect, and as\n            values the functions collecting those pieces of information.\n\n        save_git_info:\n            Optionally save the git commit hash and the git state\n            (clean or dirty) for all source files. This requires the GitPython\n            package.\n        \"\"\"\n        self.additional_host_info = additional_host_info or []\n        check_additional_host_info(self.additional_host_info)\n        self.additional_cli_options = additional_cli_options or []\n        self.all_cli_options = (\n            gather_command_line_options() + self.additional_cli_options\n        )\n        caller_globals = inspect.stack()[1][0].f_globals\n        if name is None:\n            if interactive:\n                raise RuntimeError(\"name is required in interactive mode.\")\n            mainfile = caller_globals.get(\"__file__\")\n            if mainfile is None:\n                raise RuntimeError(\n                    \"No main-file found. Are you running in \"\n                    \"interactive mode? If so please provide a \"\n                    \"name and set interactive=True.\"\n                )\n            name = os.path.basename(mainfile)\n            if name.endswith(\".py\"):\n                name = name[:-3]\n            elif name.endswith(\".pyc\"):\n                name = name[:-4]\n        super().__init__(\n            path=name,\n            ingredients=ingredients,\n            interactive=interactive,\n            base_dir=base_dir,\n            _caller_globals=caller_globals,\n            save_git_info=save_git_info,\n        )\n        self.default_command = None\n        self.command(print_config, unobserved=True)\n        self.command(print_dependencies, unobserved=True)\n        self.command(save_config, unobserved=True)\n        self.command(print_named_configs(self), unobserved=True)\n        self.observers = []\n        self.current_run = None\n        self.captured_out_filter = None\n        \"\"\"Filter function to be applied to captured output of a run\"\"\"\n        self.option_hooks = []\n\n    # =========================== Decorators ==================================\n\n\n\n# The code to be completed is:\n    def main(self, function):\n\n        \"\"\"\n        This function is a decorator that is used to define the main function of an experiment. The main function is the default command that is executed when no command is specified or when calling the run() method. It captures the decorated function and sets it as the default command for the experiment.\n        Input-Output Arguments\n        :param self: Experiment. An instance of the Experiment class.\n        :param function: The function to be decorated and set as the main function.\n        :return: The captured function that is set as the default command.\n        \"\"\""}
{"namespace": "fs.wildcard.match_any", "prompt": "# Please complete the match_any function based on the contexts above the function.\n\n# The contexts above the function are:\n\"\"\"Match wildcard filenames.\n\"\"\"\n# Adapted from https://hg.python.org/cpython/file/2.7/Lib/fnmatch.py\n\nfrom __future__ import print_function, unicode_literals\n\nimport typing\n\nimport re\nfrom functools import partial\n\nfrom .lrucache import LRUCache\n\nif typing.TYPE_CHECKING:\n    from typing import Callable, Iterable, Pattern, Text, Tuple\n\n\n_PATTERN_CACHE = LRUCache(1000)  # type: LRUCache[Tuple[Text, bool], Pattern]\n\n\ndef match(pattern, name):\n    # type: (Text, Text) -> bool\n    \"\"\"Test whether a name matches a wildcard pattern.\n\n    Arguments:\n        pattern (str): A wildcard pattern, e.g. ``\"*.py\"``.\n        name (str): A filename.\n\n    Returns:\n        bool: `True` if the filename matches the pattern.\n\n    \"\"\"\n    try:\n        re_pat = _PATTERN_CACHE[(pattern, True)]\n    except KeyError:\n        res = \"(?ms)\" + _translate(pattern) + r\"\\Z\"\n        _PATTERN_CACHE[(pattern, True)] = re_pat = re.compile(res)\n    return re_pat.match(name) is not None\n\n\ndef imatch(pattern, name):\n    # type: (Text, Text) -> bool\n    \"\"\"Test whether a name matches a wildcard pattern (case insensitive).\n\n    Arguments:\n        pattern (str): A wildcard pattern, e.g. ``\"*.py\"``.\n        name (bool): A filename.\n\n    Returns:\n        bool: `True` if the filename matches the pattern.\n\n    \"\"\"\n    try:\n        re_pat = _PATTERN_CACHE[(pattern, False)]\n    except KeyError:\n        res = \"(?ms)\" + _translate(pattern, case_sensitive=False) + r\"\\Z\"\n        _PATTERN_CACHE[(pattern, False)] = re_pat = re.compile(res, re.IGNORECASE)\n    return re_pat.match(name) is not None\n\n\n\n\n# The code to be completed is:\ndef match_any(patterns, name):\n    # type: (Iterable[Text], Text) -> bool\n\n    \"\"\"\n    This function tests if a name matches any of a list of patterns. It returns True if the patterns list is empty.\n    Input-Output Arguments\n    :param patterns: Iterable of Text. A list of wildcard patterns, e.g., [\"*.py\", \"*.pyc\"].\n    :param name: Text. A filename.\n    :return: bool. True if the name matches at least one of the patterns.\n    \"\"\""}
{"namespace": "faker.utils.loading.find_available_locales", "prompt": "# Please complete the find_available_locales function based on the contexts above the function.\n\n# The contexts above the function are:\nimport pkgutil\nimport sys\n\nfrom importlib import import_module\nfrom pathlib import Path\nfrom types import ModuleType\nfrom typing import List\n\n\ndef get_path(module: ModuleType) -> str:\n    if getattr(sys, \"frozen\", False):\n        # frozen\n\n        if getattr(sys, \"_MEIPASS\", False):\n            # PyInstaller\n            lib_dir = Path(getattr(sys, \"_MEIPASS\"))\n        else:\n            # others\n            lib_dir = Path(sys.executable).parent / \"lib\"\n\n        path = lib_dir.joinpath(*module.__package__.split(\".\"))  # type: ignore\n    else:\n        # unfrozen\n        if module.__file__ is not None:\n            path = Path(module.__file__).parent\n        else:\n            raise RuntimeError(f\"Can't find path from module `{module}.\")\n    return str(path)\n\n\ndef list_module(module: ModuleType) -> List[str]:\n    path = get_path(module)\n\n    if getattr(sys, \"_MEIPASS\", False):\n        # PyInstaller\n        return [file.parent.name for file in Path(path).glob(\"*/__init__.py\")]\n    else:\n        return [name for _, name, is_pkg in pkgutil.iter_modules([str(path)]) if is_pkg]\n\n\n\n\n# The code to be completed is:\ndef find_available_locales(providers: List[str]) -> List[str]:\n\n    \"\"\"\n    This function finds and returns a list of available locales based on the given list of providers. It iterates through each provider, imports the provider module, checks if it is localized, and retrieves the list of languages from the module. The available locales are then updated with the languages found and returned in sorted order.\n    Input-Output Arguments\n    :param providers: List of strings. A list of provider paths.\n    :return: List of strings. A sorted list of available locales.\n    \"\"\""}
{"namespace": "pycoin.satoshi.IntStreamer.IntStreamer.int_from_script_bytes", "prompt": "# Please complete the int_from_script_bytes function based on the contexts above the function.\n\n# The contexts above the function are:\n\n\n\n\n\nclass IntStreamer(object):\n\n    @classmethod\n\n\n# The code to be completed is:\n    def int_from_script_bytes(class_, s, require_minimal=False):\n\n        \"\"\"\n        This function converts a byte array into an integer value. It first checks if the byte array is empty, and if so, returns 0. Then it reverses the byte array and extracts the first byte. It extracts the value from the first byte by performing a bitwise AND operation with 0x7f. If the \"require_minimal\" parameter is set to True, it checks if the value is 0 and if the byte array is non-minimally encoded. If so, it raises a ScriptError. It then checks if the first byte has the sign bit set, indicating a negative value. It iterates over the remaining bytes in the byte array, left-shifting the value by 8 bits and adding the current byte. If the value is negative, it negates it. Finally, it returns the resulting integer value.\n        Input-Output Arguments\n        :param class_: The class object. It is not used in the function.\n        :param s: The byte array to convert into an integer.\n        :param require_minimal: Bool. Whether to check for minimal encoding. Defaults to False.\n        :return: The converted integer value.\n        \"\"\""}
{"namespace": "zulipterminal.platform_code.successful_GUI_return_code", "prompt": "# Please complete the successful_GUI_return_code function based on the contexts above the function.\n\n# The contexts above the function are:\nimport platform\nimport subprocess\n\nfrom typing_extensions import Literal\n\n\n# PLATFORM DETECTION\nSupportedPlatforms = Literal[\"Linux\", \"MacOS\", \"WSL\"]\nAllPlatforms = Literal[SupportedPlatforms, \"unsupported\"]\n\nraw_platform = platform.system()\n\nPLATFORM: AllPlatforms\n\nif raw_platform == \"Linux\":\n    PLATFORM = \"WSL\" if \"microsoft\" in platform.release().lower() else \"Linux\"\nelif raw_platform == \"Darwin\":\n    PLATFORM = \"MacOS\"\nelse:\n    PLATFORM = \"unsupported\"\n\n\n# PLATFORM DEPENDENT HELPERS\nMOUSE_SELECTION_KEY = \"Fn + Alt\" if PLATFORM == \"MacOS\" else \"Shift\"\n\n\ndef notify(title: str, text: str) -> str:\n    command_list = None\n    if PLATFORM == \"MacOS\":\n        command_list = [\n            \"osascript\",\n            \"-e\",\n            \"on run(argv)\",\n            \"-e\",\n            \"return display notification item 1 of argv with title \"\n            'item 2 of argv sound name \"ZT_NOTIFICATION_SOUND\"',\n            \"-e\",\n            \"end\",\n            \"--\",\n            text,\n            title,\n        ]\n    elif PLATFORM == \"Linux\":\n        command_list = [\"notify-send\", \"--\", title, text]\n\n    if command_list is not None:\n        try:\n            subprocess.run(\n                command_list, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL\n            )\n        except FileNotFoundError:\n            # This likely means the notification command could not be found\n            return command_list[0]\n    return \"\"\n\n\n\n\n# The code to be completed is:\ndef successful_GUI_return_code() -> int:\n\n    \"\"\"\n    This function returns the success return code for GUI commands, which can be OS specific. If the platform is Windows Subsystem for Linux (WSL), it returns 1. Otherwise, it returns 0.\n    Input-Output Arguments\n    :param: No input parameters.\n    :return: int. The success return code for GUI commands.\n    \"\"\""}
{"namespace": "pyt.core.project_handler.get_directory_modules", "prompt": "# Please complete the get_directory_modules function based on the contexts above the function.\n\n# The contexts above the function are:\n\"\"\"Generates a list of CFGs from a path.\n\nThe module finds all python modules and generates an ast for them.\n\"\"\"\nimport os\n\n\n_local_modules = list()\n\n\n\n\n# The code to be completed is:\ndef get_directory_modules(directory):\n\n    \"\"\"\n    This function returns a list of tuples containing the names and paths of the modules in a given directory. It first checks if the list of local modules is already populated and if the directory matches the directory of the first module in the list. If so, it returns the list as is. If not, it checks if the given directory is a valid directory. If it is not, it sets the directory to the parent directory of the given file path. Then, it iterates through the files in the directory and checks if each file is a Python file. If it is, it extracts the module name by removing the file extension and adds a tuple of the module name and the file path to the list of local modules. Finally, it returns the list of local modules.\n    Input-Output Arguments\n    :param directory: String. The directory to search for modules.\n    :return: List of tuples. A list containing tuples of module names and file paths.\n    \"\"\""}
{"namespace": "whereami.predict.crossval", "prompt": "# Please complete the crossval function based on the contexts above the function.\n\n# The contexts above the function are:\nimport json\nfrom collections import Counter\n\nfrom access_points import get_scanner\n\nfrom whereami.get_data import get_train_data, get_external_sample\nfrom whereami.get_data import sample\nfrom whereami.pipeline import get_model\nfrom whereami.get_data import aps_to_dict\nfrom whereami.compat import cross_val_score\n\n\ndef predict_proba(input_path=None, model_path=None, device=\"\"):\n    lp = get_model(model_path)\n    data_sample = sample(device) if input_path is None else get_external_sample(input_path)\n    print(json.dumps(dict(zip(lp.classes_, lp.predict_proba(data_sample)[0]))))\n\n\ndef predict(input_path=None, model_path=None, device=\"\"):\n    lp = get_model(model_path)\n    data_sample = sample(device) if input_path is None else get_external_sample(input_path)\n    return lp.predict(data_sample)[0]\n\n\n\n\n# The code to be completed is:\ndef crossval(clf=None, X=None, y=None, folds=10, n=5, path=None):\n\n    \"\"\"\n    Perform cross-validation on a given classifier using the specified data. First, if the input data X or labels y are not provided, the function will retrieve them from a given path. Then, if the number of samples in X is less than the number of folds, it will raise a ValueError 'There are not enough samples ({length of X}). Need at least {folds number}.'.\n    Next, if no classifier model is provided, it will obtain one from the given path.\n    It then prints \"KFold folds={folds number}, running {n} times\". The function then performs cross-validation by iterating n times. In each iteration, it  evaluate the performance of the classifier on each fold, and calculates the average accuracy. After each iteration, it prints \"{iteration number (starting from 1)}/{n}: {average accuracy of the iteration}\". Finally, after all iterations are complete, it prints \"-------- total --------\" and then prints the total average accuracy obtained from all iterations and returns this value.\n    Input-Output Arguments\n    :param clf: Classifier. The classifier to be used for cross-validation. If not provided, it retrieves the classifier from the specified path.\n    :param X: Array-like. The input data features. If not provided, it retrieves the training data features from the specified path.\n    :param y: Array-like. The target variable. If not provided, it retrieves the training data target variable from the specified path.\n    :param folds: Integer. The number of folds to be used in cross-validation. Defaults to 10.\n    :param n: Integer. The number of times to run cross-validation. Defaults to 5.\n    :param path: String. The path to the training data. If not provided, the data is assumed to be already provided in X and y.\n    :return: Float. The average score obtained from cross-validation.\n    \"\"\""}
{"namespace": "sslyze.plugins.certificate_info._symantec.SymantecDistructTester.get_distrust_timeline", "prompt": "# Please complete the get_distrust_timeline function based on the contexts above the function.\n\n# The contexts above the function are:\nimport binascii\nfrom datetime import datetime\nfrom enum import Enum\nfrom typing import List, Optional\n\nfrom cryptography.x509 import Certificate\n\n\n\n\nclass SymantecDistrustTimelineEnum(Enum):\n    MARCH_2018 = 1\n    SEPTEMBER_2018 = 2\n\n\nclass SymantecDistructTester:\n    \"\"\"Logic to detect Synmantec certificates, to be distrusted by Google and Mozilla.\n\n    https://security.googleblog.com/2017/09/chromes-plan-to-distrust-symantec.html\n    \"\"\"\n\n    # Taken from https://cs.chromium.org/chromium/src/net/cert/symantec_certs.cc\n    _CA_KEYS_BLACKLIST = [\n        # kSymantecRoots\n        \"023c81cce8e7c64fa942d3c15048707d35d9bb5b87f4f544c5bf1bc5643af2fa\",\n        \"0999bf900bd5c297865e21e1aade6cf6bb3a94d11ae5ea798442a4e2f813241f\",\n        \"0bdd5abe940caaabe8b2bba88348fb6f4aa4cc84436f880bece66b48bda913d8\",\n        \"16a9e012d32329f282b10bbf57c7c0b42ae80f6ac9542eb409bc1c2cde50d322\",\n        \"17755a5c295f3d2d72e6f031a1f07f400c588b9e582b22f17eae31a1590d1185\",\n        \"1906c6124dbb438578d00e066d5054c6c37f0fa6028c05545e0994eddaec8629\",\n        \"1916f3508ec3fad795f8dc4bd316f9c6085a64de3c4153ac6d62d5ea19515d39\",\n        \"1d75d0831b9e0885394d32c7a1bfdb3dbc1c28e2b0e8391fb135981dbc5ba936\",\n        \"22076e5aef44bb9a416a28b7d1c44322d7059f60feffa5caf6c5be8447891303\",\n        \"25b41b506e4930952823a6eb9f1d31def645ea38a5c6c6a96d71957e384df058\",\n        \"26c18dc6eea6f632f676bceba1d8c2b48352f29c2d5fcda878e09dcb832dd6e5\",\n        \"2dc9470be63ef4acf1bd828609402bb7b87bd99638a643934e88682d1be8c308\",\n        \"2dee5171596ab8f3cd3c7635fea8e6c3006aa9e31db39d03a7480ddb2428a33e\",\n        \"3027a298fa57314dc0e3dd1019411b8f404c43c3f934ce3bdf856512c80aa15c\",\n        \"31512680233f5f2a1f29437f56d4988cf0afc41cc6c5da6275928e9c0beade27\",\n        \"43b3107d7342165d406cf975cd79b36ed1645048f05d7ff6ea0096e427b7db84\",\n        \"463dbb9b0a26ed2616397b643125fbd29b66cf3a46fdb4384b209e78237a1aff\",\n        \"479d130bf3fc61dc2f1d508d239a13276ae7b3c9841011a02c1402c7e677bd5f\",\n        \"4905466623ab4178be92ac5cbd6584f7a1e17f27652d5a85af89504ea239aaaa\",\n        \"495a96ba6bad782407bd521a00bace657bb355555e4bb7f8146c71bba57e7ace\",\n        \"4ba6031ca305b09e53bde3705145481d0332b651fe30370dd5254cc4d2cb32f3\",\n        \"5192438ec369d7ee0ce71f5c6db75f941efbf72e58441715e99eab04c2c8acee\",\n        \"567b8211fd20d3d283ee0cd7ce0672cb9d99bc5b487a58c9d54ec67f77d4a8f5\",\n        \"5c4f285388f38336269a55c7c12c0b3ca73fef2a5a4df82b89141e841a6c4de4\",\n        \"67dc4f32fa10e7d01a79a073aa0c9e0212ec2ffc3d779e0aa7f9c0f0e1c2c893\",\n        \"6b86de96a658a56820a4f35d90db6c3efdd574ce94b909cb0d7ff17c3c189d83\",\n        \"7006a38311e58fb193484233218210c66125a0e4a826aed539ac561dfbfbd903\",\n        \"781f1c3a6a42e3e915222db4967702a2e577aeb017075fa3c159851fddd0535e\",\n        \"7caa03465124590c601e567e52148e952c0cffe89000530fe0d95b6d50eaae41\",\n        \"809f2baae35afb4f36bd6476ce75c2001077901b6af5c4dab82e188c6b95c1a1\",\n        \"81a98fc788c35f557645a95224e50cd1dac8ffb209dc1e5688aa29205f132218\",\n        \"860a7f19210d5ead057a78532b80951453cb2907315f3ba7aa47b69897d70f3f\",\n        \"87af34d66fb3f2fdf36e09111e9aba2f6f44b207f3863f3d0b54b25023909aa5\",\n        \"95735473bd67a3b95a8d5f90c5a21ace1e0d7947320674d4ab847972b91544d2\",\n        \"967b0cd93fcef7f27ce2c245767ae9b05a776b0649f9965b6290968469686872\",\n        \"9699225c5de52e56cdd32df2e96d1cfea5aa3ca0bb52cd8933c23b5c27443820\",\n        \"9c6f6a123cbaa4ee34dbeceee24c97d738878cb423f3c2273903424f5d1f6dd5\",\n        \"a6f1f9bf8a0a9ddc080fb49b1efc3d1a1c2c32dc0e136a5b00c97316f2a3dc11\",\n        \"ab3876c3da5de0c9cf6736868ee5b88bf9ba1dff9c9d72d2fe5a8d2f78302166\",\n        \"ab39a4b025955691a40269f353fa1d5cb94eaf6c7ea9808484bbbb62fd9f68f3\",\n        \"ab5cdb3356397356d6e691973c25b8618b65d76a90486ea7a8a5c17767f4673a\",\n        \"ab98495276adf1ecaff28f35c53048781e5c1718dab9c8e67a504f4f6a51328f\",\n        \"acf65e1d62cb58a2bafd6ffab40fb88699c47397cf5cb483d42d69cad34cd48b\",\n        \"af207c61fd9c7cf92c2afe8154282dc3f2cbf32f75cd172814c52b03b7ebc258\",\n        \"b1124142a5a1a5a28819c735340eff8c9e2f8168fee3ba187f253bc1a392d7e2\",\n        \"b2def5362ad3facd04bd29047a43844f767034ea4892f80e56bee690243e2502\",\n        \"bcfb44aab9ad021015706b4121ea761c81c9e88967590f6f94ae744dc88b78fb\",\n        \"c07135f6b452398264a4776dbd0a6a307c60a36f967bd26321dcb817b5c0c481\",\n        \"cab482cd3e820c5ce72aa3b6fdbe988bb8a4f0407ecafd8c926e36824eab92dd\",\n        \"d2f91a04e3a61d4ead7848c8d43b5e1152d885727489bc65738b67c0a22785a7\",\n        \"d3a25da80db7bab129a066ab41503dddffa02c768c0589f99fd71193e69916b6\",\n        \"d4af6c0a482310bd7c54bb7ab121916f86c0c07cd52fcac32d3844c26005115f\",\n        \"da800b80b2a87d399e66fa19d72fdf49983b47d8cf322c7c79503a0c7e28feaf\",\n        \"f15f1d323ed9ca98e9ea95b33ec5dda47ea4c329f952c16f65ad419e64520476\",\n        \"f2e9365ea121df5eebd8de2468fdc171dc0a9e46dadc1ab41d52790ba980a7c2\",\n        \"f53c22059817dd96f400651639d2f857e21070a59abed9079400d9f695506900\",\n        \"f6b59c8e2789a1fd5d5b253742feadc6925cb93edc345e53166e12c52ba2a601\",\n        \"ff5680cd73a5703da04817a075fd462506a73506c4b81a1583ef549478d26476\",\n    ]\n\n    _CA_KEYS_WHITELIST = [\n        # kSymantecExceptions\n        \"56e98deac006a729afa2ed79f9e419df69f451242596d2aaf284c74a855e352e\",\n        \"7289c06dedd16b71a7dcca66578572e2e109b11d70ad04c2601b6743bc66d07b\",\n        \"8bb593a93be1d0e8a822bb887c547890c3e706aad2dab76254f97fb36b82fc26\",\n        \"b5cf82d47ef9823f9aa78f123186c52e8879ea84b0f822c91d83e04279b78fd5\",\n        \"b94c198300cec5c057ad0727b70bbe91816992256439a7b32f4598119dda9c97\",\n        \"c0554bde87a075ec13a61f275983ae023957294b454caf0a9724e3b21b7935bc\",\n        \"e24f8e8c2185da2f5e88d4579e817c47bf6eafbc8505f0f960fd5a0df4473ad3\",\n        \"ec722969cb64200ab6638f68ac538e40abab5b19a6485661042a1061c4612776\",\n        \"fae46000d8f7042558541e98acf351279589f83b6d3001c18442e4403d111849\",\n        # kSymantecManagedCAs\n        \"7cac9a0ff315387750ba8bafdb1c2bc29b3f0bba16362ca93a90f84da2df5f3e\",\n        \"ac50b5fb738aed6cb781cc35fbfff7786f77109ada7c08867c04a573fd5cf9ee\",\n    ]\n\n    @classmethod\n\n\n# The code to be completed is:\n    def get_distrust_timeline(\n        cls, verified_certificate_chain: List[Certificate]\n    ) -> Optional[SymantecDistrustTimelineEnum]:\n\n        \"\"\"\n        This function checks the given list of verified certificates for the presence of Symantec root certificates. It determines the distrust timeline based on the presence of blacklisted and whitelisted certificates in the chain.\n        Input-Output Arguments\n        :param cls: The class object of SymantecDistructTester.\n        :param verified_certificate_chain: List of Certificate. A list of verified certificates.\n        :return: Optional[SymantecDistrustTimelineEnum]. The distrust timeline enum value, which can be either \"MARCH_2018\" or \"SEPTEMBER_2018\", or None if no distrust is detected.\n        \"\"\""}
{"namespace": "gif_for_cli.display.display_txt_frames", "prompt": "# Please complete the display_txt_frames function based on the contexts above the function.\n\n# The contexts above the function are:\n\"\"\"\nCopyright 2018 Google LLC\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    https://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n\"\"\"\nimport time\n\nfrom .constants import STORED_CELL_CHAR\nfrom .utils import get_sorted_filenames\n\n\n\n\n# The code to be completed is:\ndef display_txt_frames(txt_frames, stdout, num_loops, seconds_per_frame):\n\n    \"\"\"\n    This function displays a sequence of text frames on the standard output. It iterates through the given text frames and prints each frame on a new line. It also allows for a specified number of loops and a delay between frames. A KeyboardInterrupt will be raised if there is any exception.\n    Input-Output Arguments\n    :param txt_frames: List of strings. The text frames to be displayed.\n    :param stdout: Standard output. The output stream where the frames will be printed.\n    :param num_loops: Integer. The number of times the frames should be displayed. If not specified, the frames will be displayed indefinitely.\n    :param seconds_per_frame: Float. The delay in seconds between each frame.\n    :return: No return values.\n    \"\"\""}
{"namespace": "datasette.utils.asgi.Request.post_body", "prompt": "# Please complete the post_body function based on the contexts above the function.\n\n# The contexts above the function are:\nimport json\n\nfrom mimetypes import guess_type\nfrom urllib.parse import parse_qs, urlunparse, parse_qsl\nfrom pathlib import Path\nfrom http.cookies import SimpleCookie, Morsel\nimport aiofiles\nimport aiofiles.os\n\n# Workaround for adding samesite support to pre 3.8 python\nMorsel._reserved[\"samesite\"] = \"SameSite\"\n# Thanks, Starlette:\n# https://github.com/encode/starlette/blob/519f575/starlette/responses.py#L17\n\n\nclass Base400(Exception):\n    status = 400\n\n\nclass NotFound(Base400):\n    status = 404\n\n\nclass Forbidden(Base400):\n    status = 403\n\n\nclass BadRequest(Base400):\n    status = 400\n\n\nSAMESITE_VALUES = (\"strict\", \"lax\", \"none\")\n\n\nclass Request:\n    def __init__(self, scope, receive):\n        self.scope = scope\n        self.receive = receive\n\n    def __repr__(self):\n        return '<asgi.Request method=\"{}\" url=\"{}\">'.format(self.method, self.url)\n\n    @property\n    def method(self):\n        return self.scope[\"method\"]\n\n    @property\n    def url(self):\n        return urlunparse(\n            (self.scheme, self.host, self.path, None, self.query_string, None)\n        )\n\n    @property\n    def url_vars(self):\n        return (self.scope.get(\"url_route\") or {}).get(\"kwargs\") or {}\n\n    @property\n    def scheme(self):\n        return self.scope.get(\"scheme\") or \"http\"\n\n    @property\n    def headers(self):\n        return {\n            k.decode(\"latin-1\").lower(): v.decode(\"latin-1\")\n            for k, v in self.scope.get(\"headers\") or []\n        }\n\n    @property\n    def host(self):\n        return self.headers.get(\"host\") or \"localhost\"\n\n    @property\n    def cookies(self):\n        cookies = SimpleCookie()\n        cookies.load(self.headers.get(\"cookie\", \"\"))\n        return {key: value.value for key, value in cookies.items()}\n\n    @property\n    def path(self):\n        if self.scope.get(\"raw_path\") is not None:\n            return self.scope[\"raw_path\"].decode(\"latin-1\").partition(\"?\")[0]\n        else:\n            path = self.scope[\"path\"]\n            if isinstance(path, str):\n                return path\n            else:\n                return path.decode(\"utf-8\")\n\n    @property\n    def query_string(self):\n        return (self.scope.get(\"query_string\") or b\"\").decode(\"latin-1\")\n\n    @property\n    def full_path(self):\n        qs = self.query_string\n        return \"{}{}\".format(self.path, (\"?\" + qs) if qs else \"\")\n\n    @property\n    def args(self):\n\n        parsed_params = MultiParams()\n        query_string = self.scope.get('query_string', b'').decode('utf-8')\n        if query_string:\n            for key, value in parse_qs(query_string).items():\n                parsed_params.add(key, value)\n        return parsed_params\n\n\n    @property\n    def actor(self):\n        return self.scope.get(\"actor\", None)\n\n\n\n# The code to be completed is:\n    async def post_body(self):\n\n        \"\"\"\n        This function reads the body of a POST request asynchronously. It continuously receives messages from the request until there is no more body to read, and appends the body content to a byte string. It then returns the complete body content.\n        Input-Output Arguments\n        :param self: Request. An instance of the Request class.\n        :return: bytes. The body content of the POST request.\n        \"\"\""}
{"namespace": "diffprivlib.utils.check_random_state", "prompt": "# Please complete the check_random_state function based on the contexts above the function.\n\n# The contexts above the function are:\n# MIT License\n#\n# Copyright (C) IBM Corporation 2019\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the\n# rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit\n# persons to whom the Software is furnished to do so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the\n# Software.\n#\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE\n# WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,\n# TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\"\"\"\nBasic functions and other utilities for the differential privacy library\n\"\"\"\nimport secrets\nimport warnings\n\nimport numpy as np\nfrom sklearn.utils import check_random_state as skl_check_random_state\n\n\ndef copy_docstring(source):\n    \"\"\"Decorator function to copy a docstring from a `source` function to a `target` function.\n\n    The docstring is only copied if a docstring is present in `source`, and if none is present in `target`.  Takes\n    inspiration from similar in `matplotlib`.\n\n    Parameters\n    ----------\n    source : method\n        Source function from which to copy the docstring.  If ``source.__doc__`` is empty, do nothing.\n\n    Returns\n    -------\n    target : method\n        Target function with new docstring.\n\n    \"\"\"\n    def copy_func(target):\n        if source.__doc__ and not target.__doc__:\n            target.__doc__ = source.__doc__\n        return target\n    return copy_func\n\n\ndef warn_unused_args(args):\n    \"\"\"Warn the user about supplying unused `args` to a diffprivlib model.\n\n    Arguments can be supplied as a string, a list of strings, or a dictionary as supplied to kwargs.\n\n    Parameters\n    ----------\n    args : str or list or dict\n        Arguments for which warnings should be thrown.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n    if isinstance(args, str):\n        args = [args]\n\n    for arg in args:\n        warnings.warn(f\"Parameter '{arg}' is not functional in diffprivlib.  Remove this parameter to suppress this \"\n                      \"warning.\", DiffprivlibCompatibilityWarning)\n\n\n\n\n# The code to be completed is:\ndef check_random_state(seed, secure=False):\n\n    \"\"\"\n    This function turns the seed into a np.random.RandomState or secrets.SystemRandom instance based on the given condition. If seed is None and secure is False, return the RandomState singleton used by np.random. If seed is None and secure is True, return a SystemRandom instance from secrets. If seed is an int, return a new RandomState instance seeded with seed. If seed is already a RandomState or SystemRandom instance, return it. Otherwise raise ValueError.\n    Input-Output Arguments\n    :param seed : None, int or instance of RandomState.\n    :param secure : bool, default: False. Specifies if a secure random number generator from secrets can be used.\n    :return: np.random.RandomState or secrets.SystemRandom instance.\n    \"\"\""}
{"namespace": "zulipterminal.config.themes.complete_and_incomplete_themes", "prompt": "# Please complete the complete_and_incomplete_themes function based on the contexts above the function.\n\n# The contexts above the function are:\nfrom typing import Any, Dict, List, Optional, Tuple, Union\n\nfrom pygments.token import STANDARD_TYPES\n\n\nfrom zulipterminal.themes import gruvbox_dark, gruvbox_light, zt_blue, zt_dark, zt_light\n\n\nStyleSpec = Union[\n    Tuple[Optional[str], str, str],\n    Tuple[Optional[str], str, str, Optional[str]],\n    Tuple[Optional[str], str, str, Optional[str], str, str],\n]\nThemeSpec = List[StyleSpec]\n\n# fmt: off\n# The keys in REQUIRED_STYLES specify what styles are necessary for a theme to\n# be complete, while the values are those used to style each element in\n# monochrome (1-bit) mode - independently of the specified theme\nREQUIRED_STYLES = {\n    # style name      : monochrome style\n    None              : '',\n    'selected'        : 'standout',\n    'msg_selected'    : 'standout',\n    'header'          : 'bold',\n    'general_narrow'  : 'standout',\n    'general_bar'     : '',\n    'name'            : '',\n    'unread'          : 'strikethrough',\n    'user_active'     : 'bold',\n    'user_idle'       : '',\n    'user_offline'    : '',\n    'user_inactive'   : '',\n    'title'           : 'bold',\n    'column_title'    : 'bold',\n    'time'            : '',\n    'bar'             : 'standout',\n    'msg_emoji'       : 'bold',\n    'reaction'        : 'bold',\n    'reaction_mine'   : 'standout',\n    'msg_heading'     : 'bold',\n    'msg_math'        : 'standout',\n    'msg_mention'     : 'bold',\n    'msg_link'        : '',\n    'msg_link_index'  : 'bold',\n    'msg_quote'       : 'underline',\n    'msg_code'        : 'bold',\n    'msg_bold'        : 'bold',\n    'msg_time'        : 'bold',\n    'footer'          : 'standout',\n    'footer_contrast' : 'standout',\n    'starred'         : 'bold',\n    'unread_count'    : 'bold',\n    'starred_count'   : '',\n    'table_head'      : 'bold',\n    'filter_results'  : 'bold',\n    'edit_topic'      : 'standout',\n    'edit_tag'        : 'standout',\n    'edit_author'     : 'bold',\n    'edit_time'       : 'bold',\n    'current_user'    : '',\n    'muted'           : 'bold',\n    'popup_border'    : 'bold',\n    'popup_category'  : 'bold',\n    'popup_contrast'  : 'standout',\n    'popup_important' : 'bold',\n    'widget_disabled' : 'strikethrough',\n    'area:help'       : 'standout',\n    'area:msg'        : 'standout',\n    'area:stream'     : 'standout',\n    'area:error'      : 'standout',\n    'area:user'       : 'standout',\n    'search_error'    : 'standout',\n    'task:success'    : 'standout',\n    'task:error'      : 'standout',\n    'task:warning'    : 'standout',\n}\n\nREQUIRED_META = {\n    'pygments': {\n        'styles'     : None,\n        'background' : None,\n        'overrides'  : None,\n    }\n}\n# fmt: on\n\n# This is the main list of themes\nTHEMES: Dict[str, Any] = {\n    \"gruvbox_dark\": gruvbox_dark,\n    \"gruvbox_light\": gruvbox_light,\n    \"zt_dark\": zt_dark,\n    \"zt_light\": zt_light,\n    \"zt_blue\": zt_blue,\n}\n\n# These are older aliases to some of the above, for compatibility\n# NOTE: Do not add to this section, and only modify if a theme name changes\nTHEME_ALIASES = {\n    \"default\": \"zt_dark\",\n    \"gruvbox\": \"gruvbox_dark\",\n    \"light\": \"zt_light\",\n    \"blue\": \"zt_blue\",\n}\n\n# These are urwid color names with underscores instead of spaces\nvalid_16_color_codes = [\n    \"default\",\n    \"black\",\n    \"dark_red\",\n    \"dark_green\",\n    \"brown\",\n    \"dark_blue\",\n    \"dark_magenta\",\n    \"dark_cyan\",\n    \"dark_gray\",\n    \"light_red\",\n    \"light_green\",\n    \"yellow\",\n    \"light_blue\",\n    \"light_magenta\",\n    \"light_cyan\",\n    \"light_gray\",\n    \"white\",\n]\n\n\nclass InvalidThemeColorCode(Exception):\n    pass\n\n\ndef all_themes() -> List[str]:\n    return list(THEMES.keys())\n\n\ndef aliased_themes() -> Dict[str, str]:\n    return dict(THEME_ALIASES)\n\n\n\n\n# The code to be completed is:\ndef complete_and_incomplete_themes() -> Tuple[List[str], List[str]]:\n\n    \"\"\"\n    This function determines the complete and incomplete themes based on the predefined set of required styles and meta information. It iterates through the themes dictionary and checks if the styles and meta information of each theme match the required styles and meta. The themes that meet the requirements are considered complete, while the rest are considered incomplete.\n    Input-Output Arguments\n    :param: No input parameters.\n    :return: Tuple of two lists. The first list contains the names of the complete themes, sorted in alphabetical order. The second list contains the names of the incomplete themes, also sorted in alphabetical order.\n    \"\"\""}
{"namespace": "mistune.plugins.table.table_in_quote", "prompt": "# Please complete the table_in_quote function based on the contexts above the function.\n\n# The contexts above the function are:\nimport re\nfrom ..helpers import PREVENT_BACKSLASH\n\n# https://michelf.ca/projects/php-markdown/extra/#table\n\n__all__ = ['table', 'table_in_quote', 'table_in_list']\n\n\nTABLE_PATTERN = (\n  r'^ {0,3}\\|(?P<table_head>.+)\\|[ \\t]*\\n'\n  r' {0,3}\\|(?P<table_align> *[-:]+[-| :]*)\\|[ \\t]*\\n'\n  r'(?P<table_body>(?: {0,3}\\|.*\\|[ \\t]*(?:\\n|$))*)\\n*'\n)\nNP_TABLE_PATTERN = (\n  r'^ {0,3}(?P<nptable_head>\\S.*\\|.*)\\n'\n  r' {0,3}(?P<nptable_align>[-:]+ *\\|[-| :]*)\\n'\n  r'(?P<nptable_body>(?:.*\\|.*(?:\\n|$))*)\\n*'\n)\n\nTABLE_CELL = re.compile(r'^ {0,3}\\|(.+)\\|[ \\t]*$')\nCELL_SPLIT = re.compile(r' *' + PREVENT_BACKSLASH + r'\\| *')\nALIGN_CENTER = re.compile(r'^ *:-+: *$')\nALIGN_LEFT = re.compile(r'^ *:-+ *$')\nALIGN_RIGHT = re.compile(r'^ *-+: *$')\n\n\ndef parse_table(block, m, state):\n    pos = m.end()\n    header = m.group('table_head')\n    align = m.group('table_align')\n    thead, aligns = _process_thead(header, align)\n    if not thead:\n        return\n\n    rows = []\n    body = m.group('table_body')\n    for text in body.splitlines():\n        m = TABLE_CELL.match(text)\n        if not m:  # pragma: no cover\n            return\n        row = _process_row(m.group(1), aligns)\n        if not row:\n            return\n        rows.append(row)\n\n    children = [thead, {'type': 'table_body', 'children': rows}]\n    state.append_token({'type': 'table', 'children': children})\n    return pos\n\n\ndef parse_nptable(block, m, state):\n    header = m.group('nptable_head')\n    align = m.group('nptable_align')\n    thead, aligns = _process_thead(header, align)\n    if not thead:\n        return\n\n    rows = []\n    body = m.group('nptable_body')\n    for text in body.splitlines():\n        row = _process_row(text, aligns)\n        if not row:\n            return\n        rows.append(row)\n\n    children = [thead, {'type': 'table_body', 'children': rows}]\n    state.append_token({'type': 'table', 'children': children})\n    return m.end()\n\n\ndef _process_thead(header, align):\n    headers = CELL_SPLIT.split(header)\n    aligns = CELL_SPLIT.split(align)\n    if len(headers) != len(aligns):\n      return None, None\n\n    for i, v in enumerate(aligns):\n        if ALIGN_CENTER.match(v):\n            aligns[i] = 'center'\n        elif ALIGN_LEFT.match(v):\n            aligns[i] = 'left'\n        elif ALIGN_RIGHT.match(v):\n            aligns[i] = 'right'\n        else:\n            aligns[i] = None\n\n    children = [\n        {\n            'type': 'table_cell',\n            'text': text.strip(),\n            'attrs': {'align': aligns[i], 'head': True}\n        }\n        for i, text in enumerate(headers)\n    ]\n    thead = {'type': 'table_head', 'children': children}\n    return thead, aligns\n\n\ndef _process_row(text, aligns):\n    cells = CELL_SPLIT.split(text)\n    if len(cells) != len(aligns):\n        return None\n\n    children = [\n        {\n            'type': 'table_cell',\n            'text': text.strip(),\n            'attrs': {'align': aligns[i], 'head': False}\n        }\n        for i, text in enumerate(cells)\n    ]\n    return {'type': 'table_row', 'children': children}\n\n\ndef render_table(renderer, text):\n    return '<table>\\n' + text + '</table>\\n'\n\n\ndef render_table_head(renderer, text):\n    return '<thead>\\n<tr>\\n' + text + '</tr>\\n</thead>\\n'\n\n\ndef render_table_body(renderer, text):\n    return '<tbody>\\n' + text + '</tbody>\\n'\n\n\ndef render_table_row(renderer, text):\n    return '<tr>\\n' + text + '</tr>\\n'\n\n\ndef render_table_cell(renderer, text, align=None, head=False):\n    if head:\n        tag = 'th'\n    else:\n        tag = 'td'\n\n    html = '  <' + tag\n    if align:\n        html += ' style=\"text-align:' + align + '\"'\n\n    return html + '>' + text + '</' + tag + '>\\n'\n\n\ndef table(md):\n    \"\"\"A mistune plugin to support table, spec defined at\n    https://michelf.ca/projects/php-markdown/extra/#table\n\n    Here is an example:\n\n    .. code-block:: text\n\n        First Header  | Second Header\n        ------------- | -------------\n        Content Cell  | Content Cell\n        Content Cell  | Content Cell\n\n    :param md: Markdown instance\n    \"\"\"\n    md.block.register('table', TABLE_PATTERN, parse_table, before='paragraph')\n    md.block.register('nptable', NP_TABLE_PATTERN, parse_nptable, before='paragraph')\n\n    if md.renderer and md.renderer.NAME == 'html':\n        md.renderer.register('table', render_table)\n        md.renderer.register('table_head', render_table_head)\n        md.renderer.register('table_body', render_table_body)\n        md.renderer.register('table_row', render_table_row)\n        md.renderer.register('table_cell', render_table_cell)\n\n\n\n\n# The code to be completed is:\ndef table_in_quote(md):\n\n    \"\"\"\n    This function enables the table plugin in block quotes by inserting rules for table and nptable before the paragraph in the block quote rules.\n    Input-Output Arguments\n    :param md: Markdown. The Markdown instance.\n    :return: No return values.\n    \"\"\""}
{"namespace": "sacred.host_info.host_info_getter", "prompt": "# Please complete the host_info_getter function based on the contexts above the function.\n\n# The contexts above the function are:\n\"\"\"Helps to collect information about the host of an experiment.\"\"\"\n\nimport os\nimport platform\nimport re\nimport subprocess\nfrom xml.etree import ElementTree\nimport warnings\nfrom typing import List\n\nimport cpuinfo\n\nfrom sacred.utils import optional_kwargs_decorator\nfrom sacred.settings import SETTINGS\n\n__all__ = (\"host_info_gatherers\", \"get_host_info\", \"host_info_getter\")\n\n# Legacy global dict of functions that are used\n# to collect the host information.\nhost_info_gatherers = {}\n\n\nclass IgnoreHostInfo(Exception):\n    \"\"\"Used by host_info_getters to signal that this cannot be gathered.\"\"\"\n\n\nclass HostInfoGetter:\n    def __init__(self, getter_function, name):\n        self.getter_function = getter_function\n        self.name = name\n\n    def __call__(self):\n        return self.getter_function()\n\n    def get_info(self):\n        return self.getter_function()\n\n\ndef host_info_gatherer(name):\n    def wrapper(f):\n        return HostInfoGetter(f, name)\n\n    return wrapper\n\n\ndef check_additional_host_info(additional_host_info: List[HostInfoGetter]):\n    names_taken = [x.name for x in _host_info_gatherers_list]\n    for getter in additional_host_info:\n        if getter.name in names_taken:\n            error_msg = (\n                \"Key {} used in `additional_host_info` already exists as a \"\n                \"default gatherer function. Do not use the following keys: \"\n                \"{}\"\n            ).format(getter.name, names_taken)\n            raise KeyError(error_msg)\n\n\ndef get_host_info(additional_host_info: List[HostInfoGetter] = None):\n    \"\"\"Collect some information about the machine this experiment runs on.\n\n    Returns\n    -------\n    dict\n        A dictionary with information about the CPU, the OS and the\n        Python version of this machine.\n\n    \"\"\"\n    additional_host_info = additional_host_info or []\n    # can't use += because we don't want to modify the mutable argument.\n    additional_host_info = additional_host_info + _host_info_gatherers_list\n    all_host_info_gatherers = host_info_gatherers.copy()\n    for getter in additional_host_info:\n        all_host_info_gatherers[getter.name] = getter\n    host_info = {}\n    for k, v in all_host_info_gatherers.items():\n        try:\n            host_info[k] = v()\n        except IgnoreHostInfo:\n            pass\n    return host_info\n\n\n@optional_kwargs_decorator\n\n\n# The code to be completed is:\ndef host_info_getter(func, name=None):\n\n    \"\"\"\n    This function is a decorator that adds the decorated function to the process of collecting host information.\n    Input-Output Arguments\n    :param func: callable. A function that can be called without arguments and returns some JSON-serializable information.\n    :param name: str, optional. The name of the corresponding entry in host_info. Defaults to the name of the function.\n    :return: The function itself.\n    \"\"\""}
{"namespace": "jwt.utils.base64url_decode", "prompt": "# Please complete the base64url_decode function based on the contexts above the function.\n\n# The contexts above the function are:\nimport base64\nimport binascii\nimport re\nfrom typing import Union\n\ntry:\n    from cryptography.hazmat.primitives.asymmetric.ec import EllipticCurve\n    from cryptography.hazmat.primitives.asymmetric.utils import (\n        decode_dss_signature,\n        encode_dss_signature,\n    )\nexcept ModuleNotFoundError:\n    pass\n\n\ndef force_bytes(value: Union[bytes, str]) -> bytes:\n    if isinstance(value, str):\n        return value.encode(\"utf-8\")\n    elif isinstance(value, bytes):\n        return value\n    else:\n        raise TypeError(\"Expected a string value\")\n\n\n\n\n# The code to be completed is:\ndef base64url_decode(input: Union[bytes, str]) -> bytes:\n\n    \"\"\"\n    Decode a base64url-encoded input string or bytes and return the decoded bytes. It first converts the input to bytes if it is a string, then pads the input with \"=\" characters if necessary, and finally decodes the input using the base64.urlsafe_b64decode() function.\n    Input-Output Arguments\n    :param input: Union[bytes, str]. The base64url-encoded input string or bytes to be decoded.\n    :return: bytes. The decoded bytes.\n    \"\"\""}
{"namespace": "mackup.utils.copy", "prompt": "# Please complete the copy function based on the contexts above the function.\n\n# The contexts above the function are:\n\"\"\"System static utilities being used by the modules.\"\"\"\nimport base64\nimport os\nimport platform\nimport shutil\nimport stat\nimport subprocess\nimport sys\nimport sqlite3\nfrom six.moves import input\n\nfrom . import constants\n\n\n# Flag that controls how user confirmation works.\n# If True, the user wants to say \"yes\" to everything.\nFORCE_YES = False\n\n# Flag that control if mackup can be run as root\nCAN_RUN_AS_ROOT = False\n\n\ndef confirm(question):\n    \"\"\"\n    Ask the user if he really wants something to happen.\n\n    Args:\n        question(str): What can happen\n\n    Returns:\n        (boolean): Confirmed or not\n    \"\"\"\n    if FORCE_YES:\n        return True\n\n    while True:\n        answer = input(question + \" <Yes|No> \").lower()\n\n        if answer == \"yes\" or answer == \"y\":\n            confirmed = True\n            break\n        if answer == \"no\" or answer == \"n\":\n            confirmed = False\n            break\n\n    return confirmed\n\n\ndef delete(filepath):\n    \"\"\"\n    Delete the given file, directory or link.\n\n    It Should support undelete later on.\n\n    Args:\n        filepath (str): Absolute full path to a file. e.g. /path/to/file\n    \"\"\"\n    # Some files have ACLs, let's remove them recursively\n    remove_acl(filepath)\n\n    # Some files have immutable attributes, let's remove them recursively\n    remove_immutable_attribute(filepath)\n\n    # Finally remove the files and folders\n    if os.path.isfile(filepath) or os.path.islink(filepath):\n        os.remove(filepath)\n    elif os.path.isdir(filepath):\n        shutil.rmtree(filepath)\n\n\n\n\n# The code to be completed is:\ndef copy(src, dst):\n\n    \"\"\"\n    This function copies a file or a folder (recursively) from the source path to the destination path. It first checks if the source and destination paths are valid and absolute paths. Then, it creates the necessary directories in the destination path if they do not exist. If the source is a file, it copies the file to the destination. If the source is a folder, it copies the entire folder to the destination. If the source is neither a file nor a folder, it raises a ValueError. Finally, it sets the appropriate file permissions for the copied file or folder.\n    Input-Output Arguments\n    :param src: str. The source file or folder path.\n    :param dst: str. The destination file or folder path.\n    :return: No return values.\n    \"\"\""}
{"namespace": "kinto.plugins.quotas.scripts.rebuild_quotas", "prompt": "# Please complete the rebuild_quotas function based on the contexts above the function.\n\n# The contexts above the function are:\n\"\"\"\nkinto.plugins.quotas.scripts: scripts to maintain quotas and fix them when they're broken\n\"\"\"\nimport logging\n\nfrom kinto.core.storage import Sort\nfrom kinto.core.storage.utils import paginated\n\nfrom .listener import COLLECTION_QUOTA_OBJECT_ID\nfrom .utils import record_size\n\nlogger = logging.getLogger(__name__)\n\nOLDEST_FIRST = Sort(\"last_modified\", 1)\n\n\n\n\n# The code to be completed is:\ndef rebuild_quotas(storage, dry_run=False):\n\n    \"\"\"\n    This function rebuilds quotas for a given storage. It iterates through each bucket in the storage and calculates the total record count, storage size, and collection count for each bucket. It then updates the quota information for each bucket in the storage. Finally, it logs the final size of each bucket.\n    Input-Output Arguments\n    :param storage: The storage object to rebuild quotas for.\n    :param dry_run: Bool. Whether to perform a dry run without actually updating the quotas. Defaults to False.\n    :return: No return values.\n    \"\"\""}
{"namespace": "playhouse.kv.KeyValue.setdefault", "prompt": "# Please complete the setdefault function based on the contexts above the function.\n\n# The contexts above the function are:\nimport operator\n\nfrom peewee import *\nfrom peewee import sqlite3\nfrom peewee import Expression\nfrom playhouse.fields import PickleField\ntry:\n    from playhouse.sqlite_ext import CSqliteExtDatabase as SqliteExtDatabase\nexcept ImportError:\n    from playhouse.sqlite_ext import SqliteExtDatabase\n\n\nSentinel = type('Sentinel', (object,), {})\n\n\nclass KeyValue(object):\n    \"\"\"\n    Persistent dictionary.\n\n    :param Field key_field: field to use for key. Defaults to CharField.\n    :param Field value_field: field to use for value. Defaults to PickleField.\n    :param bool ordered: data should be returned in key-sorted order.\n    :param Database database: database where key/value data is stored.\n    :param str table_name: table name for data.\n    \"\"\"\n    def __init__(self, key_field=None, value_field=None, ordered=False,\n                 database=None, table_name='keyvalue'):\n        if key_field is None:\n            key_field = CharField(max_length=255, primary_key=True)\n        if not key_field.primary_key:\n            raise ValueError('key_field must have primary_key=True.')\n\n        if value_field is None:\n            value_field = PickleField()\n\n        self._key_field = key_field\n        self._value_field = value_field\n        self._ordered = ordered\n        self._database = database or SqliteExtDatabase(':memory:')\n        self._table_name = table_name\n        support_on_conflict = (isinstance(self._database, PostgresqlDatabase) or\n                              (isinstance(self._database, SqliteDatabase) and\n                               self._database.server_version >= (3, 24)))\n        if support_on_conflict:\n            self.upsert = self._postgres_upsert\n            self.update = self._postgres_update\n        else:\n            self.upsert = self._upsert\n            self.update = self._update\n\n        self.model = self.create_model()\n        self.key = self.model.key\n        self.value = self.model.value\n\n        # Ensure table exists.\n        self.model.create_table()\n\n    def create_model(self):\n        class KeyValue(Model):\n            key = self._key_field\n            value = self._value_field\n            class Meta:\n                database = self._database\n                table_name = self._table_name\n        return KeyValue\n\n    def query(self, *select):\n        query = self.model.select(*select).tuples()\n        if self._ordered:\n            query = query.order_by(self.key)\n        return query\n\n    def convert_expression(self, expr):\n        if not isinstance(expr, Expression):\n            return (self.key == expr), True\n        return expr, False\n\n    def __contains__(self, key):\n        expr, _ = self.convert_expression(key)\n        return self.model.select().where(expr).exists()\n\n    def __len__(self):\n        return len(self.model)\n\n    def __getitem__(self, expr):\n        converted, is_single = self.convert_expression(expr)\n        query = self.query(self.value).where(converted)\n        item_getter = operator.itemgetter(0)\n        result = [item_getter(row) for row in query]\n        if len(result) == 0 and is_single:\n            raise KeyError(expr)\n        elif is_single:\n            return result[0]\n        return result\n\n    def _upsert(self, key, value):\n        (self.model\n         .insert(key=key, value=value)\n         .on_conflict('replace')\n         .execute())\n\n    def _postgres_upsert(self, key, value):\n        (self.model\n         .insert(key=key, value=value)\n         .on_conflict(conflict_target=[self.key],\n                      preserve=[self.value])\n         .execute())\n\n    def __setitem__(self, expr, value):\n        if isinstance(expr, Expression):\n            self.model.update(value=value).where(expr).execute()\n        else:\n            self.upsert(expr, value)\n\n    def __delitem__(self, expr):\n        converted, _ = self.convert_expression(expr)\n        self.model.delete().where(converted).execute()\n\n    def __iter__(self):\n        return iter(self.query().execute())\n\n    def keys(self):\n        return map(operator.itemgetter(0), self.query(self.key))\n\n    def values(self):\n        return map(operator.itemgetter(0), self.query(self.value))\n\n    def items(self):\n        return iter(self.query().execute())\n\n    def _update(self, __data=None, **mapping):\n        if __data is not None:\n            mapping.update(__data)\n        return (self.model\n                .insert_many(list(mapping.items()),\n                             fields=[self.key, self.value])\n                .on_conflict('replace')\n                .execute())\n\n    def _postgres_update(self, __data=None, **mapping):\n        if __data is not None:\n            mapping.update(__data)\n        return (self.model\n                .insert_many(list(mapping.items()),\n                             fields=[self.key, self.value])\n                .on_conflict(conflict_target=[self.key],\n                             preserve=[self.value])\n                .execute())\n\n    def get(self, key, default=None):\n        try:\n            return self[key]\n        except KeyError:\n            return default\n\n\n\n# The code to be completed is:\n    def setdefault(self, key, default=None):\n\n        \"\"\"\n        Set the default value for the key in the KeyValue instance. If the key is not found, it sets the default value for the key and returns it.\n        Input-Output Arguments\n        :param self: KeyValue. An instance of the KeyValue class.\n        :param key: The key to set the default value.\n        :param default: The default value to set for the key. Defaults to None.\n        :return: The value corresponding to the key.\n        \"\"\""}
{"namespace": "alembic.testing.env.env_file_fixture", "prompt": "# Please complete the env_file_fixture function based on the contexts above the function.\n\n# The contexts above the function are:\nimport importlib.machinery\nimport os\nimport shutil\nimport textwrap\n\nfrom sqlalchemy.testing import config\nfrom sqlalchemy.testing import provision\n\nfrom . import util as testing_util\nfrom .. import command\nfrom .. import script\nfrom .. import util\nfrom ..script import Script\nfrom ..script import ScriptDirectory\n\n\ndef _get_staging_directory():\n    if provision.FOLLOWER_IDENT:\n        return \"scratch_%s\" % provision.FOLLOWER_IDENT\n    else:\n        return \"scratch\"\n\n\ndef staging_env(create=True, template=\"generic\", sourceless=False):\n    cfg = _testing_config()\n    if create:\n        path = os.path.join(_get_staging_directory(), \"scripts\")\n        assert not os.path.exists(path), (\n            \"staging directory %s already exists; poor cleanup?\" % path\n        )\n\n        command.init(cfg, path, template=template)\n        if sourceless:\n            try:\n                # do an import so that a .pyc/.pyo is generated.\n                util.load_python_file(path, \"env.py\")\n            except AttributeError:\n                # we don't have the migration context set up yet\n                # so running the .env py throws this exception.\n                # theoretically we could be using py_compiler here to\n                # generate .pyc/.pyo without importing but not really\n                # worth it.\n                pass\n            assert sourceless in (\n                \"pep3147_envonly\",\n                \"simple\",\n                \"pep3147_everything\",\n            ), sourceless\n            make_sourceless(\n                os.path.join(path, \"env.py\"),\n                \"pep3147\" if \"pep3147\" in sourceless else \"simple\",\n            )\n\n    sc = script.ScriptDirectory.from_config(cfg)\n    return sc\n\n\ndef clear_staging_env():\n    from sqlalchemy.testing import engines\n\n    engines.testing_reaper.close_all()\n    shutil.rmtree(_get_staging_directory(), True)\n\n\ndef script_file_fixture(txt):\n    dir_ = os.path.join(_get_staging_directory(), \"scripts\")\n    path = os.path.join(dir_, \"script.py.mako\")\n    with open(path, \"w\") as f:\n        f.write(txt)\n\n\n\n\n# The code to be completed is:\ndef env_file_fixture(txt):\n\n    \"\"\"\n    This function creates an environment file fixture by writing the given text to a file named \"env.py\" in a specific directory.\n    Input-Output Arguments\n    :param txt: String. The text to be written to the file.\n    :return: None.\n    \"\"\""}
{"namespace": "hypertools._shared.helpers.reshape_data", "prompt": "# Please complete the reshape_data function based on the contexts above the function.\n\n# The contexts above the function are:\n#!/usr/bin/env python\n\n\"\"\"\nHelper functions\n\"\"\"\n\n##PACKAGES##\nimport functools\nimport sys\nimport numpy as np\nimport copy\nfrom scipy.interpolate import PchipInterpolator as pchip\nimport seaborn as sns\nimport itertools\nimport pandas as pd\nfrom matplotlib.lines import Line2D\nnp.seterr(divide='ignore', invalid='ignore')\n\n\ndef center(x):\n    assert type(x) is list, \"Input data to center must be list\"\n    x_stacked = np.vstack(x)\n    return [i - np.mean(x_stacked, 0) for i in x]\n\n\ndef scale(x):\n    assert type(x) is list, \"Input data to scale must be list\"\n    x_stacked = np.vstack(x)\n    m1 = np.min(x_stacked)\n    m2 = np.max(x_stacked - m1)\n    f = lambda x: 2*(np.divide(x - m1, m2)) - 1\n    return [f(i) for i in x]\n\n\ndef group_by_category(vals):\n    if any(isinstance(el, list) for el in vals):\n        vals = list(itertools.chain(*vals))\n    val_set = list(sorted(set(vals), key=list(vals).index))\n    return [val_set.index(val) for val in vals]\n\n\ndef vals2colors(vals, cmap='GnBu',res=100):\n    \"\"\"Maps values to colors\n    Args:\n    values (list or list of lists) - list of values to map to colors\n    cmap (str) - color map (default is 'GnBu')\n    res (int) - resolution of the color map (default: 100)\n    Returns:\n    list of rgb tuples\n    \"\"\"\n    # flatten if list of lists\n    if any(isinstance(el, list) for el in vals):\n        vals = list(itertools.chain(*vals))\n\n    # get palette from seaborn\n    palette = np.array(sns.color_palette(cmap, res))\n    ranks = np.digitize(vals, np.linspace(np.min(vals), np.max(vals)+1, res+1)) - 1\n    return [tuple(i) for i in palette[ranks, :]]\n\n\ndef vals2bins(vals,res=100):\n    \"\"\"Maps values to bins\n    Args:\n    values (list or list of lists) - list of values to map to colors\n    res (int) - resolution of the color map (default: 100)\n    Returns:\n    list of numbers representing bins\n    \"\"\"\n    # flatten if list of lists\n    if any(isinstance(el, list) for el in vals):\n        vals = list(itertools.chain(*vals))\n    return list(np.digitize(vals, np.linspace(np.min(vals), np.max(vals)+1, res+1)) - 1)\n\n\ndef interp_array(arr,interp_val=10):\n    x=np.arange(0, len(arr), 1)\n    xx=np.arange(0, len(arr)-1, 1/interp_val)\n    q=pchip(x,arr)\n    return q(xx)\n\n\ndef interp_array_list(arr_list,interp_val=10):\n    smoothed= [np.zeros(arr_list[0].shape) for item in arr_list]\n    for idx,arr in enumerate(arr_list):\n        smoothed[idx] = interp_array(arr,interp_val)\n    return smoothed\n\n\ndef parse_args(x,args):\n    args_list = []\n    for i,item in enumerate(x):\n        tmp = []\n        for ii, arg in enumerate(args):\n            if isinstance(arg, (tuple, list)):\n                if len(arg) == len(x):\n                    tmp.append(arg[i])\n                else:\n                    print('Error: arguments must be a list of the same length as x')\n                    sys.exit(1)\n            else:\n                tmp.append(arg)\n        args_list.append(tuple(tmp))\n    return args_list\n\n\ndef parse_kwargs(x, kwargs):\n    kwargs_list = []\n    for i,item in enumerate(x):\n        tmp = {}\n        for kwarg in kwargs:\n            if isinstance(kwargs[kwarg], (tuple, list)):\n                if len(kwargs[kwarg]) == len(x):\n                    tmp[kwarg]=kwargs[kwarg][i]\n                else:\n                    tmp[kwarg] = None\n            else:\n                tmp[kwarg]=kwargs[kwarg]\n        kwargs_list.append(tmp)\n    return kwargs_list\n\n\n\n\n# The code to be completed is:\ndef reshape_data(x, hue, labels):\n\n    \"\"\"\n    Reshape the input data based on the hue and labels. It stacks the input data and reshapes it based on the categories in the hue. It also reshapes the labels based on the categories in the hue.\n    Input-Output Arguments\n    :param x: Array. The input data to be reshaped.\n    :param hue: Array. The categories based on which the data is reshaped.\n    :param labels: Array. The labels corresponding to the input data. Defaults to None.\n    :return: List of arrays. The reshaped input data based on the categories in the hue, and the reshaped labels.\n    \"\"\""}
{"namespace": "mrjob.fs.base.Filesystem.join", "prompt": "# Please complete the join function based on the contexts above the function.\n\n# The contexts above the function are:\n# Copyright 2009-2015 Yelp and Contributors\n# Copyright 2017 Yelp\n# Copyright 2019 Yelp\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport logging\nimport os.path\nimport posixpath\n\n\n\n\nlog = logging.getLogger(__name__)\n\n\nclass Filesystem(object):\n    \"\"\"Some simple filesystem operations that are common across the local\n    filesystem, S3, GCS, HDFS, and remote machines via SSH.\n\n    Different runners provide functionality for different filesystems via their\n    :py:attr:`~mrjob.runner.MRJobRunner.fs` attribute. Generally a runner will\n    wrap one or more filesystems with\n    :py:class:`mrjob.fs.composite.CompositeFilesystem`.\n\n    Schemes supported:\n\n    * :py:class:`mrjob.fs.gcs.GCSFilesystem`: ``gs://``\n    * :py:class:`mrjob.fs.hadoop.HadoopFilesystem`: ``hdfs://`` and other URIs\n    * :py:class:`mrjob.fs.local.LocalFilesystem`: paths and ``file://`` URIs\n    * :py:class:`mrjob.fs.s3.S3Filesystem`: ``s3://``, ``s3a://``, ``s3n://``,\n    * :py:class:`mrjob.fs.ssh.SSHFilesystem`: ``ssh://``\n\n    .. versionchanged:: 0.6.12\n\n       `LocalFilesystem` added support for ``file://`` URIs\n    \"\"\"\n    # Note: currently, we're not very consistent about the names of arguments\n    # to these methods in subclasses, which we'll fix in v0.7.0 (see #1979).\n\n    def can_handle_path(self, path):\n        \"\"\"Can we handle this path at all?\"\"\"\n        False\n\n    def cat(self, path_glob):\n        \"\"\"cat all files matching **path_glob**, decompressing if necessary\n\n        This yields bytes, which don't necessarily correspond to lines\n        (see #1544). If multiple files are catted, yields ``b''`` between\n        each file.\n        \"\"\"\n        for i, filename in enumerate(self.ls(path_glob)):\n            if i > 0:\n                yield b''  # mark end of previous file\n\n            for line in self._cat_file(filename):\n                yield line\n\n    def du(self, path_glob):\n        \"\"\"Get the total size of files matching ``path_glob``\n\n        Corresponds roughly to: ``hadoop fs -du path_glob``\n        \"\"\"\n        raise NotImplementedError\n\n    def ls(self, path_glob):\n        \"\"\"Recursively list all files in the given path.\n\n        We don't return directories for compatibility with S3 (which\n        has no concept of them)\n\n        Corresponds roughly to: ``hadoop fs -ls -R path_glob``\n        \"\"\"\n        raise NotImplementedError\n\n    def _cat_file(self, path):\n        \"\"\"Yield the contents of the file at *path* as a series of ``bytes``,\n        not necessarily respecting line boundaries.\"\"\"\n        raise NotImplementedError\n\n    def exists(self, path_glob):\n        \"\"\"Does the given path/URI exist?\n\n        Corresponds roughly to: ``hadoop fs -test -e path_glob``\n        \"\"\"\n        raise NotImplementedError\n\n\n\n# The code to be completed is:\n    def join(self, path, *paths):\n\n        \"\"\"\n        Join multiple paths onto a base path. If the base path is a URI, it only considers the URI and the paths that follow it. It extract the scheme, netloc, and path from the URI, and then join the URI path and the remaining paths. If the base path is not a URI, it join all the paths together.\n        Input-Output Arguments\n        :param self: Filesystem. An instance of the Filesystem class.\n        :param path: String. The base path to join the other paths onto.\n        :param *paths: Tuple of strings. The paths to be joined onto the base path.\n        :return: String. The joined path.\n        \"\"\""}
{"namespace": "feedparser.api._open_resource", "prompt": "# Please complete the _open_resource function based on the contexts above the function.\n\n# The contexts above the function are:\n# The public API for feedparser\n# Copyright 2010-2022 Kurt McKee <contactme@kurtmckee.org>\n# Copyright 2002-2008 Mark Pilgrim\n# All rights reserved.\n#\n# This file is a part of feedparser.\n#\n# Redistribution and use in source and binary forms, with or without modification,\n# are permitted provided that the following conditions are met:\n#\n# * Redistributions of source code must retain the above copyright notice,\n#   this list of conditions and the following disclaimer.\n# * Redistributions in binary form must reproduce the above copyright notice,\n#   this list of conditions and the following disclaimer in the documentation\n#   and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS 'AS IS'\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE\n# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n# POSSIBILITY OF SUCH DAMAGE.\n\nimport io\nimport urllib.error\nimport urllib.parse\nimport xml.sax\n\nfrom .datetimes import registerDateHandler, _parse_date\nfrom .encodings import convert_to_utf8\nfrom .exceptions import *\nfrom .html import _BaseHTMLProcessor\n\nfrom . import mixin\nfrom .mixin import _FeedParserMixin\nfrom .parsers.loose import _LooseFeedParser\nfrom .parsers.strict import _StrictFeedParser\nfrom .sanitizer import replace_doctype\nfrom .sgml import *\nfrom .urls import convert_to_idn, make_safe_absolute_uri\nfrom .util import FeedParserDict\n\n\n# List of preferred XML parsers, by SAX driver name.  These will be tried first,\n# but if they're not installed, Python will keep searching through its own list\n# of pre-installed parsers until it finds one that supports everything we need.\nPREFERRED_XML_PARSERS = [\"drv_libxml2\"]\n\n_XML_AVAILABLE = True\n\nSUPPORTED_VERSIONS = {\n    '': 'unknown',\n    'rss090': 'RSS 0.90',\n    'rss091n': 'RSS 0.91 (Netscape)',\n    'rss091u': 'RSS 0.91 (Userland)',\n    'rss092': 'RSS 0.92',\n    'rss093': 'RSS 0.93',\n    'rss094': 'RSS 0.94',\n    'rss20': 'RSS 2.0',\n    'rss10': 'RSS 1.0',\n    'rss': 'RSS (unknown version)',\n    'atom01': 'Atom 0.1',\n    'atom02': 'Atom 0.2',\n    'atom03': 'Atom 0.3',\n    'atom10': 'Atom 1.0',\n    'atom': 'Atom (unknown version)',\n    'cdf': 'CDF',\n}\n\n\n\n\n# The code to be completed is:\ndef _open_resource(url_file_stream_or_string, etag, modified, agent, referrer, handlers, request_headers, result):\n\n    \"\"\"\n    This function takes in a URL, pathname to local or network file, or string as input and returns a stream object. It provides a uniform way to handle different types of input sources. The returned object has standard read methods (read, readline, readlines) and should be closed using the .close() method when no longer needed.\n    Input-Output Arguments\n    :param url_file_stream_or_string: The input source, which can be a URL, filename, or string.\n    :param etag: The value of the If-None-Match request header. Defaults to None.\n    :param modified: The value of the If-Modified-Since request header. Can be a tuple of 9 integers or a date string. Defaults to None.\n    :param agent: The value of the User-Agent request header. Defaults to None.\n    :param referrer: The value of the Referer request header. Defaults to None.\n    :param handlers: A list of handlers used to build a urllib2 opener. Defaults to None.\n    :param request_headers: A dictionary of HTTP request headers that override the values generated by FeedParser. Defaults to None.\n    :param result: A placeholder for the result. Defaults to None.\n    :return: A bytes object representing the stream.\n    \"\"\""}
{"namespace": "sslyze.plugins.certificate_info._certificate_utils.parse_subject_alternative_name_extension", "prompt": "# Please complete the parse_subject_alternative_name_extension function based on the contexts above the function.\n\n# The contexts above the function are:\nfrom dataclasses import dataclass\nfrom hashlib import sha256\nfrom typing import List, cast\n\nfrom cryptography.hazmat.primitives.serialization import Encoding, PublicFormat\nfrom cryptography.x509 import (\n    ExtensionOID,\n    DNSName,\n    ExtensionNotFound,\n    NameOID,\n    DuplicateExtension,\n    IPAddress,\n    Certificate,\n    SubjectAlternativeName,\n    Name,\n)\n\n\n@dataclass(frozen=True)\nclass SubjectAlternativeNameExtension:\n    dns_names: List[str]\n    ip_addresses: List[str]\n\n\n\n\n# The code to be completed is:\ndef parse_subject_alternative_name_extension(certificate: Certificate) -> SubjectAlternativeNameExtension:\n\n    \"\"\"\n    This function parses the Subject Alternative Name (SAN) extension of a certificate. It retrieves the SAN extension from the certificate and extracts the DNS names and IP addresses from it. It then returns a SubjectAlternativeNameExtension object containing the extracted DNS names and IP addresses.\n    Input-Output Arguments\n    :param certificate: Certificate. The certificate from which to parse the SAN extension.\n    :return: SubjectAlternativeNameExtension. An object containing the extracted DNS names and IP addresses from the SAN extension.\n    \"\"\""}
{"namespace": "sacred.config.signature.Signature.get_free_parameters", "prompt": "# Please complete the get_free_parameters function based on the contexts above the function.\n\n# The contexts above the function are:\n#!/usr/bin/env python\n# coding=utf-8\n\nimport inspect\nfrom inspect import Parameter\nfrom collections import OrderedDict\nfrom sacred.utils import MissingConfigError, SignatureError\n\nARG_TYPES = [\n    Parameter.POSITIONAL_ONLY,\n    Parameter.POSITIONAL_OR_KEYWORD,\n    Parameter.KEYWORD_ONLY,\n]\nPOSARG_TYPES = [Parameter.POSITIONAL_ONLY, Parameter.POSITIONAL_OR_KEYWORD]\n\n\ndef get_argspec(f):\n    sig = inspect.signature(f)\n    args = [n for n, p in sig.parameters.items() if p.kind in ARG_TYPES]\n    pos_args = [\n        n\n        for n, p in sig.parameters.items()\n        if p.kind in POSARG_TYPES and p.default == inspect._empty\n    ]\n    varargs = [\n        n for n, p in sig.parameters.items() if p.kind == Parameter.VAR_POSITIONAL\n    ]\n    # only use first vararg  (how on earth would you have more anyways?)\n    vararg_name = varargs[0] if varargs else None\n\n    varkws = [n for n, p in sig.parameters.items() if p.kind == Parameter.VAR_KEYWORD]\n    # only use first varkw  (how on earth would you have more anyways?)\n    kw_wildcard_name = varkws[0] if varkws else None\n    kwargs = OrderedDict(\n        [\n            (n, p.default)\n            for n, p in sig.parameters.items()\n            if p.default != inspect._empty\n        ]\n    )\n\n    return args, vararg_name, kw_wildcard_name, pos_args, kwargs\n\n\nclass Signature:\n    \"\"\"\n    Extracts and stores information about the signature of a function.\n\n    name : the functions name\n    arguments : list of all arguments\n    vararg_name : name of the *args variable\n    kw_wildcard_name : name of the **kwargs variable\n    positional_args : list of all positional-only arguments\n    kwargs : dict of all keyword arguments mapped to their default\n    \"\"\"\n\n    def __init__(self, f):\n        self.name = f.__name__\n        args, vararg_name, kw_wildcard_name, pos_args, kwargs = get_argspec(f)\n        self.arguments = args\n        self.vararg_name = vararg_name\n        self.kw_wildcard_name = kw_wildcard_name\n        self.positional_args = pos_args\n        self.kwargs = kwargs\n\n\n\n# The code to be completed is:\n    def get_free_parameters(self, args, kwargs, bound=False):\n\n        \"\"\"\n        This function returns a list of free parameters based on the given arguments and keyword arguments. Free parameters are those that need to be filled in by the user.\n        Input-Output Arguments\n        :param self: Signature. An instance of the Signature class.\n        :param args: list. The positional arguments passed to the function.\n        :param kwargs: dict. The keyword arguments passed to the function.\n        :param bound: bool. Whether the signature is bound to an instance or not. Defaults to False.\n        :return: List[str]. The list of free parameters.\n        \"\"\""}
{"namespace": "twilio.jwt.client.ClientCapabilityToken.allow_event_stream", "prompt": "# Please complete the allow_event_stream function based on the contexts above the function.\n\n# The contexts above the function are:\nfrom twilio.jwt import Jwt\n\nfrom urllib.parse import urlencode\n\n\nclass ClientCapabilityToken(Jwt):\n    \"\"\"A token to control permissions with Twilio Client\"\"\"\n\n    ALGORITHM = \"HS256\"\n\n    def __init__(\n        self,\n        account_sid,\n        auth_token,\n        nbf=Jwt.GENERATE,\n        ttl=3600,\n        valid_until=None,\n        **kwargs\n    ):\n        \"\"\"\n        :param str account_sid: The account sid to which this token is granted access.\n        :param str auth_token: The secret key used to sign the token. Note, this auth token is not\n                               visible to the user of the token.\n        :param int nbf: Time in secs from epic before which this token is considered invalid.\n        :param int ttl: the amount of time in seconds from generation that this token is valid for.\n        :param kwargs:\n\n\n        :returns: A new CapabilityToken with zero permissions\n        \"\"\"\n        super(ClientCapabilityToken, self).__init__(\n            algorithm=self.ALGORITHM,\n            secret_key=auth_token,\n            issuer=account_sid,\n            nbf=nbf,\n            ttl=ttl,\n            valid_until=None,\n        )\n\n        self.account_sid = account_sid\n        self.auth_token = auth_token\n        self.client_name = None\n        self.capabilities = {}\n\n        if \"allow_client_outgoing\" in kwargs:\n            self.allow_client_outgoing(**kwargs[\"allow_client_outgoing\"])\n        if \"allow_client_incoming\" in kwargs:\n            self.allow_client_incoming(**kwargs[\"allow_client_incoming\"])\n        if \"allow_event_stream\" in kwargs:\n            self.allow_event_stream(**kwargs[\"allow_event_stream\"])\n\n    def allow_client_outgoing(self, application_sid, **kwargs):\n        \"\"\"\n        Allow the user of this token to make outgoing connections. Keyword arguments are passed\n        to the application.\n\n        :param str application_sid: Application to contact\n        \"\"\"\n        scope = ScopeURI(\"client\", \"outgoing\", {\"appSid\": application_sid})\n        if kwargs:\n            scope.add_param(\"appParams\", urlencode(kwargs, doseq=True))\n\n        self.capabilities[\"outgoing\"] = scope\n\n    def allow_client_incoming(self, client_name):\n        \"\"\"\n        Allow the user of this token to accept incoming connections.\n\n        :param str client_name: Client name to accept calls from\n        \"\"\"\n        self.client_name = client_name\n        self.capabilities[\"incoming\"] = ScopeURI(\n            \"client\", \"incoming\", {\"clientName\": client_name}\n        )\n\n\n\n# The code to be completed is:\n    def allow_event_stream(self, **kwargs):\n\n        \"\"\"\n        This function allows the user of the ClientCapabilityToken instance to access their event stream. It creates a scope URI with the necessary parameters and adds it to the capabilities dictionary of the instance.\n        Input-Output Arguments\n        :param self: ClientCapabilityToken. An instance of the ClientCapabilityToken class.\n        :param kwargs: Keyword arguments that can be used to specify additional parameters for the event stream.\n        :return: No return values.\n        \"\"\""}
{"namespace": "chatette.parsing.UnitRefBuilder.create_concrete", "prompt": "# Please complete the create_concrete function based on the contexts above the function.\n\n# The contexts above the function are:\n# coding: utf-8\n\"\"\"\nModule `chatette.parsing`\nContains everything that is related to the management and parsing\nof the template file(s).\nThe most important classes defined in this module are:\n- Parser, which runs the whole parsing of template files.\n- Lexer, in charge of \"lexing\" the information present in those files.\n- All the lexing rules the lexer will use.\n- InputFileManager, which manages the opening, closing and read of those files.\n- ItemBuilders that are used by the parser to create concrete items.\n\"\"\"\n# TODO Add LineCountFileWrapper in here\n\nfrom abc import ABCMeta, abstractmethod\nfrom future.utils import with_metaclass\n\n\n\n\n\n\n\nfrom chatette.modifiers.representation import \\\n    ModifiersRepresentation, RandgenRepresentation\n\nfrom chatette.units.ast import AST\nfrom chatette.utils import UnitType\n\n\nclass ItemBuilder(with_metaclass(ABCMeta, object)):\n    \"\"\"\n    An intermediate representation of generating items that are used by the\n    parser. It is able to construct the corresponding item once it has\n    all the required information.\n    NOTE: This does not correspond to the *Builder* design pattern.\n    \"\"\"\n    def __init__(self):\n        self.leading_space = False\n        self.casegen = False\n        self.randgen = False\n        self.randgen_name = None\n        self.randgen_opposite = False\n        self.randgen_percent = 50\n\n    def _check_information(self):\n        if not self.randgen and self.randgen_name is not None:  # Should never happen\n            raise ValueError(\n                \"There was a problem with some modifiers: detected \" + \\\n                \"a random generation modifier name but no \" + \\\n                \"random generation modifier.\"\n            )\n\n    def _build_modifiers_repr(self):\n        \"\"\"\n        Returns an instance of `ModifiersRepresentation` that corresponds\n        to the modifiers set in `self`.\n        \"\"\"\n        modifiers = ModifiersRepresentation()\n        modifiers.casegen = self.casegen\n\n        randgen = RandgenRepresentation()\n        randgen._present = self.randgen\n        randgen.name = self.randgen_name\n        randgen.opposite = self.randgen_opposite\n        randgen.percentage = self.randgen_percent\n        modifiers.randgen = randgen\n\n        return modifiers\n\n    @abstractmethod\n    def create_concrete(self):\n        raise NotImplementedError()\n\nclass ChoiceBuilder(ItemBuilder):\n    def __init__(self):\n        super(ChoiceBuilder, self).__init__()\n        self.rules = []\n\n    def create_concrete(self):\n        from chatette.units.modifiable.choice import Choice\n        self._check_information()\n        return Choice(\n            self.leading_space, self._build_modifiers_repr(),\n            self.rules\n        )\n\nclass UnitRefBuilder(ItemBuilder):\n    def __init__(self):\n        super(UnitRefBuilder, self).__init__()\n        self.type = None\n        self.identifier = None\n        self.variation = None\n        self.arg_value = None\n\n    def _check_information(self):\n        super(UnitRefBuilder, self)._check_information()\n        if self.type is None or self.identifier is None:  # Should never happen\n            raise ValueError(\n                \"Tried to create a concrete unit reference without setting \" + \\\n                \"its identifier or type.\"\n            )\n\n    def _build_modifiers_repr(self):\n        modifiers = super(UnitRefBuilder, self)._build_modifiers_repr()\n        modifiers.argument_value = self.arg_value\n        modifiers.variation_name = self.variation\n        return modifiers\n\n\n\n# The code to be completed is:\n    def create_concrete(self):\n\n        \"\"\"\n        Create a concrete UnitReference object based on the information stored in the UnitRefBuilder instance. It first checks if all the necessary information is available, and then uses that information to create the UnitReference object.\n        Input-Output Arguments\n        :param self: UnitRefBuilder. An instance of the UnitRefBuilder class.\n        :return: UnitReference. The created UnitReference object.\n        \"\"\""}
{"namespace": "pycorrector.en_spell.EnSpell.correct_word", "prompt": "# Please complete the correct_word function based on the contexts above the function.\n\n# The contexts above the function are:\n# -*- coding: utf-8 -*-\n\"\"\"\n@author:XuMing(xuming624@qq.com)\n@description: english correction\nrefer: http://norvig.com/spell-correct.html\n\"\"\"\n\nimport gzip\nimport json\nimport operator\nimport os\nfrom codecs import open\nfrom collections import Counter\nfrom loguru import logger\nfrom pycorrector import config\n\n\n\n\ndef get_word_freq_dict_from_text(text):\n    from pycorrector.utils.tokenizer import whitespace_tokenize\n    return Counter(whitespace_tokenize(text))\n\n\nclass EnSpell(object):\n    def __init__(self, word_freq_dict={}):\n        # Word freq dict, k=word, v=int(freq)\n        self.word_freq_dict = word_freq_dict\n        self.custom_confusion = {}\n\n    def _init(self):\n        with gzip.open(config.en_dict_path, \"rb\") as f:\n            all_word_freq_dict = json.loads(f.read())\n            word_freq = {}\n            for k, v in all_word_freq_dict.items():\n                # 3400\n                if v > 400:\n                    word_freq[k] = v\n            self.word_freq_dict = word_freq\n            logger.debug(\"load en spell data: %s, size: %d\" % (config.en_dict_path,\n                                                               len(self.word_freq_dict)))\n\n    def check_init(self):\n        if not self.word_freq_dict:\n            self._init()\n\n    @staticmethod\n    def edits1(word):\n        \"\"\"\n        all edits that are one edit away from 'word'\n        :param word:\n        :return:\n        \"\"\"\n        letters = 'abcdefghijklmnopqrstuvwxyz'\n        splits = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n        deletes = [L + R[1:] for L, R in splits if R]\n        transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R) > 1]\n        replaces = [L + c + R[1:] for L, R in splits if R for c in letters]\n        inserts = [L + c + R for L, R in splits for c in letters]\n        return set(deletes + transposes + replaces + inserts)\n\n    def edits2(self, word):\n        \"\"\"\n        all edit that are two edits away from 'word'\n        :param word:\n        :return:\n        \"\"\"\n        return (e2 for e1 in self.edits1(word) for e2 in self.edits1(e1))\n\n    def known(self, word_freq_dict):\n        \"\"\"\n        the subset of 'word_freq_dict' that appear in the dictionary of word_freq_dict\n        :param word_freq_dict:\n        :param limit_count:\n        :return:\n        \"\"\"\n        self.check_init()\n        return set(w for w in word_freq_dict if w in self.word_freq_dict)\n\n    def probability(self, word):\n        \"\"\"\n        probability of word\n        :param word:\n        :return:float\n        \"\"\"\n        self.check_init()\n        N = sum(self.word_freq_dict.values())\n        return self.word_freq_dict.get(word, 0) / N\n\n    def candidates(self, word):\n        \"\"\"\n        generate possible spelling corrections for word.\n        :param word:\n        :return:\n        \"\"\"\n        self.check_init()\n        return self.known([word]) or self.known(self.edits1(word)) or self.known(self.edits2(word)) or {word}\n\n\n\n# The code to be completed is:\n    def correct_word(self, word):\n\n        \"\"\"\n        This function corrects the spelling of a given word by finding the most probable spelling correction. It first checks if the EnSpell instance has been initialized. Then, it calculates the probability of each candidate spelling correction for the word and sorts them in ascending order. Finally, it returns the correction with the highest probability.\n        Input-Output Arguments\n        :param self: EnSpell. An instance of the EnSpell class.\n        :param word: String. The word to be corrected.\n        :return: String. The most probable spelling correction for the word.\n        \"\"\""}
{"namespace": "boto.cloudsearch.connect_to_region", "prompt": "# Please complete the connect_to_region function based on the contexts above the function.\n\n# The contexts above the function are:\n# Copyright (c) 2012 Mitch Garnaat http://garnaat.org/\n# Copyright (c) 2012 Amazon.com, Inc. or its affiliates.\n# All Rights Reserved\n#\n# Permission is hereby granted, free of charge, to any person obtaining a\n# copy of this software and associated documentation files (the\n# \"Software\"), to deal in the Software without restriction, including\n# without limitation the rights to use, copy, modify, merge, publish, dis-\n# tribute, sublicense, and/or sell copies of the Software, and to permit\n# persons to whom the Software is furnished to do so, subject to the fol-\n# lowing conditions:\n#\n# The above copyright notice and this permission notice shall be included\n# in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS\n# OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABIL-\n# ITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT\n# SHALL THE AUTHOR BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY,\n# WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS\n# IN THE SOFTWARE.\n#\nfrom boto.regioninfo import RegionInfo, get_regions\n\n\n\ndef regions():\n    \"\"\"\n    Get all available regions for the Amazon CloudSearch service.\n\n    :rtype: list\n    :return: A list of :class:`boto.regioninfo.RegionInfo`\n    \"\"\"\n    from boto.cloudsearch.layer1 import Layer1\n    return get_regions('cloudsearch',connection_cls=Layer1)\n\n\n\n\n# The code to be completed is:\ndef connect_to_region(region_name, **kw_params):\n\n    \"\"\"\n    Connect to a specific region in the cloudsearch service. It creates a connection to the cloudsearch service in the specified region.\n    Input-Output Arguments\n    :param region_name: String. The name of the region to connect to.\n    :param **kw_params: Additional keyword arguments that can be passed to the connect function.\n    :return: Connection. The connection object to the cloudsearch service in the specified region.\n    \"\"\""}
{"namespace": "faker.utils.decorators.slugify_unicode", "prompt": "# Please complete the slugify_unicode function based on the contexts above the function.\n\n# The contexts above the function are:\nfrom functools import wraps\nfrom typing import Callable, Dict, Tuple, TypeVar\n\nfrom ..utils import text\n\nT = TypeVar(\"T\")\n\n\ndef slugify(fn: Callable) -> Callable:\n    @wraps(fn)\n    def wrapper(*args: Tuple[T, ...], **kwargs: Dict[str, T]) -> str:\n        return text.slugify(fn(*args, **kwargs))\n\n    return wrapper\n\n\ndef slugify_domain(fn: Callable) -> Callable:\n    @wraps(fn)\n    def wrapper(*args: Tuple[T, ...], **kwargs: Dict[str, T]) -> str:\n        return text.slugify(fn(*args, **kwargs), allow_dots=True)\n\n    return wrapper\n\n\n\n\n# The code to be completed is:\ndef slugify_unicode(fn: Callable) -> Callable:\n    @wraps(fn)\n\n    \"\"\"\n    This function is a decorator that wraps the input function and returns a new function. The new function slugifies the output of the input function and returns the slugified string.\n    Input-Output Arguments\n    :param fn: Callable. The input function to be wrapped and modified.\n    :return: Callable. The wrapper function that slugifies the output of the input function.\n    \"\"\""}
{"namespace": "mopidy.config.types.Integer.deserialize", "prompt": "# Please complete the deserialize function based on the contexts above the function.\n\n# The contexts above the function are:\nimport logging\nimport re\nimport socket\n\nfrom mopidy.config import validators\nfrom mopidy.internal import log, path\n\n\ndef decode(value):\n    if isinstance(value, bytes):\n        value = value.decode(errors=\"surrogateescape\")\n\n    for char in (\"\\\\\", \"\\n\", \"\\t\"):\n        value = value.replace(\n            char.encode(encoding=\"unicode-escape\").decode(), char\n        )\n\n    return value\n\n\ndef encode(value):\n    if isinstance(value, bytes):\n        value = value.decode(errors=\"surrogateescape\")\n\n    for char in (\"\\\\\", \"\\n\", \"\\t\"):\n        value = value.replace(\n            char, char.encode(encoding=\"unicode-escape\").decode()\n        )\n\n    return value\n\n\nclass DeprecatedValue:\n    pass\n\n\nclass _TransformedValue(str):\n    def __new__(cls, original, transformed):\n        return super().__new__(cls, transformed)\n\n    def __init__(self, original, transformed):\n        self.original = original\n\n\nclass ConfigValue:\n    \"\"\"Represents a config key's value and how to handle it.\n\n    Normally you will only be interacting with sub-classes for config values\n    that encode either deserialization behavior and/or validation.\n\n    Each config value should be used for the following actions:\n\n    1. Deserializing from a raw string and validating, raising ValueError on\n       failure.\n    2. Serializing a value back to a string that can be stored in a config.\n    3. Formatting a value to a printable form (useful for masking secrets).\n\n    :class:`None` values should not be deserialized, serialized or formatted,\n    the code interacting with the config should simply skip None config values.\n    \"\"\"\n\n    def deserialize(self, value):\n        \"\"\"Cast raw string to appropriate type.\"\"\"\n        return decode(value)\n\n    def serialize(self, value, display=False):\n        \"\"\"Convert value back to string for saving.\"\"\"\n        if value is None:\n            return \"\"\n        return str(value)\n\n\nclass Deprecated(ConfigValue):\n    \"\"\"Deprecated value.\n\n    Used for ignoring old config values that are no longer in use, but should\n    not cause the config parser to crash.\n    \"\"\"\n\n    def deserialize(self, value):\n        return DeprecatedValue()\n\n    def serialize(self, value, display=False):\n        return DeprecatedValue()\n\n\nclass String(ConfigValue):\n    \"\"\"String value.\n\n    Is decoded as utf-8 and \\\\n \\\\t escapes should work and be preserved.\n    \"\"\"\n\n    def __init__(self, optional=False, choices=None, transformer=None):\n        self._required = not optional\n        self._choices = choices\n        self._transformer = transformer\n\n    def deserialize(self, value):\n        value = decode(value).strip()\n        validators.validate_required(value, self._required)\n        if not value:\n            return None\n\n        # This is necessary for backwards-compatibility, in case subclasses\n        # aren't calling their parent constructor.\n        transformer = getattr(self, \"_transformer\", None)\n        if transformer:\n            transformed_value = transformer(value)\n            value = _TransformedValue(value, transformed_value)\n\n        validators.validate_choice(value, self._choices)\n        return value\n\n    def serialize(self, value, display=False):\n        if value is None:\n            return \"\"\n        if isinstance(value, _TransformedValue):\n            value = value.original\n        return encode(value)\n\n\nclass Secret(String):\n    \"\"\"Secret string value.\n\n    Is decoded as utf-8 and \\\\n \\\\t escapes should work and be preserved.\n\n    Should be used for passwords, auth tokens etc. Will mask value when being\n    displayed.\n    \"\"\"\n\n    def __init__(self, optional=False, choices=None, transformer=None):\n        super().__init__(\n            optional=optional,\n            choices=None,  # Choices doesn't make sense for secrets\n            transformer=transformer,\n        )\n\n    def serialize(self, value, display=False):\n        if value is not None and display:\n            return \"********\"\n        return super().serialize(value, display)\n\n\nclass Integer(ConfigValue):\n    \"\"\"Integer value.\"\"\"\n\n    def __init__(\n        self, minimum=None, maximum=None, choices=None, optional=False\n    ):\n        self._required = not optional\n        self._minimum = minimum\n        self._maximum = maximum\n        self._choices = choices\n\n\n\n# The code to be completed is:\n    def deserialize(self, value):\n\n        \"\"\"\n        Deserialize a value into an Integer object. It decodes the input value, validates it based on the specified constraints, and returns the deserialized integer value.\n        Input-Output Arguments\n        :param self: Integer. An instance of the Integer class.\n        :param value: The value to be deserialized.\n        :return: int. The deserialized integer value.\n        \"\"\""}
{"namespace": "mopidy.internal.validation.check_instance", "prompt": "# Please complete the check_instance function based on the contexts above the function.\n\n# The contexts above the function are:\nimport urllib\nfrom collections.abc import Iterable, Mapping\n\nfrom mopidy import exceptions\n\nPLAYBACK_STATES = {\"paused\", \"stopped\", \"playing\"}\n\nTRACK_FIELDS_WITH_TYPES = {\n    \"uri\": str,\n    \"track_name\": str,\n    \"album\": str,\n    \"artist\": str,\n    \"albumartist\": str,\n    \"composer\": str,\n    \"performer\": str,\n    \"track_no\": int,\n    \"genre\": str,\n    \"date\": str,\n    \"comment\": str,\n    \"disc_no\": int,\n    \"musicbrainz_albumid\": str,\n    \"musicbrainz_artistid\": str,\n    \"musicbrainz_trackid\": str,\n}\n\nSEARCH_FIELDS = set(TRACK_FIELDS_WITH_TYPES).union({\"any\"})\n\nPLAYLIST_FIELDS = {\"uri\", \"name\"}  # TODO: add length and last_modified?\n\nTRACKLIST_FIELDS = {  # TODO: add bitrate, length, disc_no, track_no, modified?\n    \"uri\",\n    \"name\",\n    \"genre\",\n    \"date\",\n    \"comment\",\n    \"musicbrainz_id\",\n}\n\nDISTINCT_FIELDS = dict(TRACK_FIELDS_WITH_TYPES)\n\n\n# TODO: _check_iterable(check, msg, **kwargs) + [check(a) for a in arg]?\ndef _check_iterable(arg, msg, **kwargs):\n    \"\"\"Ensure we have an iterable which is not a string or an iterator\"\"\"\n    if isinstance(arg, str):\n        raise exceptions.ValidationError(msg.format(arg=arg, **kwargs))\n    elif not isinstance(arg, Iterable):\n        raise exceptions.ValidationError(msg.format(arg=arg, **kwargs))\n    elif iter(arg) is iter(arg):\n        raise exceptions.ValidationError(msg.format(arg=arg, **kwargs))\n\n\ndef check_choice(arg, choices, msg=\"Expected one of {choices}, not {arg!r}\"):\n    if arg not in choices:\n        raise exceptions.ValidationError(\n            msg.format(arg=arg, choices=tuple(choices))\n        )\n\n\ndef check_boolean(arg, msg=\"Expected a boolean, not {arg!r}\"):\n    check_instance(arg, bool, msg=msg)\n\n\n\n\n# The code to be completed is:\ndef check_instance(arg, cls, msg=\"Expected a {name} instance, not {arg!r}\"):\n\n    \"\"\"\n    Check if the given argument is an instance of a specified class. If not, raise a validation error with a customizable error message.\n    Input-Output Arguments\n    :param arg: Any. The argument to be checked.\n    :param cls: Class. The class to check against.\n    :param msg: String. The error message to be displayed if the argument is not an instance of the class. It defaults to \"Expected a {name} instance, not {arg!r}\".\n    :return: No return values.\n    \"\"\""}
{"namespace": "benedict.utils.type_util.is_json_serializable", "prompt": "# Please complete the is_json_serializable function based on the contexts above the function.\n\n# The contexts above the function are:\nimport pathlib\nimport re\nfrom datetime import datetime\nfrom decimal import Decimal\n\nregex = re.compile(\"\").__class__\nuuid_re = re.compile(\n    \"^([0-9a-f]{32}){1}$|^([0-9a-f]{8}-([0-9a-f]{4}-){3}[0-9a-f]{12}){1}$\",\n    flags=re.IGNORECASE,\n)\n\n\ndef is_bool(val):\n    return isinstance(val, bool)\n\n\ndef is_collection(val):\n    return isinstance(val, (dict, list, set, tuple))\n\n\ndef is_datetime(val):\n    return isinstance(val, datetime)\n\n\ndef is_decimal(val):\n    return isinstance(val, Decimal)\n\n\ndef is_dict(val):\n    return isinstance(val, dict)\n\n\ndef is_dict_or_list(val):\n    return isinstance(val, (dict, list))\n\n\ndef is_dict_or_list_or_tuple(val):\n    return isinstance(val, (dict, list, tuple))\n\n\ndef is_float(val):\n    return isinstance(val, float)\n\n\ndef is_function(val):\n    return callable(val)\n\n\ndef is_integer(val):\n    return isinstance(val, int)\n\n\n\n\n# The code to be completed is:\ndef is_json_serializable(val):\n\n    \"\"\"\n    Check if the input value is JSON serializable. It checks if the input value is of the JSON serializable types.\n    Input-Output Arguments\n    :param val: Any. The input value to be checked for JSON serializability.\n    :return: Bool. True if the input value is JSON serializable, False otherwise.\n    \"\"\""}
{"namespace": "mrjob.step.StepFailedException.__str__", "prompt": "# Please complete the __str__ function based on the contexts above the function.\n\n# The contexts above the function are:\n# Copyright 2012 Yelp and Contributors\n# Copyright 2013 David Marin and Contributors\n# Copyright 2015-2018 Yelp\n# Copyright 2019 Yelp\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Representations of job steps, to use in your :py:class:`~mrjob.job.MRJob`'s\n:py:meth:`~mrjob.job.MRJob.steps` method.\n\nBecause :py:class:`the runner <mrjob.runner.MRJobRunner>` just needs to know\nhow to invoke your MRJob script, not how it works insternally, each step\ninstance's ``description()`` method produces a simplified, JSON-able\ndescription of the step, to pass to the runner.\n\"\"\"\nimport logging\n\nfrom mrjob.py2 import string_types\nfrom mrjob.util import cmd_line\n\n\nSTEP_TYPES = ('jar', 'spark', 'spark_jar', 'spark_script', 'streaming')\n\n# Function names mapping to mapper, reducer, and combiner operations\n_MAPPER_FUNCS = ('mapper', 'mapper_init', 'mapper_final', 'mapper_cmd',\n                 'mapper_pre_filter', 'mapper_raw')\n_COMBINER_FUNCS = ('combiner', 'combiner_init', 'combiner_final',\n                   'combiner_cmd', 'combiner_pre_filter')\n_REDUCER_FUNCS = ('reducer', 'reducer_init', 'reducer_final', 'reducer_cmd',\n                  'reducer_pre_filter')\n_HADOOP_OPTS = ('jobconf',)\n\n# params to specify how to run the step. need at least one of these\n_JOB_STEP_FUNC_PARAMS = _MAPPER_FUNCS + _COMBINER_FUNCS + _REDUCER_FUNCS\n# all allowable MRStep params\n_JOB_STEP_PARAMS = _JOB_STEP_FUNC_PARAMS + _HADOOP_OPTS\n\n# all allowable JarStep constructor keyword args\n_JAR_STEP_KWARGS = ['args', 'main_class']\n\n# all allowable SparkStep constructor keyword args\n_SPARK_STEP_KWARGS = ['spark', 'spark_args']\n\n# all allowable SparkJarStep constructor keyword args\n_SPARK_JAR_STEP_KWARGS = ['args', 'jar', 'main_class', 'spark_args']\n\n# all allowable SparkScriptStep constructor keyword args\n_SPARK_SCRIPT_STEP_KWARGS = ['args', 'script', 'spark_args']\n\n\n#: If passed as an argument to :py:class:`JarStep`, :py:class:`SparkJarStep`,\n#: or :py:class:`SparkScriptStep`, it'll be replaced with the step's input\n#: path(s). If there are multiple paths, they'll be joined with commas.\nINPUT = '<input>'\n\n#: If this is passed as an argument to :py:class:`JarStep`,\n#: :py:class:`SparkJarStep`, or :py:class:`SparkScriptStep`, it'll be replaced\n#: with the step's output path\nOUTPUT = '<output>'\n\n#: If this is passed as an argument to :py:class:`JarStep`,\n#: it'll be replaced with generic hadoop args (-D and -libjars)\nGENERIC_ARGS = '<generic args>'\n\n\nlog = logging.getLogger(__name__)\n\n\n# used by MRStep below, to fake no mapper\ndef _IDENTITY_MAPPER(key, value):\n    yield key, value\n\n\n# used by MRStep below, to fake no reducer\ndef _IDENTITY_REDUCER(key, values):\n    for value in values:\n        yield key, value\n\n\nclass StepFailedException(Exception):\n    \"\"\"Exception to throw when a step fails.\n\n    This will automatically be caught\n    and converted to an error message by :py:meth:`mrjob.job.MRJob.run`, but\n    you may wish to catch it if you\n    :ref:`run your job programatically <runners-programmatically>`.\n    \"\"\"\n    _FIELDS = ('reason', 'step_num', 'num_steps', 'step_desc')\n\n    def __init__(\n            self, reason=None, step_num=None, num_steps=None,\n            step_desc=None, last_step_num=None):\n        \"\"\"Initialize a reason for step failure.\n\n        :param string reason: brief explanation of which step failed\n        :param int step_num: which step failed (0-indexed)\n        :param int num_steps: number of steps in the job\n        :param string step_desc: description of step (if we don't like the\n                                 default \"Step X of Y\")\n        :param int last_step_num: if one of a range of steps failed, the\n                                  (0-indexed) last step in that range. If this\n                                  equals *step_num*, will be ignored.\n\n        *reason* should not be several lines long; use ``log.error(...)``\n        for that.\n        \"\"\"\n        self.reason = reason\n        self.step_num = step_num\n        self.num_steps = num_steps\n        self.step_desc = step_desc\n\n        # we only need this for streaming steps run by the Spark harness,\n        # so don't create noise\n        if last_step_num is None or last_step_num == step_num:\n            self.last_step_num = None\n        else:\n            self.last_step_num = last_step_num\n\n\n\n# The code to be completed is:\n    def __str__(self):\n\n        \"\"\"\n        This function returns a human-readable version of the StepFailedException exception. If the reason is available, it returns '{step description} failed: {reason}'. Otherwise, it returns '{step description} failed'. If the step description is not available, it will generate a step description based on the step number. If the step number is not available, it will use 'Step' as the step description. If the total number of steps is available, it will use '{step name} of {total number of steps}' as the step description; otherwise it will use the step name as the step description. If the last step number is available, it will use 'Steps {step number + 1}-{last step number + 1}' as the step description; otherwise it will use 'Step {step number + 1}' as the step description.\n        Input-Output Arguments\n        :param self: StepFailedException. An instance of the StepFailedException class.\n        :return: str. A human-readable version of the exception.\n        \"\"\""}
{"namespace": "sumy.summarizers.edmundson.EdmundsonSummarizer.location_method", "prompt": "# Please complete the location_method function based on the contexts above the function.\n\n# The contexts above the function are:\n# -*- coding: utf-8 -*-\n\nfrom __future__ import absolute_import\nfrom __future__ import division, print_function, unicode_literals\n\nfrom collections import defaultdict\nfrom ..nlp.stemmers import null_stemmer\nfrom ._summarizer import AbstractSummarizer\nfrom .edmundson_cue import EdmundsonCueMethod\nfrom .edmundson_key import EdmundsonKeyMethod\nfrom .edmundson_title import EdmundsonTitleMethod\nfrom .edmundson_location import EdmundsonLocationMethod\n\n\n_EMPTY_SET = frozenset()\n\n\nclass EdmundsonSummarizer(AbstractSummarizer):\n    _bonus_words = _EMPTY_SET\n    _stigma_words = _EMPTY_SET\n    _null_words = _EMPTY_SET\n\n    def __init__(self, stemmer=null_stemmer, cue_weight=1.0, key_weight=0.0,\n            title_weight=1.0, location_weight=1.0):\n        super(EdmundsonSummarizer, self).__init__(stemmer)\n\n        self._ensure_correct_weights(cue_weight, key_weight, title_weight,\n            location_weight)\n\n        self._cue_weight = float(cue_weight)\n        self._key_weight = float(key_weight)\n        self._title_weight = float(title_weight)\n        self._location_weight = float(location_weight)\n\n    def _ensure_correct_weights(self, *weights):\n        for w in weights:\n            if w < 0.0:\n                raise ValueError(\"Negative weights are not allowed.\")\n\n    @property\n    def bonus_words(self):\n        return self._bonus_words\n\n    @bonus_words.setter\n    def bonus_words(self, collection):\n        self._bonus_words = frozenset(map(self.stem_word, collection))\n\n    @property\n    def stigma_words(self):\n        return self._stigma_words\n\n    @stigma_words.setter\n    def stigma_words(self, collection):\n        self._stigma_words = frozenset(map(self.stem_word, collection))\n\n    @property\n    def null_words(self):\n        return self._null_words\n\n    @null_words.setter\n    def null_words(self, collection):\n        self._null_words = frozenset(map(self.stem_word, collection))\n\n    def __call__(self, document, sentences_count):\n        ratings = defaultdict(int)\n\n        if self._cue_weight > 0.0:\n            method = self._build_cue_method_instance()\n            ratings = self._update_ratings(ratings, method.rate_sentences(document))\n        if self._key_weight > 0.0:\n            method = self._build_key_method_instance()\n            ratings = self._update_ratings(ratings, method.rate_sentences(document))\n        if self._title_weight > 0.0:\n            method = self._build_title_method_instance()\n            ratings = self._update_ratings(ratings, method.rate_sentences(document))\n        if self._location_weight > 0.0:\n            method = self._build_location_method_instance()\n            ratings = self._update_ratings(ratings, method.rate_sentences(document))\n\n        return self._get_best_sentences(document.sentences, sentences_count, ratings)\n\n    def _update_ratings(self, ratings, new_ratings):\n        assert len(ratings) == 0 or len(ratings) == len(new_ratings)\n\n        for sentence, rating in new_ratings.items():\n            ratings[sentence] += rating\n\n        return ratings\n\n    def cue_method(self, document, sentences_count, bonus_word_value=1, stigma_word_value=1):\n        summarization_method = self._build_cue_method_instance()\n        return summarization_method(document, sentences_count, bonus_word_value,\n            stigma_word_value)\n\n    def _build_cue_method_instance(self):\n        self.__check_bonus_words()\n        self.__check_stigma_words()\n\n        return EdmundsonCueMethod(self._stemmer, self._bonus_words, self._stigma_words)\n\n    def key_method(self, document, sentences_count, weight=0.5):\n        summarization_method = self._build_key_method_instance()\n        return summarization_method(document, sentences_count, weight)\n\n    def _build_key_method_instance(self):\n        self.__check_bonus_words()\n\n        return  EdmundsonKeyMethod(self._stemmer, self._bonus_words)\n\n    def title_method(self, document, sentences_count):\n        summarization_method = self._build_title_method_instance()\n        return summarization_method(document, sentences_count)\n\n    def _build_title_method_instance(self):\n        self.__check_null_words()\n\n        return EdmundsonTitleMethod(self._stemmer, self._null_words)\n\n\n\n# The code to be completed is:\n    def location_method(self, document, sentences_count, w_h=1, w_p1=1, w_p2=1, w_s1=1, w_s2=1):\n\n        \"\"\"\n        This function applies the location-based method for text summarization. It creates an instance of the location-based method and uses it to summarize the given document based on the specified parameters.\n        Input-Output Arguments\n        :param self: EdmundsonSummarizer. An instance of the EdmundsonSummarizer class.\n        :param document: Document. The document to be summarized.\n        :param sentences_count: Integer. The number of sentences to include in the summary.\n        :param w_h: Integer. The weight for the frequency term in a sentence. Defaults to 1.\n        :param w_p1: Integer. The weight for the first paragraph. Defaults to 1.\n        :param w_p2: Integer. The weight for the last paragraph. Defaults to 1.\n        :param w_s1: Integer. The weight for the first sentence. Defaults to 1.\n        :param w_s2: Integer. The weight for the last sentence. Defaults to 1.\n        :return: Tuple. The summary of the document using the location-based method.\n        \"\"\""}
{"namespace": "kinto.core.utils.dict_subset", "prompt": "# Please complete the dict_subset function based on the contexts above the function.\n\n# The contexts above the function are:\nimport collections.abc as collections_abc\nimport hashlib\nimport hmac\nimport os\nimport re\nimport time\nfrom base64 import b64decode, b64encode\nfrom binascii import hexlify\nfrom enum import Enum\nfrom urllib.parse import unquote\n\nimport jsonpatch\nimport rapidjson\nfrom colander import null\nfrom cornice import cors\nfrom pyramid import httpexceptions\nfrom pyramid.authorization import Authenticated\nfrom pyramid.interfaces import IRoutesMapper\nfrom pyramid.request import Request, apply_request_extensions\nfrom pyramid.settings import aslist\nfrom pyramid.view import render_view_to_response\n\ntry:\n    import sqlalchemy\nexcept ImportError:  # pragma: no cover\n    sqlalchemy = None\n\ntry:\n    import memcache\nexcept ImportError:  # pragma: no cover\n    memcache = None\n\n\nclass json:\n    def dumps(v, **kw):\n        kw.setdefault(\"bytes_mode\", rapidjson.BM_NONE)\n        return rapidjson.dumps(v, **kw)\n\n    def load(v, **kw):\n        kw.setdefault(\"number_mode\", rapidjson.NM_NATIVE)\n        return rapidjson.load(v, **kw)\n\n    def loads(v, **kw):\n        kw.setdefault(\"number_mode\", rapidjson.NM_NATIVE)\n        return rapidjson.loads(v, **kw)\n\n\njson_serializer = json.dumps\n\n\ndef strip_whitespace(v):\n    \"\"\"Remove whitespace, newlines, and tabs from the beginning/end\n    of a string.\n\n    :param str v: the string to strip.\n    :rtype: str\n    \"\"\"\n    return v.strip(\" \\t\\n\\r\") if v is not null else v\n\n\ndef msec_time():\n    \"\"\"Return current epoch time in milliseconds.\n\n    :rtype: int\n    \"\"\"\n    return int(time.time() * 1000.0)  # floor\n\n\ndef classname(obj):\n    \"\"\"Get a classname from an object.\n\n    :rtype: str\n    \"\"\"\n    return obj.__class__.__name__.lower()\n\n\ndef merge_dicts(a, b):\n    \"\"\"Merge b into a recursively, without overwriting values.\n\n    :param dict a: the dict that will be altered with values of `b`.\n    \"\"\"\n    for k, v in b.items():\n        if isinstance(v, dict):\n            merge_dicts(a.setdefault(k, {}), v)\n        else:\n            a.setdefault(k, v)\n\n\ndef recursive_update_dict(root, changes, ignores=()):\n    \"\"\"Update recursively all the entries from a dict and it's children dicts.\n\n    :param dict root: root dictionary\n    :param dict changes: dictonary where changes should be made (default=root)\n    :returns dict newd: dictionary with removed entries of val.\n    \"\"\"\n    if isinstance(changes, dict):\n        for k, v in changes.items():\n            if isinstance(v, dict):\n                if k not in root:\n                    root[k] = {}\n                recursive_update_dict(root[k], v, ignores)\n            elif v in ignores:\n                if k in root:\n                    root.pop(k)\n            else:\n                root[k] = v\n\n\ndef random_bytes_hex(bytes_length):\n    \"\"\"Return a hexstring of bytes_length cryptographic-friendly random bytes.\n\n    :param int bytes_length: number of random bytes.\n    :rtype: str\n    \"\"\"\n    return hexlify(os.urandom(bytes_length)).decode(\"utf-8\")\n\n\ndef native_value(value):\n    \"\"\"Convert string value to native python values.\n\n    :param str value: value to interprete.\n    :returns: the value coerced to python type\n    \"\"\"\n    if isinstance(value, str):\n        try:\n            value = json.loads(value)\n        except ValueError:\n            return value\n    return value\n\n\ndef read_env(key, value):\n    \"\"\"Read the setting key from environment variables.\n\n    :param key: the setting name\n    :param value: default value if undefined in environment\n    :returns: the value from environment, coerced to python type, or the (uncoerced) default value\n    \"\"\"\n    envkey = key.replace(\".\", \"_\").replace(\"-\", \"_\").upper()\n    if envkey in os.environ:\n        return native_value(os.environ[envkey])\n    return value\n\n\ndef encode64(content, encoding=\"utf-8\"):\n    \"\"\"Encode some content in base64.\n\n    :rtype: str\n    \"\"\"\n    return b64encode(content.encode(encoding)).decode(encoding)\n\n\ndef decode64(encoded_content, encoding=\"utf-8\"):\n    \"\"\"Decode some base64 encoded content.\n\n    :rtype: str\n    \"\"\"\n    return b64decode(encoded_content.encode(encoding)).decode(encoding)\n\n\ndef hmac_digest(secret, message, encoding=\"utf-8\"):\n    \"\"\"Return hex digest of a message HMAC using secret\"\"\"\n    if isinstance(secret, str):\n        secret = secret.encode(encoding)\n    return hmac.new(secret, message.encode(encoding), hashlib.sha256).hexdigest()\n\n\n\n\n# The code to be completed is:\ndef dict_subset(d, keys):\n\n    \"\"\"\n    This function takes a dictionary and a list of keys as input and returns a new dictionary that contains only the specified keys and their corresponding values from the original dictionary. If a key contains a dot (.), it is treated as a nested key and the function retrieves the value of the nested key from the original dictionary.\n    Input-Output Arguments\n    :param d: dict. The original dictionary.\n    :param keys: list. A list of keys to include in the new dictionary.\n    :return: dict. A new dictionary that contains only the specified keys and their corresponding values from the original dictionary.\n    \"\"\""}
{"namespace": "sumy.summarizers.sum_basic.SumBasicSummarizer._compute_word_freq", "prompt": "# Please complete the _compute_word_freq function based on the contexts above the function.\n\n# The contexts above the function are:\n# -*- coding: utf-8 -*-\n\nfrom __future__ import absolute_import\nfrom __future__ import division, print_function, unicode_literals\n\nfrom ._summarizer import AbstractSummarizer\n\n\nclass SumBasicSummarizer(AbstractSummarizer):\n    \"\"\"\n    SumBasic: a frequency-based summarization system that adjusts word frequencies as\n    sentences are extracted.\n    Source: http://www.cis.upenn.edu/~nenkova/papers/ipm.pdf\n    \"\"\"\n    _stop_words = frozenset()\n\n    @property\n    def stop_words(self):\n        return self._stop_words\n\n    @stop_words.setter\n    def stop_words(self, words):\n        self._stop_words = frozenset(map(self.normalize_word, words))\n\n    def __call__(self, document, sentences_count):\n        sentences = document.sentences\n        ratings = self._compute_ratings(sentences)\n        return self._get_best_sentences(document.sentences, sentences_count, ratings)\n\n    def _get_all_words_in_doc(self, sentences):\n        return self._stem_words([w for s in sentences for w in s.words])\n\n    def _get_content_words_in_sentence(self, sentence):\n        normalized_words = self._normalize_words(sentence.words)\n        normalized_content_words = self._filter_out_stop_words(normalized_words)\n        stemmed_normalized_content_words = self._stem_words(normalized_content_words)\n        return stemmed_normalized_content_words\n\n    def _stem_words(self, words):\n        return [self.stem_word(w) for w in words]\n\n    def _normalize_words(self, words):\n        return [self.normalize_word(w) for w in words]\n\n    def _filter_out_stop_words(self, words):\n        return [w for w in words if w not in self.stop_words]\n\n    @staticmethod\n\n\n# The code to be completed is:\n    def _compute_word_freq(list_of_words):\n\n        \"\"\"\n        This function computes the frequency of each word in the given list of words and returns a dictionary containing the word frequencies.\n        Input-Output Arguments\n        :param list_of_words: List of strings. The list of words for which the frequency needs to be computed.\n        :return: Dictionary. A dictionary containing the frequency of each word in the input list.\n        \"\"\""}
{"namespace": "bentoml._internal.configuration.helpers.expand_env_var_in_values", "prompt": "# Please complete the expand_env_var_in_values function based on the contexts above the function.\n\n# The contexts above the function are:\nfrom __future__ import annotations\n\nimport ipaddress\nimport logging\nimport os\nimport re\nimport typing as t\nfrom functools import singledispatch\nfrom typing import TYPE_CHECKING\n\nimport schema as s\nimport yaml\n\nfrom ...exceptions import BentoMLConfigException\nfrom ..utils import LazyLoader\n\nif TYPE_CHECKING:\n    from types import ModuleType\n\nlogger = logging.getLogger(__name__)\n\nTRACING_TYPE = [\"zipkin\", \"jaeger\", \"otlp\", \"in_memory\"]\n\n\ndef import_configuration_spec(version: int) -> ModuleType:  # pragma: no cover\n    return LazyLoader(\n        f\"v{version}\",\n        globals(),\n        f\"bentoml._internal.configuration.v{version}\",\n        exc_msg=f\"Configuration version {version} does not exist.\",\n    )\n\n\n@singledispatch\ndef depth(_: t.Any, _level: int = 0):  # pragma: no cover\n    return _level\n\n\n@depth.register(dict)\ndef _(d: dict[str, t.Any], level: int = 0, **kw: t.Any):\n    return max(depth(v, level + 1, **kw) for v in d.values())\n\n\ndef rename_fields(\n    d: dict[str, t.Any],\n    current: str,\n    replace_with: str | None = None,\n    *,\n    remove_only: bool = False,\n):\n    # We assume that the given dictionary is already flattened.\n    # This function will rename the keys in the dictionary.\n    # If `replace_with` is None, then the key will be removed.\n    if depth(d) != 1:\n        raise ValueError(\n            \"Given dictionary is not flattened. Use flatten_dict first.\"\n        ) from None\n    if current in d:\n        if remove_only:\n            logger.warning(\"Field '%s' is deprecated and will be removed.\" % current)\n            d.pop(current)\n        else:\n            assert replace_with, \"'replace_with' must be provided.\"\n            logger.warning(\n                \"Field '%s' is deprecated and has been renamed to '%s'\"\n                % (current, replace_with)\n            )\n            d[replace_with] = d.pop(current)\n\n\npunctuation = r\"\"\"!\"#$%&'()*+,-./:;<=>?@[\\]^`{|}~\"\"\"\n\n\ndef flatten_dict(\n    d: t.MutableMapping[str, t.Any],\n    parent: str = \"\",\n    sep: str = \".\",\n) -> t.Generator[tuple[str, t.Any], None, None]:\n    \"\"\"Flatten nested dictionary into a single level dictionary.\"\"\"\n    for k, v in d.items():\n        k = f'\"{k}\"' if any(i in punctuation for i in k) else k\n        nkey = parent + sep + k if parent else k\n        if isinstance(v, t.MutableMapping):\n            yield from flatten_dict(\n                t.cast(t.MutableMapping[str, t.Any], v), parent=nkey, sep=sep\n            )\n        else:\n            yield nkey, v\n\n\ndef load_config_file(path: str) -> dict[str, t.Any]:\n    \"\"\"Load configuration from given path.\"\"\"\n    if not os.path.exists(path):\n        raise BentoMLConfigException(\n            \"Configuration file %s not found.\" % path\n        ) from None\n    with open(path, \"rb\") as f:\n        config = yaml.safe_load(f)\n    return config\n\n\ndef get_default_config(version: int) -> dict[str, t.Any]:\n    config = load_config_file(\n        os.path.join(\n            os.path.dirname(__file__), f\"v{version}\", \"default_configuration.yaml\"\n        )\n    )\n    mod = import_configuration_spec(version)\n    assert hasattr(mod, \"SCHEMA\"), (\n        \"version %d does not have a validation schema\" % version\n    )\n    try:\n        mod.SCHEMA.validate(config)\n    except s.SchemaError as e:\n        raise BentoMLConfigException(\n            \"Default configuration for version %d does not conform to given schema:\\n%s\"\n            % (version, e)\n        ) from None\n    return config\n\n\ndef validate_tracing_type(tracing_type: str) -> bool:\n    return tracing_type in TRACING_TYPE\n\n\ndef validate_otlp_protocol(protocol: str) -> bool:\n    return protocol in [\"grpc\", \"http\"]\n\n\ndef ensure_larger_than(target: int | float) -> t.Callable[[int | float], bool]:\n    \"\"\"Ensure that given value is (lower, inf]\"\"\"\n\n    def v(value: int | float) -> bool:\n        return value > target\n\n    return v\n\n\nensure_larger_than_zero = ensure_larger_than(0)\n\n\ndef ensure_range(\n    lower: int | float, upper: int | float\n) -> t.Callable[[int | float], bool]:\n    \"\"\"Ensure that given value is within the range of [lower, upper].\"\"\"\n\n    def v(value: int | float) -> bool:\n        return lower <= value <= upper\n\n    return v\n\n\ndef ensure_iterable_type(typ_: type) -> t.Callable[[t.MutableSequence[t.Any]], bool]:\n    \"\"\"Ensure that given mutable sequence has all elements of given types.\"\"\"\n\n    def v(value: t.MutableSequence[t.Any]) -> bool:\n        return all(isinstance(i, typ_) for i in value)\n\n    return v\n\n\ndef is_valid_ip_address(addr: str) -> bool:\n    \"\"\"Check if given string is a valid IP address.\"\"\"\n    try:\n        _ = ipaddress.ip_address(addr)\n        return True\n    except ValueError:\n        return False\n\n\n_ENV_VAR_PATTERN = r\"\\$\\{(?P<name>.+?)(?:\\:-(?P<default>.+?))?\\}\"\n\n\ndef expand_env_var(value: t.Any) -> str:\n    if not isinstance(value, str) or \"${\" not in value:\n        return value\n\n    def _replace_env(match: t.Match[str]) -> str:\n        name = match.group(\"name\")\n        default = match.group(\"default\")\n        return os.getenv(name, default or \"\")\n\n    return re.sub(_ENV_VAR_PATTERN, _replace_env, value)\n\n\n\n\n# The code to be completed is:\ndef expand_env_var_in_values(d: t.MutableMapping[str, t.Any]) -> None:\n\n    \"\"\"\n    This function expands environment variables in the values of a given dictionary. It iterates through each key-value pair in the dictionary and checks the type of the value including \"mutable mapping\", \"string\" and \"sequence\". Then it calls the corresponding functions.\n    Input-Output Arguments\n    :param d: MutableMapping[str, Any]. A dictionary-like object with string keys and arbitrary values.\n    :return: No return values.\n    \"\"\""}
{"namespace": "rest_framework.templatetags.rest_framework.add_query_param", "prompt": "# Please complete the add_query_param function based on the contexts above the function.\n\n# The contexts above the function are:\nimport re\nfrom collections import OrderedDict\n\nfrom django import template\nfrom django.template import loader\nfrom django.urls import NoReverseMatch, reverse\nfrom django.utils.encoding import iri_to_uri\nfrom django.utils.html import escape, format_html, smart_urlquote\nfrom django.utils.safestring import mark_safe\n\nfrom rest_framework.compat import apply_markdown, pygments_highlight\nfrom rest_framework.renderers import HTMLFormRenderer\n\n\nregister = template.Library()\n\n# Regex for adding classes to html snippets\nclass_re = re.compile(r'(?<=class=[\"\\'])(.*)(?=[\"\\'])')\n\n\n@register.tag(name='code')\ndef highlight_code(parser, token):\n    code = token.split_contents()[-1]\n    nodelist = parser.parse(('endcode',))\n    parser.delete_first_token()\n    return CodeNode(code, nodelist)\n\n\nclass CodeNode(template.Node):\n    style = 'emacs'\n\n    def __init__(self, lang, code):\n        self.lang = lang\n        self.nodelist = code\n\n    def render(self, context):\n        text = self.nodelist.render(context)\n        return pygments_highlight(text, self.lang, self.style)\n\n\n@register.filter()\ndef with_location(fields, location):\n    return [\n        field for field in fields\n        if field.location == location\n    ]\n\n\n@register.simple_tag\ndef form_for_link(link):\n    import coreschema\n    properties = OrderedDict([\n        (field.name, field.schema or coreschema.String())\n        for field in link.fields\n    ])\n    required = [\n        field.name\n        for field in link.fields\n        if field.required\n    ]\n    schema = coreschema.Object(properties=properties, required=required)\n    return mark_safe(coreschema.render_to_form(schema))\n\n\n@register.simple_tag\ndef render_markdown(markdown_text):\n    if apply_markdown is None:\n        return markdown_text\n    return mark_safe(apply_markdown(markdown_text))\n\n\n@register.simple_tag\ndef get_pagination_html(pager):\n    return pager.to_html()\n\n\n@register.simple_tag\ndef render_form(serializer, template_pack=None):\n    style = {'template_pack': template_pack} if template_pack else {}\n    renderer = HTMLFormRenderer()\n    return renderer.render(serializer.data, None, {'style': style})\n\n\n@register.simple_tag\ndef render_field(field, style):\n    renderer = style.get('renderer', HTMLFormRenderer())\n    return renderer.render_field(field, style)\n\n\n@register.simple_tag\ndef optional_login(request):\n    \"\"\"\n    Include a login snippet if REST framework's login view is in the URLconf.\n    \"\"\"\n    try:\n        login_url = reverse('rest_framework:login')\n    except NoReverseMatch:\n        return ''\n\n    snippet = \"<li><a href='{href}?next={next}'>Log in</a></li>\"\n    snippet = format_html(snippet, href=login_url, next=escape(request.path))\n\n    return mark_safe(snippet)\n\n\n@register.simple_tag\ndef optional_docs_login(request):\n    \"\"\"\n    Include a login snippet if REST framework's login view is in the URLconf.\n    \"\"\"\n    try:\n        login_url = reverse('rest_framework:login')\n    except NoReverseMatch:\n        return 'log in'\n\n    snippet = \"<a href='{href}?next={next}'>log in</a>\"\n    snippet = format_html(snippet, href=login_url, next=escape(request.path))\n\n    return mark_safe(snippet)\n\n\n@register.simple_tag\ndef optional_logout(request, user):\n    \"\"\"\n    Include a logout snippet if REST framework's logout view is in the URLconf.\n    \"\"\"\n    try:\n        logout_url = reverse('rest_framework:logout')\n    except NoReverseMatch:\n        snippet = format_html('<li class=\"navbar-text\">{user}</li>', user=escape(user))\n        return mark_safe(snippet)\n\n    snippet = \"\"\"<li class=\"dropdown\">\n        <a href=\"#\" class=\"dropdown-toggle\" data-toggle=\"dropdown\">\n            {user}\n            <b class=\"caret\"></b>\n        </a>\n        <ul class=\"dropdown-menu\">\n            <li><a href='{href}?next={next}'>Log out</a></li>\n        </ul>\n    </li>\"\"\"\n    snippet = format_html(snippet, user=escape(user), href=logout_url, next=escape(request.path))\n\n    return mark_safe(snippet)\n\n\n@register.simple_tag\n\n\n# The code to be completed is:\ndef add_query_param(request, key, val):\n\n    \"\"\"\n    This function adds a query parameter to the current request URL and returns the new URL. It first gets the full path of the request URL, converts it to a URI, replaces the query parameter with the given key and value, and then escapes the URI before returning it.\n    Input-Output Arguments\n    :param request: The current request object.\n    :param key: The key of the query parameter to be added.\n    :param val: The value of the query parameter to be added.\n    :return: The new URL with the added query parameter.\n    \"\"\""}
{"namespace": "onlinejudge_command.pretty_printers._render_tokens", "prompt": "# Please complete the _render_tokens function based on the contexts above the function.\n\n# The contexts above the function are:\nimport difflib\nimport enum\nimport shutil\nfrom logging import getLogger\nfrom typing import *\n\nimport colorama\n\nfrom onlinejudge_command.output_comparators import CompareMode, check_lines_match\n\nlogger = getLogger(__name__)\n\n\nclass _PrettyTokenType(enum.Enum):\n    BODY = 'BODY'\n    BODY_HIGHLIGHT_LEFT = 'BODY_HIGHLIGHT_LEFT'\n    BODY_HIGHLIGHT_RIGHT = 'BODY_HIGHLIGHT_RIGHT'\n    WHITESPACE = 'WHITESPACE'\n    NEWLINE = 'NEWLINE'\n    HINT = 'HINT'\n    LINENO = 'LINENO'\n    OTHERS = 'OTHERS'\n\n\nclass _PrettyToken(NamedTuple):\n    type: _PrettyTokenType\n    value: str\n\n\ndef _optimize_tokens(tokens: List[_PrettyToken]) -> List[_PrettyToken]:\n    optimized: List[_PrettyToken] = []\n    for token in tokens:\n        if optimized and optimized[-1].type == token.type:\n            optimized[-1] = _PrettyToken(token.type, optimized[-1].value + token.value)\n        else:\n            optimized.append(token)\n    return optimized\n\n\ndef _tokenize_str(s: str) -> List[_PrettyToken]:\n    tokens = []\n    l = 0\n    while l < len(s):\n        r = l + 1\n        while r < len(s) and (s[l] in ' \\t') == (s[r] in ' \\t'):\n            r += 1\n        if s[l] in ' \\t':\n            typ = _PrettyTokenType.WHITESPACE\n        else:\n            typ = _PrettyTokenType.BODY\n        tokens.append(_PrettyToken(typ, s[l:r]))\n        l = r\n    return tokens\n\n\ndef _tokenize_line(line: str) -> List[_PrettyToken]:\n    body = line.rstrip()\n    newline = line[len(body):]\n    tokens = []\n\n    # add the body of line\n    if body:\n        tokens += _tokenize_str(body)\n\n    # add newlines\n    if newline:\n        if newline in ('\\n', '\\r\\n'):\n            tokens.append(_PrettyToken(_PrettyTokenType.NEWLINE, newline))\n        else:\n            whitespace = newline.rstrip('\\n')\n            newline = newline[len(whitespace):]\n            if whitespace:\n                tokens.append(_PrettyToken(_PrettyTokenType.WHITESPACE, whitespace))\n            tokens.append(_PrettyToken(_PrettyTokenType.HINT, '(trailing whitespace)'))\n            if newline:\n                tokens.append(_PrettyToken(_PrettyTokenType.NEWLINE, newline))\n\n    return tokens\n\n\ndef _decode_with_recovery(content: bytes) -> Tuple[List[_PrettyToken], str]:\n    tokens = []\n    try:\n        text = content.decode()\n    except UnicodeDecodeError as e:\n        tokens.append(_PrettyToken(_PrettyTokenType.HINT, str(e)))\n        text = content.decode(errors='replace')\n    return tokens, text\n\n\ndef _warn_if_empty(tokens: List[_PrettyToken]) -> List[_PrettyToken]:\n    if not tokens:\n        return [_PrettyToken(_PrettyTokenType.HINT, '(empty)')]\n    if tokens[-1][0] == _PrettyTokenType.BODY:\n        tokens.append(_PrettyToken(_PrettyTokenType.HINT, '(no trailing newline)'))\n    if all(token.type == _PrettyTokenType.NEWLINE for token in tokens):\n        tokens.append(_PrettyToken(_PrettyTokenType.HINT, '(only newline)'))\n    return tokens\n\n\ndef _tokenize_large_file_content(*, content: bytes, limit: int, head: int, tail: int, char_in_line: int) -> List[_PrettyToken]:\n    \"\"\"`_tokenize_large_file_content` constructs the intermediate representations. They have no color infomation.\n    \"\"\"\n\n    assert head + tail < limit\n\n    def candidate_do_nothing(text: str) -> List[_PrettyToken]:\n        tokens = []\n        for line in text.splitlines(keepends=True):\n            tokens += _tokenize_line(line)\n        return tokens\n\n    def candidate_line_based(text: str) -> List[_PrettyToken]:\n        lines = text.splitlines(keepends=True)\n        if len(lines) < limit:\n            return candidate_do_nothing(text)\n\n        tokens = []\n        for line in lines[:head]:\n            tokens += _tokenize_line(line)\n        tokens.append(_PrettyToken(_PrettyTokenType.HINT, '... ({} lines) ...\\n'.format(len(lines[head:-tail]))))\n        for line in lines[-tail:]:\n            tokens += _tokenize_line(line)\n        return tokens\n\n    def candidate_char_based(text: str) -> List[_PrettyToken]:\n        if len(text) < char_in_line * limit:\n            return candidate_do_nothing(text)\n\n        l = len(text[:char_in_line * head].rstrip())\n        r = len(text) - char_in_line * tail\n        tokens = []\n        for line in text[:l].splitlines(keepends=True):\n            tokens += _tokenize_line(line)\n        tokens.append(_PrettyToken(_PrettyTokenType.HINT, '... ({} chars) ...'.format(r - l)))\n        for line in text[r:].splitlines(keepends=True):\n            tokens += _tokenize_line(line)\n        return tokens\n\n    def count_size(tokens: Iterable[_PrettyToken]) -> int:\n        size = 0\n        for _, s in tokens:\n            size += len(s)\n        return size\n\n    # Choose the shortest one from the three candidates.\n    tokens, text = _decode_with_recovery(content)\n    if text:\n        candidates: List[List[_PrettyToken]] = [\n            candidate_do_nothing(text),\n            candidate_line_based(text),\n            candidate_char_based(text),\n        ]\n        tokens.extend(min(candidates, key=count_size))\n    tokens = _warn_if_empty(tokens)\n    return tokens\n\n\ndef _replace_whitespace(s: str) -> str:\n    return s.replace(' ', '_').replace('\\t', '\\\\t').replace('\\r', '\\\\r')\n\n\n\n\n# The code to be completed is:\ndef _render_tokens(\n    *,\n    tokens: List[_PrettyToken],\n    font_bold: Optional[Callable[[str], str]] = None,\n    font_dim: Optional[Callable[[str], str]] = None,\n    font_red: Optional[Callable[[str], str]] = None,\n    font_blue: Optional[Callable[[str], str]] = None,\n    font_normal: Optional[Callable[[str], str]] = None,\n) -> str:\n\n    \"\"\"\n    This function takes a list of tokens and applies different formatting styles to each token based on its type. It then concatenates all the formatted tokens into a single string and returns it.\n    Input-Output Arguments\n    :param tokens: List of _PrettyToken. A list of tokens to be formatted.\n    :param font_bold: Optional Callable. A function that applies bold font style to a string. Defaults to None.\n    :param font_dim: Optional Callable. A function that applies dim font style to a string. Defaults to None.\n    :param font_red: Optional Callable. A function that applies red font color to a string. Defaults to None.\n    :param font_blue: Optional Callable. A function that applies blue font color to a string. Defaults to None.\n    :param font_normal: Optional Callable. A function that applies normal font style to a string. Defaults to None.\n    :return: String. The formatted string generated from the tokens.\n    \"\"\""}
{"namespace": "mackup.utils.error", "prompt": "# Please complete the error function based on the contexts above the function.\n\n# The contexts above the function are:\n\"\"\"System static utilities being used by the modules.\"\"\"\nimport base64\nimport os\nimport platform\nimport shutil\nimport stat\nimport subprocess\nimport sys\nimport sqlite3\nfrom six.moves import input\n\nfrom . import constants\n\n\n# Flag that controls how user confirmation works.\n# If True, the user wants to say \"yes\" to everything.\nFORCE_YES = False\n\n# Flag that control if mackup can be run as root\nCAN_RUN_AS_ROOT = False\n\n\ndef confirm(question):\n    \"\"\"\n    Ask the user if he really wants something to happen.\n\n    Args:\n        question(str): What can happen\n\n    Returns:\n        (boolean): Confirmed or not\n    \"\"\"\n    if FORCE_YES:\n        return True\n\n    while True:\n        answer = input(question + \" <Yes|No> \").lower()\n\n        if answer == \"yes\" or answer == \"y\":\n            confirmed = True\n            break\n        if answer == \"no\" or answer == \"n\":\n            confirmed = False\n            break\n\n    return confirmed\n\n\ndef delete(filepath):\n    \"\"\"\n    Delete the given file, directory or link.\n\n    It Should support undelete later on.\n\n    Args:\n        filepath (str): Absolute full path to a file. e.g. /path/to/file\n    \"\"\"\n    # Some files have ACLs, let's remove them recursively\n    remove_acl(filepath)\n\n    # Some files have immutable attributes, let's remove them recursively\n    remove_immutable_attribute(filepath)\n\n    # Finally remove the files and folders\n    if os.path.isfile(filepath) or os.path.islink(filepath):\n        os.remove(filepath)\n    elif os.path.isdir(filepath):\n        shutil.rmtree(filepath)\n\n\ndef copy(src, dst):\n    \"\"\"\n    Copy a file or a folder (recursively) from src to dst.\n\n    For the sake of simplicity, both src and dst must be absolute path and must\n    include the filename of the file or folder.\n    Also do not include any trailing slash.\n\n    e.g. copy('/path/to/src_file', '/path/to/dst_file')\n    or copy('/path/to/src_folder', '/path/to/dst_folder')\n\n    But not: copy('/path/to/src_file', 'path/to/')\n    or copy('/path/to/src_folder/', '/path/to/dst_folder')\n\n    Args:\n        src (str): Source file or folder\n        dst (str): Destination file or folder\n    \"\"\"\n    assert isinstance(src, str)\n    assert os.path.exists(src)\n    assert isinstance(dst, str)\n\n    # Create the path to the dst file if it does not exist\n    abs_path = os.path.dirname(os.path.abspath(dst))\n    if not os.path.isdir(abs_path):\n        os.makedirs(abs_path)\n\n    # We need to copy a single file\n    if os.path.isfile(src):\n        # Copy the src file to dst\n        shutil.copy(src, dst)\n\n    # We need to copy a whole folder\n    elif os.path.isdir(src):\n        shutil.copytree(src, dst)\n\n    # What the heck is this?\n    else:\n        raise ValueError(\"Unsupported file: {}\".format(src))\n\n    # Set the good mode to the file or folder recursively\n    chmod(dst)\n\n\ndef link(target, link_to):\n    \"\"\"\n    Create a link to a target file or a folder.\n\n    For the sake of simplicity, both target and link_to must be absolute path and must\n    include the filename of the file or folder.\n    Also do not include any trailing slash.\n\n    e.g. link('/path/to/file', '/path/to/link')\n\n    But not: link('/path/to/file', 'path/to/')\n    or link('/path/to/folder/', '/path/to/link')\n\n    Args:\n        target (str): file or folder the link will point to\n        link_to (str): Link to create\n    \"\"\"\n    assert isinstance(target, str)\n    assert os.path.exists(target)\n    assert isinstance(link_to, str)\n\n    # Create the path to the link if it does not exist\n    abs_path = os.path.dirname(os.path.abspath(link_to))\n    if not os.path.isdir(abs_path):\n        os.makedirs(abs_path)\n\n    # Make sure the file or folder recursively has the good mode\n    chmod(target)\n\n    # Create the link to target\n    os.symlink(target, link_to)\n\n\ndef chmod(target):\n    \"\"\"\n    Recursively set the chmod for files to 0600 and 0700 for folders.\n\n    It's ok unless we need something more specific.\n\n    Args:\n        target (str): Root file or folder\n    \"\"\"\n    assert isinstance(target, str)\n    assert os.path.exists(target)\n\n    file_mode = stat.S_IRUSR | stat.S_IWUSR\n    folder_mode = stat.S_IRUSR | stat.S_IWUSR | stat.S_IXUSR\n\n    # Remove the immutable attribute recursively if there is one\n    remove_immutable_attribute(target)\n\n    if os.path.isfile(target):\n        os.chmod(target, file_mode)\n\n    elif os.path.isdir(target):\n        # chmod the root item\n        os.chmod(target, folder_mode)\n\n        # chmod recursively in the folder it it's one\n        for root, dirs, files in os.walk(target):\n            for cur_dir in dirs:\n                os.chmod(os.path.join(root, cur_dir), folder_mode)\n            for cur_file in files:\n                os.chmod(os.path.join(root, cur_file), file_mode)\n\n    else:\n        raise ValueError(\"Unsupported file type: {}\".format(target))\n\n\n\n\n# The code to be completed is:\ndef error(message):\n\n    \"\"\"\n    Throw an error with the given message and immediately quit the program.\n    Input-Output Arguments\n    :param message: str. The message to display when the error occurs.\n    :return: No return values.\n    \"\"\""}
{"namespace": "alembic.script.write_hooks.register", "prompt": "# Please complete the register function based on the contexts above the function.\n\n# The contexts above the function are:\nfrom __future__ import annotations\n\nimport shlex\nimport subprocess\nimport sys\nfrom typing import Any\nfrom typing import Callable\nfrom typing import Dict\nfrom typing import List\nfrom typing import Mapping\nfrom typing import Optional\nfrom typing import Union\n\nfrom .. import util\nfrom ..util import compat\n\n\nREVISION_SCRIPT_TOKEN = \"REVISION_SCRIPT_FILENAME\"\n\n_registry: dict = {}\n\n\n\n\n# The code to be completed is:\ndef register(name: str) -> Callable:\n\n    \"\"\"\n    This function is a function decorator that registers the function as a write hook. It adds the function to the registry with the given name.\n    Input-Output Arguments\n    :param name: String. The name to register the function with.\n    :return: Callable. A callable function that registers the input function.\n    \"\"\""}
{"namespace": "mmcv.image.colorspace._convert_output_type_range", "prompt": "# Please complete the _convert_output_type_range function based on the contexts above the function.\n\n# The contexts above the function are:\n# Copyright (c) OpenMMLab. All rights reserved.\nfrom typing import Callable, Union\n\nimport cv2\nimport numpy as np\n\n\ndef imconvert(img: np.ndarray, src: str, dst: str) -> np.ndarray:\n    \"\"\"Convert an image from the src colorspace to dst colorspace.\n\n    Args:\n        img (ndarray): The input image.\n        src (str): The source colorspace, e.g., 'rgb', 'hsv'.\n        dst (str): The destination colorspace, e.g., 'rgb', 'hsv'.\n\n    Returns:\n        ndarray: The converted image.\n    \"\"\"\n    code = getattr(cv2, f'COLOR_{src.upper()}2{dst.upper()}')\n    out_img = cv2.cvtColor(img, code)\n    return out_img\n\n\ndef bgr2gray(img: np.ndarray, keepdim: bool = False) -> np.ndarray:\n    \"\"\"Convert a BGR image to grayscale image.\n\n    Args:\n        img (ndarray): The input image.\n        keepdim (bool): If False (by default), then return the grayscale image\n            with 2 dims, otherwise 3 dims.\n\n    Returns:\n        ndarray: The converted grayscale image.\n    \"\"\"\n    out_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n    if keepdim:\n        out_img = out_img[..., None]\n    return out_img\n\n\ndef rgb2gray(img: np.ndarray, keepdim: bool = False) -> np.ndarray:\n    \"\"\"Convert a RGB image to grayscale image.\n\n    Args:\n        img (ndarray): The input image.\n        keepdim (bool): If False (by default), then return the grayscale image\n            with 2 dims, otherwise 3 dims.\n\n    Returns:\n        ndarray: The converted grayscale image.\n    \"\"\"\n    out_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n    if keepdim:\n        out_img = out_img[..., None]\n    return out_img\n\n\ndef gray2bgr(img: np.ndarray) -> np.ndarray:\n    \"\"\"Convert a grayscale image to BGR image.\n\n    Args:\n        img (ndarray): The input image.\n\n    Returns:\n        ndarray: The converted BGR image.\n    \"\"\"\n    img = img[..., None] if img.ndim == 2 else img\n    out_img = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)\n    return out_img\n\n\ndef gray2rgb(img: np.ndarray) -> np.ndarray:\n    \"\"\"Convert a grayscale image to RGB image.\n\n    Args:\n        img (ndarray): The input image.\n\n    Returns:\n        ndarray: The converted RGB image.\n    \"\"\"\n    img = img[..., None] if img.ndim == 2 else img\n    out_img = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)\n    return out_img\n\n\ndef _convert_input_type_range(img: np.ndarray) -> np.ndarray:\n    \"\"\"Convert the type and range of the input image.\n\n    It converts the input image to np.float32 type and range of [0, 1].\n    It is mainly used for pre-processing the input image in colorspace\n    conversion functions such as rgb2ycbcr and ycbcr2rgb.\n\n    Args:\n        img (ndarray): The input image. It accepts:\n            1. np.uint8 type with range [0, 255];\n            2. np.float32 type with range [0, 1].\n\n    Returns:\n        (ndarray): The converted image with type of np.float32 and range of\n            [0, 1].\n    \"\"\"\n    img_type = img.dtype\n    img = img.astype(np.float32)\n    if img_type == np.float32:\n        pass\n    elif img_type == np.uint8:\n        img /= 255.\n    else:\n        raise TypeError('The img type should be np.float32 or np.uint8, '\n                        f'but got {img_type}')\n    return img\n\n\n\n\n# The code to be completed is:\ndef _convert_output_type_range(\n        img: np.ndarray, dst_type: Union[np.uint8, np.float32]) -> np.ndarray:\n\n    \"\"\"\n    Convert the type and range of the input image according to the specified destination type. It converts the image to the desired type and range based on the destination type.\n    Input-Output Arguments\n    :param img: np.ndarray. The input image to be converted with np.float32 type and range [0, 255].\n    :param dst_type: Union[np.uint8, np.float32]. The destination type to which the image should be converted. If dst_type is np.uint8, it converts the image to np.uint8 type with range [0, 255]. If dst_type is np.float32, it converts the image to np.float32 type with range [0, 1].\n    :return: np.ndarray. The converted image with the desired type and range.\n    \"\"\""}
{"namespace": "imapclient.fixed_offset.FixedOffset.for_system", "prompt": "# Please complete the for_system function based on the contexts above the function.\n\n# The contexts above the function are:\n# Copyright (c) 2014, Menno Smits\n# Released subject to the New BSD License\n# Please see http://en.wikipedia.org/wiki/BSD_licenses\n\nimport datetime\nimport time\nfrom typing import Optional\n\nZERO = datetime.timedelta(0)\n\n\nclass FixedOffset(datetime.tzinfo):\n    \"\"\"\n    This class describes fixed timezone offsets in hours and minutes\n    east from UTC\n    \"\"\"\n\n    def __init__(self, minutes: float) -> None:\n        self.__offset = datetime.timedelta(minutes=minutes)\n\n        sign = \"+\"\n        if minutes < 0:\n            sign = \"-\"\n        hours, remaining_mins = divmod(abs(minutes), 60)\n        self.__name = \"%s%02d%02d\" % (sign, hours, remaining_mins)\n\n    def utcoffset(self, _: Optional[datetime.datetime]) -> datetime.timedelta:\n        return self.__offset\n\n    def tzname(self, _: Optional[datetime.datetime]) -> str:\n        return self.__name\n\n    def dst(self, _: Optional[datetime.datetime]) -> datetime.timedelta:\n        return ZERO\n\n    @classmethod\n\n\n# The code to be completed is:\n    def for_system(cls) -> \"FixedOffset\":\n\n        \"\"\"\n        This function returns a FixedOffset instance based on the current working timezone and DST conditions. It checks if the current time is in daylight saving time and if daylight saving time is enabled. If both conditions are true, it sets the offset to the alternate time zone offset. Otherwise, it sets the offset to the default time zone offset.\n        Input-Output Arguments\n        :param cls: Class. The class object.\n        :return: FixedOffset. The created FixedOffset instance.\n        \"\"\""}
{"namespace": "pycoin.bloomfilter.BloomFilter.add_spendable", "prompt": "# Please complete the add_spendable function based on the contexts above the function.\n\n# The contexts above the function are:\nimport math\nimport struct\n\nfrom pycoin.encoding.b58 import a2b_hashed_base58\n\n\nLOG_2 = math.log(2)\n\n\ndef filter_size_required(element_count, false_positive_probability):\n    # The size S of the filter in bytes is given by\n    # (-1 / pow(log(2), 2) * N * log(P)) / 8\n    # Of course you must ensure it does not go over the maximum size\n    # (36,000: selected as it represents a filter of 20,000 items with false\n    # positive rate of < 0.1% or 10,000 items and a false positive rate of < 0.0001%).\n    lfpp = math.log(false_positive_probability)\n    return min(36000, int(((-1 / pow(LOG_2, 2) * element_count * lfpp)+7) // 8))\n\n\ndef hash_function_count_required(filter_size, element_count):\n    # The number of hash functions required is given by S * 8 / N * log(2).\n    return int(filter_size * 8.0 / element_count * LOG_2 + 0.5)\n\n\nclass BloomFilter(object):\n    MASK_ARRAY = [1 << _ for _ in range(8)]\n\n    def __init__(self, size_in_bytes, hash_function_count, tweak):\n        if size_in_bytes > 36000:\n            raise ValueError(\"too large\")\n        self.filter_bytes = bytearray(size_in_bytes)\n        self.bit_count = 8 * size_in_bytes\n        self.hash_function_count = hash_function_count\n        self.tweak = tweak\n\n    def add_item(self, item_bytes):\n        for hash_index in range(self.hash_function_count):\n            seed = hash_index * 0xFBA4C795 + self.tweak\n            self.set_bit(murmur3(item_bytes, seed=seed) % self.bit_count)\n\n    def add_address(self, address):\n        the_hash160 = a2b_hashed_base58(address)[1:]\n        self.add_item(the_hash160)\n\n    def add_hash160(self, the_hash160):\n        self.add_item(the_hash160)\n\n\n\n# The code to be completed is:\n    def add_spendable(self, spendable):\n\n        \"\"\"\n        Add a spendable to the BloomFilter instance. It converts the spendable into bytes and adds it to the BloomFilter.\n        Input-Output Arguments\n        :param self: BloomFilter. An instance of the BloomFilter class.\n        :param spendable: The spendable to be added to the BloomFilter.\n        :return: No return values.\n        \"\"\""}
{"namespace": "arctic.chunkstore.date_chunker.DateChunker.filter", "prompt": "# Please complete the filter function based on the contexts above the function.\n\n# The contexts above the function are:\nimport pandas as pd\n\nfrom arctic.date import DateRange, to_pandas_closed_closed\nfrom ._chunker import Chunker, START, END\n\n\nclass DateChunker(Chunker):\n    TYPE = 'date'\n\n    def to_chunks(self, df, chunk_size='D', func=None, **kwargs):\n        \"\"\"\n        chunks the dataframe/series by dates\n\n        Parameters\n        ----------\n        df: pandas dataframe or series\n        chunk_size: str\n            any valid Pandas frequency string\n        func: function\n            func will be applied to each `chunk` generated by the chunker.\n            This function CANNOT modify the date column of the dataframe!\n\n        Returns\n        -------\n        generator that produces tuples: (start date, end date,\n                  chunk_size, dataframe/series)\n        \"\"\"\n        if 'date' in df.index.names:\n            dates = df.index.get_level_values('date')\n            if not df.index.is_monotonic_increasing:\n                df = df.sort_index()\n        elif 'date' in df.columns:\n            dates = pd.DatetimeIndex(df.date)\n            if not dates.is_monotonic_increasing:\n                # providing support for pandas 0.16.2 to 0.20.x\n                # neither sort method exists in both\n                try:\n                    df = df.sort_values('date')\n                except AttributeError:\n                    df = df.sort(columns='date')\n                dates = pd.DatetimeIndex(df.date)\n        else:\n            raise Exception(\"Data must be datetime indexed or have a column named 'date'\")\n\n        period_obj = dates.to_period(chunk_size)\n        period_obj_reduced = period_obj.drop_duplicates()\n        count = 0\n        for _, g in df.groupby(period_obj._data):\n            start = period_obj_reduced[count].start_time.to_pydatetime(warn=False)\n            end = period_obj_reduced[count].end_time.to_pydatetime(warn=False)\n            count += 1\n            if func:\n                yield start, end, chunk_size, func(g)\n            else:\n                yield start, end, chunk_size, g\n\n    def to_range(self, start, end):\n        \"\"\"\n        takes start, end from to_chunks and returns a \"range\" that can be used\n        as the argument to methods require a chunk_range\n\n        returns\n        -------\n        A range object (dependent on type of chunker)\n        \"\"\"\n        return DateRange(start, end)\n\n    def chunk_to_str(self, chunk_id):\n        \"\"\"\n        Converts parts of a chunk range (start or end) to a string. These\n        chunk ids/indexes/markers are produced by to_chunks.\n        (See to_chunks)\n\n        returns\n        -------\n        string\n        \"\"\"\n        return str(chunk_id).encode('ascii')\n\n    def to_mongo(self, range_obj):\n        \"\"\"\n        takes the range object used for this chunker type\n        and converts it into a string that can be use for a\n        mongo query that filters by the range\n\n        returns\n        -------\n        dict\n        \"\"\"\n        if isinstance(range_obj, (pd.DatetimeIndex, tuple)):\n            range_obj = DateRange(range_obj[0], range_obj[-1])\n        if range_obj.start and range_obj.end:\n            return {'$and': [{START: {'$lte': range_obj.end}}, {END: {'$gte': range_obj.start}}]}\n        elif range_obj.start:\n            return {END: {'$gte': range_obj.start}}\n        elif range_obj.end:\n            return {START: {'$lte': range_obj.end}}\n        else:\n            return {}\n\n\n\n# The code to be completed is:\n    def filter(self, data, range_obj):\n\n        \"\"\"\n        This function ensures that the data is properly subset to the range in range_obj. It checks the type of range_obj and converts it to DateRange if it is a tuple or pd.DatetimeIndex. Then, it filters the data based on the Pandas DateRange.\n        Input-Output Arguments\n        :param self: DateChunker. An instance of the DateChunker class.\n        :param data: DataFrame. The data to be filtered.\n        :param range_obj: DateRange or tuple. The range to filter the data.\n        :return: DataFrame. The data filtered by range_obj.\n        \"\"\""}
{"namespace": "kinto.core.testing.get_user_headers", "prompt": "# Please complete the get_user_headers function based on the contexts above the function.\n\n# The contexts above the function are:\nimport os\nimport threading\nimport unittest\nfrom collections import defaultdict\nfrom unittest import mock\n\nimport webtest\nfrom cornice import errors as cornice_errors\nfrom pyramid.url import parse_url_overrides\n\nfrom kinto.core import DEFAULT_SETTINGS, statsd\nfrom kinto.core.storage import generators\nfrom kinto.core.utils import follow_subrequest, memcache, sqlalchemy\n\nskip_if_ci = unittest.skipIf(\"CI\" in os.environ, \"ci\")\nskip_if_no_postgresql = unittest.skipIf(sqlalchemy is None, \"postgresql is not installed.\")\nskip_if_no_memcached = unittest.skipIf(memcache is None, \"memcached is not installed.\")\nskip_if_no_statsd = unittest.skipIf(not statsd.statsd_module, \"statsd is not installed.\")\n\n\nclass DummyRequest(mock.MagicMock):\n    \"\"\"Fully mocked request.\"\"\"\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.upath_info = \"/v0/\"\n        self.registry = mock.MagicMock(settings={**DEFAULT_SETTINGS})\n        self.registry.id_generators = defaultdict(generators.UUID4)\n        self.GET = {}\n        self.headers = {}\n        self.errors = cornice_errors.Errors()\n        self.authenticated_userid = \"bob\"\n        self.authn_type = \"basicauth\"\n        self.prefixed_userid = \"basicauth:bob\"\n        self.effective_principals = [\"system.Everyone\", \"system.Authenticated\", \"bob\"]\n        self.prefixed_principals = self.effective_principals + [self.prefixed_userid]\n        self.json = {}\n        self.validated = {}\n        self.log_context = lambda **kw: kw\n        self.matchdict = {}\n        self.response = mock.MagicMock(headers={})\n        self.application_url = \"\"  # used by parse_url_overrides\n\n        def route_url(*a, **kw):\n            # XXX: refactor DummyRequest to take advantage of `pyramid.testing`\n            parts = parse_url_overrides(self, kw)\n            return \"\".join([p for p in parts if p])\n\n        self.route_url = route_url\n\n    follow_subrequest = follow_subrequest\n\n\ndef get_request_class(prefix):\n    class PrefixedRequestClass(webtest.app.TestRequest):\n        @classmethod\n        def blank(cls, path, *args, **kwargs):\n            if prefix:\n                path = f\"/{prefix}{path}\"\n            return webtest.app.TestRequest.blank(path, *args, **kwargs)\n\n    return PrefixedRequestClass\n\n\nclass FormattedErrorMixin:\n    \"\"\"Test mixin in order to perform advanced error responses assertions.\"\"\"\n\n    def assertFormattedError(self, response, code, errno, error, message=None, info=None):\n        self.assertIn(\"application/json\", response.headers[\"Content-Type\"])\n        self.assertEqual(response.json[\"code\"], code)\n        self.assertEqual(response.json[\"errno\"], errno.value)\n        self.assertEqual(response.json[\"error\"], error)\n        if message is not None:\n            self.assertIn(message, response.json[\"message\"])\n        else:  # pragma: no cover\n            self.assertNotIn(\"message\", response.json)\n\n        if info is not None:\n            self.assertIn(info, response.json[\"info\"])\n        else:  # pragma: no cover\n            self.assertNotIn(\"info\", response.json)\n\n\n\n\n# The code to be completed is:\ndef get_user_headers(user, password=\"secret\"):\n\n    \"\"\"\n    This function is a helper function that generates Basic Auth authorization headers based on the specified user and password. It encodes the \"user:password\" string using Base64 encoding and returns the headers as a dictionary {\"Authorization\": encodes result}.\n    Input-Output Arguments\n    :param user: String. The username to be used for authentication.\n    :param password: String. The password to be used for authentication. It defaults to \"secret\" if not specified.\n    :return: dict. The generated authorization headers as a dictionary.\n    \"\"\""}
{"namespace": "sqlite_utils.utils.maximize_csv_field_size_limit", "prompt": "# Please complete the maximize_csv_field_size_limit function based on the contexts above the function.\n\n# The contexts above the function are:\nimport base64\nimport contextlib\nimport csv\nimport enum\nimport hashlib\nimport io\nimport itertools\nimport json\nimport os\nimport sys\nfrom . import recipes\nfrom typing import Dict, cast, BinaryIO, Iterable, Optional, Tuple, Type\n\nimport click\n\ntry:\n    import pysqlite3 as sqlite3  # noqa: F401\n    from pysqlite3 import dbapi2  # noqa: F401\n\n    OperationalError = dbapi2.OperationalError\nexcept ImportError:\n    try:\n        import sqlean as sqlite3  # noqa: F401\n        from sqlean import dbapi2  # noqa: F401\n\n        OperationalError = dbapi2.OperationalError\n    except ImportError:\n        import sqlite3  # noqa: F401\n        from sqlite3 import dbapi2  # noqa: F401\n\n        OperationalError = dbapi2.OperationalError\n\n\nSPATIALITE_PATHS = (\n    \"/usr/lib/x86_64-linux-gnu/mod_spatialite.so\",\n    \"/usr/local/lib/mod_spatialite.dylib\",\n    \"/usr/local/lib/mod_spatialite.so\",\n    \"/opt/homebrew/lib/mod_spatialite.dylib\",\n)\n\n# Mainly so we can restore it if needed in the tests:\nORIGINAL_CSV_FIELD_SIZE_LIMIT = csv.field_size_limit()\n\n\n\n\n# The code to be completed is:\ndef maximize_csv_field_size_limit():\n\n    \"\"\"\n    Increase the CSV field size limit to the maximum possible. It first set the limit of the field's size to be max size of system and then iteratively tries to set the field size limit to the maximum possible value until it succeeds.\n    Input-Output Arguments\n    :param: No input parameters.\n    :return: No return values.\n    \"\"\""}
{"namespace": "mopidy.internal.validation.check_uri", "prompt": "# Please complete the check_uri function based on the contexts above the function.\n\n# The contexts above the function are:\nimport urllib\nfrom collections.abc import Iterable, Mapping\n\nfrom mopidy import exceptions\n\nPLAYBACK_STATES = {\"paused\", \"stopped\", \"playing\"}\n\nTRACK_FIELDS_WITH_TYPES = {\n    \"uri\": str,\n    \"track_name\": str,\n    \"album\": str,\n    \"artist\": str,\n    \"albumartist\": str,\n    \"composer\": str,\n    \"performer\": str,\n    \"track_no\": int,\n    \"genre\": str,\n    \"date\": str,\n    \"comment\": str,\n    \"disc_no\": int,\n    \"musicbrainz_albumid\": str,\n    \"musicbrainz_artistid\": str,\n    \"musicbrainz_trackid\": str,\n}\n\nSEARCH_FIELDS = set(TRACK_FIELDS_WITH_TYPES).union({\"any\"})\n\nPLAYLIST_FIELDS = {\"uri\", \"name\"}  # TODO: add length and last_modified?\n\nTRACKLIST_FIELDS = {  # TODO: add bitrate, length, disc_no, track_no, modified?\n    \"uri\",\n    \"name\",\n    \"genre\",\n    \"date\",\n    \"comment\",\n    \"musicbrainz_id\",\n}\n\nDISTINCT_FIELDS = dict(TRACK_FIELDS_WITH_TYPES)\n\n\n# TODO: _check_iterable(check, msg, **kwargs) + [check(a) for a in arg]?\ndef _check_iterable(arg, msg, **kwargs):\n    \"\"\"Ensure we have an iterable which is not a string or an iterator\"\"\"\n    if isinstance(arg, str):\n        raise exceptions.ValidationError(msg.format(arg=arg, **kwargs))\n    elif not isinstance(arg, Iterable):\n        raise exceptions.ValidationError(msg.format(arg=arg, **kwargs))\n    elif iter(arg) is iter(arg):\n        raise exceptions.ValidationError(msg.format(arg=arg, **kwargs))\n\n\ndef check_choice(arg, choices, msg=\"Expected one of {choices}, not {arg!r}\"):\n    if arg not in choices:\n        raise exceptions.ValidationError(\n            msg.format(arg=arg, choices=tuple(choices))\n        )\n\n\ndef check_boolean(arg, msg=\"Expected a boolean, not {arg!r}\"):\n    check_instance(arg, bool, msg=msg)\n\n\ndef check_instance(arg, cls, msg=\"Expected a {name} instance, not {arg!r}\"):\n    if not isinstance(arg, cls):\n        raise exceptions.ValidationError(msg.format(arg=arg, name=cls.__name__))\n\n\ndef check_instances(arg, cls, msg=\"Expected a list of {name}, not {arg!r}\"):\n    _check_iterable(arg, msg, name=cls.__name__)\n    if not all(isinstance(instance, cls) for instance in arg):\n        raise exceptions.ValidationError(msg.format(arg=arg, name=cls.__name__))\n\n\ndef check_integer(arg, min=None, max=None):\n    if not isinstance(arg, int):\n        raise exceptions.ValidationError(f\"Expected an integer, not {arg!r}\")\n    elif min is not None and arg < min:\n        raise exceptions.ValidationError(\n            f\"Expected number larger or equal to {min}, not {arg!r}\"\n        )\n    elif max is not None and arg > max:\n        raise exceptions.ValidationError(\n            f\"Expected number smaller or equal to {max}, not {arg!r}\"\n        )\n\n\ndef check_query(arg, fields=None, list_values=True):\n    if fields is None:\n        fields = SEARCH_FIELDS\n    # TODO: normalize name  -> track_name\n    # TODO: normalize value -> [value]\n    # TODO: normalize blank -> [] or just remove field?\n    # TODO: remove list_values?\n\n    if not isinstance(arg, Mapping):\n        raise exceptions.ValidationError(\n            f\"Expected a query dictionary, not {arg!r}\"\n        )\n\n    for key, value in arg.items():\n        check_choice(\n            key,\n            fields,\n            msg=\"Expected query field to be one of \" \"{choices}, not {arg!r}\",\n        )\n        if list_values:\n            msg = 'Expected \"{key}\" to be list of strings, not {arg!r}'\n            _check_iterable(value, msg, key=key)\n            [_check_query_value(key, v, msg) for v in value]\n        else:\n            _check_query_value(\n                key, value, 'Expected \"{key}\" to be a string, not {arg!r}'\n            )\n\n\ndef _check_query_value(key, arg, msg):\n    if not isinstance(arg, str) or not arg.strip():\n        raise exceptions.ValidationError(msg.format(arg=arg, key=key))\n\n\n\n\n# The code to be completed is:\ndef check_uri(arg, msg=\"Expected a valid URI, not {arg!r}\"):\n\n    \"\"\"\n    Check if the given argument is a valid URI. If not, raise a validation error with a custom error message. The function first checks if the argument is a string, and then checks if the scheme of the URI is empty.\n    Input-Output Arguments\n    :param arg: Any. The argument to be checked if it is a valid URI.\n    :param msg: String. The custom error message to be raised if the argument is not a valid URI. It defaults to \"Expected a valid URI, not {arg!r}\".\n    :return: No return values.\n    \"\"\""}
{"namespace": "mrjob.util.cmd_line", "prompt": "# Please complete the cmd_line function based on the contexts above the function.\n\n# The contexts above the function are:\n# Copyright 2009-2016 Yelp and Contributors\n# Copyright 2017-2018 Yelp\n# Copyright 2019 Yelp\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Utility functions for MRJob\n\"\"\"\n# don't add imports here that aren't part of the standard Python library,\n# since MRJobs need to run in Amazon's generic EMR environment\nimport logging\nimport os\nimport os.path\nimport pipes\nimport random\nimport shlex\nimport shutil\nimport sys\nimport tarfile\nfrom contextlib import contextmanager\nfrom datetime import timedelta\nfrom distutils.spawn import find_executable\nfrom logging import getLogger\nfrom zipfile import ZIP_DEFLATED\nfrom zipfile import ZIP_STORED\nfrom zipfile import ZipFile\nfrom zipfile import is_zipfile\n\n\n\n\nlog = getLogger(__name__)\n\n\nclass NullHandler(logging.Handler):\n    def emit(self, record):\n        pass\n\n\n\n\n# The code to be completed is:\ndef cmd_line(args):\n\n    \"\"\"\n    This function builds a command line that works in a shell. It takes a list of arguments and converts each argument to a string. Then, it joins the arguments with a space and quotes each argument.\n    Input-Output Arguments\n    :param args: List. A list of arguments to be used in the command line.\n    :return: str. The command line that works in a shell.\n    \"\"\""}
{"namespace": "sshuttle.ssh.parse_hostport", "prompt": "# Please complete the parse_hostport function based on the contexts above the function.\n\n# The contexts above the function are:\nimport sys\nimport os\nimport re\nimport socket\nimport zlib\nimport importlib\nimport importlib.util\nimport subprocess as ssubprocess\nimport shlex\nfrom shlex import quote\nimport ipaddress\nfrom urllib.parse import urlparse\n\nimport sshuttle.helpers as helpers\nfrom sshuttle.helpers import debug2, which, get_path, Fatal\n\n\ndef get_module_source(name):\n    spec = importlib.util.find_spec(name)\n    with open(spec.origin, \"rt\") as f:\n        return f.read().encode(\"utf-8\")\n\n\ndef empackage(z, name, data=None):\n    if not data:\n        data = get_module_source(name)\n    content = z.compress(data)\n    content += z.flush(zlib.Z_SYNC_FLUSH)\n\n    return b'%s\\n%d\\n%s' % (name.encode(\"ASCII\"), len(content), content)\n\n\n\n\n# The code to be completed is:\ndef parse_hostport(rhostport):\n\n    \"\"\"\n    Parse the given rhostport variable looking like [username[:password]@]host[:port]. If only host is given, can be a hostname, IPv4/v6 address or a ssh alias from ~/.ssh/config. It returns a tuple containing username, password, port, and host. It first checks if the rhostport is empty and returns None for all values. Then, it parses the rhostport and extracts the username, password, port, and host based on the given format.\n    Input-Output Arguments\n    :param rhostport: str. The input rhostport variable to be parsed.\n    :return: Tuple. A tuple containing username, password, port, and host.\n    ```\n    \"\"\""}
{"namespace": "zxcvbn.matching.reverse_dictionary_match", "prompt": "# Please complete the reverse_dictionary_match function based on the contexts above the function.\n\n# The contexts above the function are:\nfrom zxcvbn import scoring\nfrom . import adjacency_graphs\nfrom zxcvbn.frequency_lists import FREQUENCY_LISTS\nimport re\n\n\n\n\ndef build_ranked_dict(ordered_list):\n    return {word: idx for idx, word in enumerate(ordered_list, 1)}\n\nRANKED_DICTIONARIES = {}\n\n\ndef add_frequency_lists(frequency_lists_):\n    for name, lst in frequency_lists_.items():\n        RANKED_DICTIONARIES[name] = build_ranked_dict(lst)\n\n\nadd_frequency_lists(FREQUENCY_LISTS)\n\nGRAPHS = {\n    'qwerty': adjacency_graphs.ADJACENCY_GRAPHS['qwerty'],\n    'dvorak': adjacency_graphs.ADJACENCY_GRAPHS['dvorak'],\n    'keypad': adjacency_graphs.ADJACENCY_GRAPHS['keypad'],\n    'mac_keypad': adjacency_graphs.ADJACENCY_GRAPHS['mac_keypad'],\n}\n\nL33T_TABLE = {\n    'a': ['4', '@'],\n    'b': ['8'],\n    'c': ['(', '{', '[', '<'],\n    'e': ['3'],\n    'g': ['6', '9'],\n    'i': ['1', '!', '|'],\n    'l': ['1', '|', '7'],\n    'o': ['0'],\n    's': ['$', '5'],\n    't': ['+', '7'],\n    'x': ['%'],\n    'z': ['2'],\n}\n\nREGEXEN = {\n    'recent_year': re.compile(r'19\\d\\d|200\\d|201\\d'),\n}\n\nDATE_MAX_YEAR = 2050\nDATE_MIN_YEAR = 1000\nDATE_SPLITS = {\n    4: [  # for length-4 strings, eg 1191 or 9111, two ways to split:\n        [1, 2],  # 1 1 91 (2nd split starts at index 1, 3rd at index 2)\n        [2, 3],  # 91 1 1\n    ],\n    5: [\n        [1, 3],  # 1 11 91\n        [2, 3],  # 11 1 91\n    ],\n    6: [\n        [1, 2],  # 1 1 1991\n        [2, 4],  # 11 11 91\n        [4, 5],  # 1991 1 1\n    ],\n    7: [\n        [1, 3],  # 1 11 1991\n        [2, 3],  # 11 1 1991\n        [4, 5],  # 1991 1 11\n        [4, 6],  # 1991 11 1\n    ],\n    8: [\n        [2, 4],  # 11 11 1991\n        [4, 6],  # 1991 11 11\n    ],\n}\n\n\n# omnimatch -- perform all matches\ndef omnimatch(password, _ranked_dictionaries=RANKED_DICTIONARIES):\n    matches = []\n    for matcher in [\n        dictionary_match,\n        reverse_dictionary_match,\n        l33t_match,\n        spatial_match,\n        repeat_match,\n        sequence_match,\n        regex_match,\n        date_match,\n    ]:\n        matches.extend(matcher(password, _ranked_dictionaries=_ranked_dictionaries))\n\n    return sorted(matches, key=lambda x: (x['i'], x['j']))\n\n\n# dictionary match (common passwords, english, last names, etc)\ndef dictionary_match(password, _ranked_dictionaries=RANKED_DICTIONARIES):\n    matches = []\n    length = len(password)\n    password_lower = password.lower()\n    for dictionary_name, ranked_dict in _ranked_dictionaries.items():\n        for i in range(length):\n            for j in range(i, length):\n                if password_lower[i:j + 1] in ranked_dict:\n                    word = password_lower[i:j + 1]\n                    rank = ranked_dict[word]\n                    matches.append({\n                        'pattern': 'dictionary',\n                        'i': i,\n                        'j': j,\n                        'token': password[i:j + 1],\n                        'matched_word': word,\n                        'rank': rank,\n                        'dictionary_name': dictionary_name,\n                        'reversed': False,\n                        'l33t': False,\n                    })\n\n    return sorted(matches, key=lambda x: (x['i'], x['j']))\n\n\n\n\n# The code to be completed is:\ndef reverse_dictionary_match(password,\n                             _ranked_dictionaries=RANKED_DICTIONARIES):\n\n    \"\"\"\n    This function takes a password as input and performs a reverse dictionary match on it. It reverses the password, performs a dictionary match on the reversed password, and then reverses the matched tokens back to their original order. Finally, it sorts the matches based on their positions in the original password.\n    Input-Output Arguments\n    :param password: String. The password to perform reverse dictionary match on.\n    :param _ranked_dictionaries: List of dictionaries. A list of ranked dictionaries to use for matching. Defaults to RANKED_DICTIONARIES.\n    :return: List of matches. The matches found during the reverse dictionary match, sorted based on their positions in the original password.\n    \"\"\""}
{"namespace": "pyramid.config.assets.PackageOverrides.real_loader", "prompt": "# Please complete the real_loader function based on the contexts above the function.\n\n# The contexts above the function are:\nimport os\nimport pkg_resources\nimport sys\nfrom zope.interface import implementer\n\nfrom pyramid.config.actions import action_method\nfrom pyramid.exceptions import ConfigurationError\nfrom pyramid.interfaces import PHASE1_CONFIG, IPackageOverrides\nfrom pyramid.threadlocal import get_current_registry\n\n\nclass OverrideProvider(pkg_resources.DefaultProvider):\n    def __init__(self, module):\n        pkg_resources.DefaultProvider.__init__(self, module)\n        self.module_name = module.__name__\n\n    def _get_overrides(self):\n        reg = get_current_registry()\n        overrides = reg.queryUtility(IPackageOverrides, self.module_name)\n        return overrides\n\n    def get_resource_filename(self, manager, resource_name):\n        \"\"\"Return a true filesystem path for resource_name,\n        co-ordinating the extraction with manager, if the resource\n        must be unpacked to the filesystem.\n        \"\"\"\n        overrides = self._get_overrides()\n        if overrides is not None:\n            filename = overrides.get_filename(resource_name)\n            if filename is not None:\n                return filename\n        return pkg_resources.DefaultProvider.get_resource_filename(\n            self, manager, resource_name\n        )\n\n    def get_resource_stream(self, manager, resource_name):\n        \"\"\"Return a readable file-like object for resource_name.\"\"\"\n        overrides = self._get_overrides()\n        if overrides is not None:\n            stream = overrides.get_stream(resource_name)\n            if stream is not None:\n                return stream\n        return pkg_resources.DefaultProvider.get_resource_stream(\n            self, manager, resource_name\n        )\n\n    def get_resource_string(self, manager, resource_name):\n        \"\"\"Return a string containing the contents of resource_name.\"\"\"\n        overrides = self._get_overrides()\n        if overrides is not None:\n            string = overrides.get_string(resource_name)\n            if string is not None:\n                return string\n        return pkg_resources.DefaultProvider.get_resource_string(\n            self, manager, resource_name\n        )\n\n    def has_resource(self, resource_name):\n        overrides = self._get_overrides()\n        if overrides is not None:\n            result = overrides.has_resource(resource_name)\n            if result is not None:\n                return result\n        return pkg_resources.DefaultProvider.has_resource(self, resource_name)\n\n    def resource_isdir(self, resource_name):\n        overrides = self._get_overrides()\n        if overrides is not None:\n            result = overrides.isdir(resource_name)\n            if result is not None:\n                return result\n        return pkg_resources.DefaultProvider.resource_isdir(\n            self, resource_name\n        )\n\n    def resource_listdir(self, resource_name):\n        overrides = self._get_overrides()\n        if overrides is not None:\n            result = overrides.listdir(resource_name)\n            if result is not None:\n                return result\n        return pkg_resources.DefaultProvider.resource_listdir(\n            self, resource_name\n        )\n\n\n@implementer(IPackageOverrides)\nclass PackageOverrides:\n    # pkg_resources arg in kw args below for testing\n    def __init__(self, package, pkg_resources=pkg_resources):\n        loader = self._real_loader = getattr(package, '__loader__', None)\n        if isinstance(loader, self.__class__):\n            self._real_loader = None\n        # We register ourselves as a __loader__ *only* to support the\n        # setuptools _find_adapter adapter lookup; this class doesn't\n        # actually support the PEP 302 loader \"API\".  This is\n        # excusable due to the following statement in the spec:\n        # ... Loader objects are not\n        # required to offer any useful functionality (any such functionality,\n        # such as the zipimport get_data() method mentioned above, is\n        # optional)...\n        # A __loader__ attribute is basically metadata, and setuptools\n        # uses it as such.\n        package.__loader__ = self\n        # we call register_loader_type for every instantiation of this\n        # class; that's OK, it's idempotent to do it more than once.\n        pkg_resources.register_loader_type(self.__class__, OverrideProvider)\n        self.overrides = []\n        self.overridden_package_name = package.__name__\n\n    def insert(self, path, source):\n        if not path or path.endswith('/'):\n            override = DirectoryOverride(path, source)\n        else:\n            override = FileOverride(path, source)\n        self.overrides.insert(0, override)\n        return override\n\n    def filtered_sources(self, resource_name):\n        for override in self.overrides:\n            o = override(resource_name)\n            if o is not None:\n                yield o\n\n    def get_filename(self, resource_name):\n        for source, path in self.filtered_sources(resource_name):\n            result = source.get_filename(path)\n            if result is not None:\n                return result\n\n    def get_stream(self, resource_name):\n        for source, path in self.filtered_sources(resource_name):\n            result = source.get_stream(path)\n            if result is not None:\n                return result\n\n    def get_string(self, resource_name):\n        for source, path in self.filtered_sources(resource_name):\n            result = source.get_string(path)\n            if result is not None:\n                return result\n\n    def has_resource(self, resource_name):\n        for source, path in self.filtered_sources(resource_name):\n            if source.exists(path):\n                return True\n\n    def isdir(self, resource_name):\n        for source, path in self.filtered_sources(resource_name):\n            result = source.isdir(path)\n            if result is not None:\n                return result\n\n    def listdir(self, resource_name):\n        for source, path in self.filtered_sources(resource_name):\n            result = source.listdir(path)\n            if result is not None:\n                return result\n\n    @property\n\n\n# The code to be completed is:\n    def real_loader(self):\n\n        \"\"\"\n        This function returns the real loader of a PackageOverrides instance. If the real loader is not set, it raises a NotImplementedError.\n        Input-Output Arguments\n        :param self: PackageOverrides. An instance of the PackageOverrides class.\n        :return: Object. The real loader of the PackageOverrides instance.\n        \"\"\""}
{"namespace": "mingus.containers.note_container.NoteContainer.from_chord_shorthand", "prompt": "# Please complete the from_chord_shorthand function based on the contexts above the function.\n\n# The contexts above the function are:\n# -*- coding: utf-8 -*-\n\nfrom __future__ import absolute_import\n\n#    mingus - Music theory Python package, note_container module.\n#    Copyright (C) 2008-2009, Bart Spaans\n#\n#    This program is free software: you can redistribute it and/or modify\n#    it under the terms of the GNU General Public License as published by\n#    the Free Software Foundation, either version 3 of the License, or\n#    (at your option) any later version.\n#\n#    This program is distributed in the hope that it will be useful,\n#    but WITHOUT ANY WARRANTY; without even the implied warranty of\n#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#    GNU General Public License for more details.\n#\n#    You should have received a copy of the GNU General Public License\n#    along with this program.  If not, see <http://www.gnu.org/licenses/>.\n\nfrom mingus.containers.note import Note\nfrom mingus.core import intervals, chords\nfrom mingus.containers.mt_exceptions import UnexpectedObjectError\nimport six\n\n\nclass NoteContainer(object):\n\n    \"\"\"A container for notes.\n\n    The NoteContainer provides a container for the mingus.containers.Note\n    objects.\n\n    It can be used to store single and multiple notes and is required for\n    working with Bars.\n    \"\"\"\n\n    notes = []\n\n    def __init__(self, notes=None):\n        if notes is None:\n            notes = []\n        self.empty()\n        self.add_notes(notes)\n\n    def empty(self):\n        \"\"\"Empty the container.\"\"\"\n        self.notes = []\n\n    def add_note(self, note, octave=None, dynamics=None):\n        \"\"\"Add a note to the container and sorts the notes from low to high.\n\n        The note can either be a string, in which case you could also use\n        the octave and dynamics arguments, or a Note object.\n        \"\"\"\n        if dynamics is None:\n            dynamics = {}\n        if isinstance(note, six.string_types):\n            if octave is not None:\n                note = Note(note, octave, dynamics)\n            elif len(self.notes) == 0:\n                note = Note(note, 4, dynamics)\n            else:\n                if Note(note, self.notes[-1].octave) < self.notes[-1]:\n                    note = Note(note, self.notes[-1].octave + 1, dynamics)\n                else:\n                    note = Note(note, self.notes[-1].octave, dynamics)\n        if not hasattr(note, \"name\"):\n            raise UnexpectedObjectError(\n                \"Object '%s' was not expected. \"\n                \"Expecting a mingus.containers.Note object.\" % note\n            )\n        if note not in self.notes:\n            self.notes.append(note)\n            self.notes.sort()\n        return self.notes\n\n    def add_notes(self, notes):\n        \"\"\"Feed notes to self.add_note.\n\n        The notes can either be an other NoteContainer, a list of Note\n        objects or strings or a list of lists formatted like this:\n        >>> notes = [['C', 5], ['E', 5], ['G', 6]]\n\n        or even:\n        >>> notes = [['C', 5, {'velocity': 20}], ['E', 6, {'velocity': 20}]]\n        \"\"\"\n        if hasattr(notes, \"notes\"):\n            for x in notes.notes:\n                self.add_note(x)\n            return self.notes\n        elif hasattr(notes, \"name\"):\n            self.add_note(notes)\n            return self.notes\n        elif isinstance(notes, six.string_types):\n            self.add_note(notes)\n            return self.notes\n        for x in notes:\n            if isinstance(x, list) and len(x) != 1:\n                if len(x) == 2:\n                    self.add_note(x[0], x[1])\n                else:\n                    self.add_note(x[0], x[1], x[2])\n            else:\n                self.add_note(x)\n        return self.notes\n\n    def from_chord(self, shorthand):\n        \"\"\"Shortcut to from_chord_shorthand.\"\"\"\n        return self.from_chord_shorthand(shorthand)\n\n\n\n# The code to be completed is:\n    def from_chord_shorthand(self, shorthand):\n\n        \"\"\"\n        This function clears the NoteContainer and adds the notes corresponding to the shorthand notation.\n\n        Input-Output Arguments\n        :param self: NoteContainer. An instance of the NoteContainer class.\n        :param shorthand: str. The shorthand notation representing the chords.\n        :return: NoteContainer. The updated NoteContainer instance.\n\n        \"\"\""}
{"namespace": "faker.providers.person.pl_PL.checksum_identity_card_number", "prompt": "# Please complete the checksum_identity_card_number function based on the contexts above the function.\n\n# The contexts above the function are:\nfrom datetime import datetime\nfrom typing import List, Optional, Sequence, Tuple, Union\n\nfrom .. import Provider as PersonProvider\n\n\n\n\n# The code to be completed is:\ndef checksum_identity_card_number(characters: Sequence[Union[str, int]]) -> int:\n\n    \"\"\"\n    This function calculates and returns a control digit for a given list of characters based on the Identity Card Number standards. This control digit is often used for error checking and validation of identity card numbers. The weights for check digits is [7, 3, 1, 0, 7, 3, 1, 7, 3].\n    Input-Output Arguments\n    :param characters: Sequence of Union of string and integer. A list of characters for which the control digit is to be calculated.\n    :return: Integer. The calculated control digit.\n    \"\"\""}
{"namespace": "twtxt.config.Config.from_file", "prompt": "# Please complete the from_file function based on the contexts above the function.\n\n# The contexts above the function are:\n\"\"\"\n    twtxt.config\n    ~~~~~~~~~~~~\n\n    This module implements the config file parser/writer.\n\n    :copyright: (c) 2016-2022 by buckket.\n    :license: MIT, see LICENSE for more details.\n\"\"\"\n\nimport configparser\nimport logging\nimport os\n\nimport click\n\nfrom twtxt.models import Source\n\nlogger = logging.getLogger(__name__)\n\n\nclass Config:\n    \"\"\":class:`Config` interacts with the configuration file.\n\n    :param str config_file: full path to the loaded config file\n    :param ~configparser.ConfigParser cfg: a :class:`~configparser.ConfigParser` object with config loaded\n    \"\"\"\n    config_dir = click.get_app_dir(\"twtxt\")\n    config_name = \"config\"\n\n    def __init__(self, config_file, cfg):\n        self.config_file = config_file\n        self.cfg = cfg\n\n    @classmethod\n\n\n# The code to be completed is:\n    def from_file(cls, file):\n\n        \"\"\"\n        This function tries to load a given config file. It first checks if the file exists, and then reads the file. It creates a Config instance with the file path and the parsed configuration. It checks the sanity of the configuration and returns the instance if it is valid.\n        Input-Output Arguments\n        :param cls: Class. The class of the `Config` instance.\n        :param file: String. The full path to the config file to load.\n        :return: Config. The created `Config` instance.\n        \"\"\""}
{"namespace": "mopidy.internal.network.format_hostname", "prompt": "# Please complete the format_hostname function based on the contexts above the function.\n\n# The contexts above the function are:\nimport logging\nimport re\nimport socket\n\nlogger = logging.getLogger(__name__)\n\n\ndef try_ipv6_socket() -> bool:\n    \"\"\"Determine if system really supports IPv6\"\"\"\n    if not socket.has_ipv6:\n        return False\n    try:\n        socket.socket(socket.AF_INET6).close()\n        return True\n    except OSError as exc:\n        logger.debug(\n            f\"Platform supports IPv6, but socket creation failed, \"\n            f\"disabling: {exc}\"\n        )\n    return False\n\n\n#: Boolean value that indicates if creating an IPv6 socket will succeed.\nhas_ipv6 = try_ipv6_socket()\n\n\n\n\n# The code to be completed is:\ndef format_hostname(hostname: str) -> str:\n\n    \"\"\"\n    This function formats a hostname for display. If the hostname is an IPv6 address in the form of \"x:x:x:x:x:x:x:x\", it is converted to the IPv4-mapped IPv6 address format \"::ffff:x.x.x.x\".\n    Input-Output Arguments\n    :param hostname: String. The hostname to be formatted.\n    :return: String. The formatted hostname.\n    \"\"\""}
{"namespace": "mrjob.parse._parse_progress_from_job_tracker", "prompt": "# Please complete the _parse_progress_from_job_tracker function based on the contexts above the function.\n\n# The contexts above the function are:\n# Copyright 2009-2012 Yelp\n# Copyright 2013 Steve Johnson and David Marin\n# Copyright 2014 Yelp and Contributors\n# Copyright 2015-2018 Yelp\n# Copyright 2019 Yelp\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Utilities for parsing errors, counters, and status messages.\"\"\"\nimport logging\nimport re\nfrom functools import wraps\nfrom io import BytesIO\nfrom os.path import abspath\n\nfrom mrjob.py2 import ParseResult\nfrom mrjob.py2 import pathname2url\n\nfrom mrjob.py2 import urljoin\nfrom mrjob.py2 import urlparse as urlparse_buggy\n\nlog = logging.getLogger(__name__)\n\n\n### URI PARSING ###\n\ndef is_uri(uri):\n    r\"\"\"Return True if *uri* is a URI and contains ``://``\n    (we only care about URIs that can describe files)\n    \"\"\"\n    return '://' in uri and bool(urlparse(uri).scheme)\n\n\ndef is_s3_uri(uri):\n    \"\"\"Return True if *uri* can be parsed into an S3 URI, False otherwise.\"\"\"\n    try:\n        parse_s3_uri(uri)\n        return True\n    except ValueError:\n        return False\n\n\ndef parse_s3_uri(uri):\n    \"\"\"Parse an S3 URI into (bucket, key)\n\n    >>> parse_s3_uri('s3://walrus/tmp/')\n    ('walrus', 'tmp/')\n\n    If ``uri`` is not an S3 URI, raise a ValueError\n    \"\"\"\n    components = urlparse(uri)\n    if (components.scheme not in ('s3', 's3n', 's3a') or\n        '/' not in components.path):  # noqa\n\n        raise ValueError('Invalid S3 URI: %s' % uri)\n\n    return components.netloc, components.path[1:]\n\n\ndef to_uri(path_or_uri):\n    \"\"\"If *path_or_uri* is not a URI already, convert it to a ``file:///``\n    URI.\"\"\"\n    if is_uri(path_or_uri):\n        return path_or_uri\n    else:\n        return urljoin('file:', pathname2url(abspath(path_or_uri)))\n\n\n@wraps(urlparse_buggy)\ndef urlparse(urlstring, scheme='', allow_fragments=True, *args, **kwargs):\n    \"\"\"A wrapper for :py:func:`urlparse.urlparse` that splits the fragment\n    correctly in all URIs, not just Web-related ones.\n    This behavior was fixed in the Python 2.7.4 standard library but we have\n    to back-port it for previous versions.\n    \"\"\"\n    (scheme, netloc, path, params, query, fragment) = (\n        urlparse_buggy(urlstring, scheme, allow_fragments, *args, **kwargs))\n\n    if allow_fragments and '#' in path and not fragment:\n        path, fragment = path.split('#', 1)\n\n    return ParseResult(scheme, netloc, path, params, query, fragment)\n\n\n### OPTION PARSING ###\n\n\n# planning to move this into mrjob.options\ndef _parse_port_range_list(range_list_str):\n    all_ranges = []\n    for range_str in range_list_str.split(','):\n        if ':' in range_str:\n            a, b = [int(x) for x in range_str.split(':')]\n            all_ranges.extend(range(a, b + 1))\n        else:\n            all_ranges.append(int(range_str))\n    return all_ranges\n\n\n### parsing job output/stderr ###\n\n_COUNTER_RE = re.compile(br'^reporter:counter:([^,]*),([^,]*),(-?\\d+)$')\n_STATUS_RE = re.compile(br'^reporter:status:(.*)$')\n\n\ndef parse_mr_job_stderr(stderr, counters=None):\n    \"\"\"Parse counters and status messages out of MRJob output.\n\n    :param stderr: a filehandle, a list of lines (bytes), or bytes\n    :param counters: Counters so far, to update; a map from group (string to\n                     counter name (string) to count.\n\n    Returns a dictionary with the keys *counters*, *statuses*, *other*:\n\n    - *counters*: counters so far; same format as above\n    - *statuses*: a list of status messages encountered\n    - *other*: lines (strings) that aren't either counters or status messages\n    \"\"\"\n    # For the corresponding code in Hadoop Streaming, see ``incrCounter()`` in\n    # http://svn.apache.org/viewvc/hadoop/mapreduce/trunk/src/contrib/streaming/src/java/org/apache/hadoop/streaming/PipeMapRed.java?view=markup  # noqa\n    from mrjob.py2 import to_unicode\n    if isinstance(stderr, bytes):\n        stderr = BytesIO(stderr)\n\n    if counters is None:\n        counters = {}\n    statuses = []\n    other = []\n\n    for line in stderr:\n        m = _COUNTER_RE.match(line.rstrip(b'\\r\\n'))\n        if m:\n            group, counter, amount_str = m.groups()\n\n            # don't leave these as bytes on Python 3\n            group = to_unicode(group)\n            counter = to_unicode(counter)\n\n            counters.setdefault(group, {})\n            counters[group].setdefault(counter, 0)\n            counters[group][counter] += int(amount_str)\n            continue\n\n        m = _STATUS_RE.match(line.rstrip(b'\\r\\n'))\n        if m:\n            # don't leave as bytes on Python 3\n            statuses.append(to_unicode(m.group(1)))\n            continue\n\n        other.append(to_unicode(line))\n\n    return {'counters': counters, 'statuses': statuses, 'other': other}\n\n\n### job tracker/resource manager ###\n\n_JOB_TRACKER_HTML_RE = re.compile(br'\\b(\\d{1,3}\\.\\d{2})%')\n_RESOURCE_MANAGER_JS_RE = re.compile(\n    br'\\s*\\[.*application_[_\\d]+.*\"RUNNING\"'\n    br'.*width:(?P<percent>\\d{1,3}.\\d)%.*\\]'\n)\n\n\n\n\n# The code to be completed is:\ndef _parse_progress_from_job_tracker(html_bytes):\n\n    \"\"\"\n    This function parses the progress of a running job from the HTML content of a job tracker. It extracts the map_percent and reduce_percent values as floats and returns them. The content searched is extracted between 'Running Jobs' and 'Jobs' in the HTML content. If the HTML content does not contain the necessary information, it returns (None, None).\n    Input-Output Arguments\n    :param html_bytes: bytes. The HTML content of the job tracker.\n    :return: Tuple of floats. The map_percent and reduce_percent values extracted from the HTML content, or (None, None) if the information is not found.\n    \"\"\""}
{"namespace": "boto.vpc.connect_to_region", "prompt": "# Please complete the connect_to_region function based on the contexts above the function.\n\n# The contexts above the function are:\n# Copyright (c) 2009 Mitch Garnaat http://garnaat.org/\n#\n# Permission is hereby granted, free of charge, to any person obtaining a\n# copy of this software and associated documentation files (the\n# \"Software\"), to deal in the Software without restriction, including\n# without limitation the rights to use, copy, modify, merge, publish, dis-\n# tribute, sublicense, and/or sell copies of the Software, and to permit\n# persons to whom the Software is furnished to do so, subject to the fol-\n# lowing conditions:\n#\n# The above copyright notice and this permission notice shall be included\n# in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS\n# OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABIL-\n# ITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT\n# SHALL THE AUTHOR BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY,\n# WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS\n# IN THE SOFTWARE.\n\n\"\"\"\nRepresents a connection to the EC2 service.\n\"\"\"\n\nfrom boto.ec2.connection import EC2Connection\nfrom boto.resultset import ResultSet\nfrom boto.vpc.vpc import VPC\nfrom boto.vpc.customergateway import CustomerGateway\nfrom boto.vpc.networkacl import NetworkAcl\nfrom boto.vpc.routetable import RouteTable\nfrom boto.vpc.internetgateway import InternetGateway\nfrom boto.vpc.vpngateway import VpnGateway, Attachment\nfrom boto.vpc.dhcpoptions import DhcpOptions\nfrom boto.vpc.subnet import Subnet\nfrom boto.vpc.vpnconnection import VpnConnection\nfrom boto.vpc.vpc_peering_connection import VpcPeeringConnection\nfrom boto.ec2 import RegionData\nfrom boto.regioninfo import RegionInfo, get_regions\n\n\n\ndef regions(**kw_params):\n    \"\"\"\n    Get all available regions for the EC2 service.\n    You may pass any of the arguments accepted by the VPCConnection\n    object's constructor as keyword arguments and they will be\n    passed along to the VPCConnection object.\n\n    :rtype: list\n    :return: A list of :class:`boto.ec2.regioninfo.RegionInfo`\n    \"\"\"\n    return get_regions('ec2', connection_cls=VPCConnection)\n\n\n\n\n# The code to be completed is:\ndef connect_to_region(region_name, **kw_params):\n\n    \"\"\"\n    This function connects to a specific region and returns a connection object of type `boto.vpc.VPCConnection`.\n    Input-Output Arguments\n    :param region_name: str. The name of the region to connect to.\n    :param **kw_params: Additional parameters to be passed to the `connect` method of the region object.\n    :return: `boto.vpc.VPCConnection` or None. A connection to the given region, or None if an invalid region name is given.\n    \"\"\""}
{"namespace": "imapclient.datetime_util.parse_to_datetime", "prompt": "# Please complete the parse_to_datetime function based on the contexts above the function.\n\n# The contexts above the function are:\n# Copyright (c) 2014, Menno Smits\n# Released subject to the New BSD License\n# Please see http://en.wikipedia.org/wiki/BSD_licenses\n\nimport re\nfrom datetime import datetime\nfrom email.utils import parsedate_tz\n\nfrom .fixed_offset import FixedOffset\n\n_SHORT_MONTHS = \" Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec\".split(\" \")\n\n\n\n\n# The code to be completed is:\ndef parse_to_datetime(timestamp: bytes, normalise: bool = True) -> datetime:\n\n    \"\"\"\n    Convert an IMAP datetime string to a datetime object. \n\n    Input-Output Arguments\n    :param timestamp: String, the IMAP datetime string to be converted.\n    :param normalise: Bool, whether to adjust the converted datetime to the local time. If `normalise` is True (default), the returned datetime object will be timezone-naive but adjusted to the local time. If `normalise` is False, the returned datetime object will be unadjusted but will contain timezone information as per the input.\n    :return: datetime, the converted datetime object from the IMAP datetime string.\n\n    \"\"\""}
{"namespace": "lux.vis.Vis.Vis.get_attr_by_channel", "prompt": "# Please complete the get_attr_by_channel function based on the contexts above the function.\n\n# The contexts above the function are:\n#  Copyright 2019-2020 The Lux Authors.\n#\n#  Licensed under the Apache License, Version 2.0 (the \"License\");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an \"AS IS\" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n\nfrom typing import List, Callable, Union\nfrom lux.vis.Clause import Clause\n\nimport lux\nimport warnings\n\n\nclass Vis:\n    \"\"\"\n    Vis Object represents a collection of fully fleshed out specifications required for data fetching and visualization.\n    \"\"\"\n\n    def __init__(self, intent, source=None, title=\"\", score=0.0):\n        self._intent = intent  # user's original intent to Vis\n        self._inferred_intent = intent  # re-written, expanded version of user's original intent\n        self._source = source  # original data attached to the Vis\n        self._vis_data = None  # processed data for Vis (e.g., selected, aggregated, binned)\n        self._code = None\n        self._mark = \"\"\n        self._min_max = {}\n        self._postbin = None\n        self.title = title\n        self.score = score\n        self._all_column = False\n        self.approx = False\n        self.refresh_source(self._source)\n\n    def __repr__(self):\n        all_clause = all([isinstance(unit, lux.Clause) for unit in self._inferred_intent])\n        if all_clause:\n            filter_intents = None\n            channels, additional_channels = [], []\n            for clause in self._inferred_intent:\n\n                if hasattr(clause, \"value\"):\n                    if clause.value != \"\":\n                        filter_intents = clause\n                if hasattr(clause, \"attribute\"):\n                    if clause.attribute != \"\":\n                        if clause.aggregation != \"\" and clause.aggregation is not None:\n                            attribute = f\"{clause._aggregation_name.upper()}({clause.attribute})\"\n                        elif clause.bin_size > 0:\n                            attribute = f\"BIN({clause.attribute})\"\n                        else:\n                            attribute = clause.attribute\n                        if clause.channel == \"x\":\n                            channels.insert(0, [clause.channel, attribute])\n                        elif clause.channel == \"y\":\n                            channels.insert(1, [clause.channel, attribute])\n                        elif clause.channel != \"\":\n                            additional_channels.append([clause.channel, attribute])\n\n            channels.extend(additional_channels)\n            str_channels = \"\"\n            for channel in channels:\n                str_channels += f\"{channel[0]}: {channel[1]}, \"\n\n            if filter_intents:\n                return f\"<Vis  ({str_channels[:-2]} -- [{filter_intents.attribute}{filter_intents.filter_op}{filter_intents.value}]) mark: {self._mark}, score: {self.score} >\"\n            else:\n                return f\"<Vis  ({str_channels[:-2]}) mark: {self._mark}, score: {self.score} >\"\n        else:\n            # When Vis not compiled (e.g., when self._source not populated), print original intent\n            return f\"<Vis  ({str(self._intent)}) mark: {self._mark}, score: {self.score} >\"\n\n    @property\n    def data(self):\n        return self._vis_data\n\n    @property\n    def code(self):\n        return self._code\n\n    @property\n    def mark(self):\n        return self._mark\n\n    @property\n    def min_max(self):\n        return self._min_max\n\n    @property\n    def intent(self):\n        return self._intent\n\n    @intent.setter\n    def intent(self, intent: List[Clause]) -> None:\n        self.set_intent(intent)\n\n    def set_intent(self, intent: List[Clause]) -> None:\n        \"\"\"\n        Sets the intent of the Vis and refresh the source based on the new intent\n\n        Parameters\n        ----------\n        intent : List[Clause]\n                Query specifying the desired VisList\n        \"\"\"\n        self._intent = intent\n        self.refresh_source(self._source)\n\n    def _ipython_display_(self):\n        from lux.utils.utils import check_import_lux_widget\n        from IPython.display import display\n\n        check_import_lux_widget()\n        import luxwidget\n\n        if self.data is None:\n            raise Exception(\n                \"No data is populated in Vis. In order to generate data required for the vis, use the 'refresh_source' function to populate the Vis with a data source (e.g., vis.refresh_source(df)).\"\n            )\n        else:\n            from lux.core.frame import LuxDataFrame\n\n            widget = luxwidget.LuxWidget(\n                currentVis=LuxDataFrame.current_vis_to_JSON([self]),\n                recommendations=[],\n                intent=\"\",\n                message=\"\",\n                config={\"plottingScale\": lux.config.plotting_scale},\n            )\n            display(widget)\n\n    def get_attr_by_attr_name(self, attr_name):\n        return list(filter(lambda x: x.attribute == attr_name, self._inferred_intent))\n\n\n\n# The code to be completed is:\n    def get_attr_by_channel(self, channel):\n\n        \"\"\"\n        This function retrieves the attribute based on the given channel from the inferred intent list. It filters the list based on the channel and value attributes of each object in the list and returns the filtered list.\n        Input-Output Arguments\n        :param self: Vis. An instance of the Vis class.\n        :param channel: The channel to filter the inferred intent list.\n        :return: List. The filtered list of objects from the inferred intent list.\n        \"\"\""}
{"namespace": "wandb.sdk.wandb_settings._redact_dict", "prompt": "# Please complete the _redact_dict function based on the contexts above the function.\n\n# The contexts above the function are:\nimport collections.abc\nimport configparser\nimport enum\nimport getpass\nimport json\nimport logging\nimport multiprocessing\nimport os\nimport platform\nimport re\nimport shutil\nimport socket\nimport sys\nimport tempfile\nimport time\nfrom dataclasses import dataclass\nfrom datetime import datetime\nfrom distutils.util import strtobool\nfrom functools import reduce\nfrom typing import (\n    Any,\n    Callable,\n    Dict,\n    FrozenSet,\n    ItemsView,\n    Iterable,\n    Mapping,\n    Optional,\n    Sequence,\n    Set,\n    Tuple,\n    Union,\n    no_type_check,\n)\nfrom urllib.parse import quote, unquote, urlencode, urlparse, urlsplit\n\nfrom google.protobuf.wrappers_pb2 import BoolValue, DoubleValue, Int32Value, StringValue\n\nimport wandb\nimport wandb.env\nfrom wandb import util\nfrom wandb.apis.internal import Api\nfrom wandb.errors import UsageError\nfrom wandb.proto import wandb_settings_pb2\nfrom wandb.sdk.internal.system.env_probe_helpers import is_aws_lambda\nfrom wandb.sdk.lib import filesystem\nfrom wandb.sdk.lib._settings_toposort_generated import SETTINGS_TOPOLOGICALLY_SORTED\nfrom wandb.sdk.wandb_setup import _EarlyLogger\n\nfrom .lib import apikey\nfrom .lib.gitlib import GitRepo\nfrom .lib.ipython import _get_python_type\nfrom .lib.runid import generate_id\n\nif sys.version_info >= (3, 8):\n    from typing import get_args, get_origin, get_type_hints\nelif sys.version_info >= (3, 7):\n    from typing_extensions import get_args, get_origin, get_type_hints\nelse:\n\n    def get_args(obj: Any) -> Optional[Any]:\n        return obj.__args__ if hasattr(obj, \"__args__\") else None\n\n    def get_origin(obj: Any) -> Optional[Any]:\n        return obj.__origin__ if hasattr(obj, \"__origin__\") else None\n\n    def get_type_hints(obj: Any) -> Dict[str, Any]:\n        return dict(obj.__annotations__) if hasattr(obj, \"__annotations__\") else dict()\n\n\nclass SettingsPreprocessingError(UsageError):\n    \"\"\"Raised when the value supplied to a wandb.Settings() setting does not pass preprocessing.\"\"\"\n\n\nclass SettingsValidationError(UsageError):\n    \"\"\"Raised when the value supplied to a wandb.Settings() setting does not pass validation.\"\"\"\n\n\nclass SettingsUnexpectedArgsError(UsageError):\n    \"\"\"Raised when unexpected arguments are passed to wandb.Settings().\"\"\"\n\n\ndef _get_wandb_dir(root_dir: str) -> str:\n    \"\"\"Get the full path to the wandb directory.\n\n    The setting exposed to users as `dir=` or `WANDB_DIR` is the `root_dir`.\n    We add the `__stage_dir__` to it to get the full `wandb_dir`\n    \"\"\"\n    # We use the hidden version if it already exists, otherwise non-hidden.\n    if os.path.exists(os.path.join(root_dir, \".wandb\")):\n        __stage_dir__ = \".wandb\" + os.sep\n    else:\n        __stage_dir__ = \"wandb\" + os.sep\n\n    path = os.path.join(root_dir, __stage_dir__)\n    if not os.access(root_dir or \".\", os.W_OK):\n        wandb.termwarn(\n            f\"Path {path} wasn't writable, using system temp directory.\",\n            repeat=False,\n        )\n        path = os.path.join(tempfile.gettempdir(), __stage_dir__ or (\"wandb\" + os.sep))\n\n    return os.path.expanduser(path)\n\n\ndef _str_as_bool(val: Union[str, bool]) -> bool:\n    \"\"\"Parse a string as a bool.\"\"\"\n    if isinstance(val, bool):\n        return val\n    try:\n        ret_val = bool(strtobool(str(val)))\n        return ret_val\n    except (AttributeError, ValueError):\n        pass\n\n    raise UsageError(f\"Could not parse value {val} as a bool.\")\n\n\ndef _str_as_json(val: Union[str, Dict[str, Any]]) -> Any:\n    \"\"\"Parse a string as a json object.\"\"\"\n    if not isinstance(val, str):\n        return val\n    try:\n        return json.loads(val)\n    except (AttributeError, ValueError):\n        pass\n\n    raise UsageError(f\"Could not parse value {val} as JSON.\")\n\n\ndef _str_as_tuple(val: Union[str, Sequence[str]]) -> Tuple[str, ...]:\n    \"\"\"Parse a (potentially comma-separated) string as a tuple.\"\"\"\n    if isinstance(val, str):\n        return tuple(val.split(\",\"))\n    return tuple(val)\n\n\ndef _datetime_as_str(val: Union[datetime, str]) -> str:\n    \"\"\"Parse a datetime object as a string.\"\"\"\n    if isinstance(val, datetime):\n        return datetime.strftime(val, \"%Y%m%d_%H%M%S\")\n    return val\n\n\n\n\n# The code to be completed is:\ndef _redact_dict(\n    d: Dict[str, Any],\n    unsafe_keys: Union[Set[str], FrozenSet[str]] = frozenset({\"api_key\"}),\n    redact_str: str = \"***REDACTED***\",\n) -> Dict[str, Any]:\n\n    \"\"\"\n    Redact a dictionary of unsafe values specified by their key. It replaces the values of the specified keys with a redacted string.\n    Input-Output Arguments\n    :param d: Dict[str, Any]. The input dictionary.\n    :param unsafe_keys: Union[Set[str], FrozenSet[str]]. Set of unsafe keys to be redacted. Defaults to {\"api_key\"}.\n    :param redact_str: str. The redacted string to replace the unsafe values. Defaults to \"***REDACTED***\".\n    :return: Dict[str, Any]. The redacted dictionary.\n    \"\"\""}
{"namespace": "jinja2.loaders.split_template_path", "prompt": "# Please complete the split_template_path function based on the contexts above the function.\n\n# The contexts above the function are:\n\"\"\"API and implementations for loading templates from different data\nsources.\n\"\"\"\nimport importlib.util\nimport os\nimport posixpath\nimport sys\nimport typing as t\nimport weakref\nimport zipimport\nfrom collections import abc\nfrom hashlib import sha1\nfrom importlib import import_module\nfrom types import ModuleType\n\nfrom .exceptions import TemplateNotFound\nfrom .utils import internalcode\nfrom .utils import open_if_exists\n\nif t.TYPE_CHECKING:\n    from .environment import Environment\n    from .environment import Template\n\n\n\n\n# The code to be completed is:\ndef split_template_path(template: str) -> t.List[str]:\n\n    \"\"\"\n    This function takes a template path as input and splits it into segments and performs a sanity check. If it contains path separators or alternate path separators or parent directory references, it raises a template not found error.\n    Input-Output Arguments\n    :param template: str. The template path to be split.\n    :return: List[str]. The segments of the template path after splitting.\n    \"\"\""}
{"namespace": "pythonforandroid.build.Context.setup_dirs", "prompt": "# Please complete the setup_dirs function based on the contexts above the function.\n\n# The contexts above the function are:\nfrom contextlib import suppress\nimport copy\nimport glob\nimport os\nfrom os import environ\nfrom os.path import (\n    abspath, join, realpath, dirname, expanduser, exists\n)\nimport re\nimport shutil\nimport subprocess\n\nimport sh\n\nfrom pythonforandroid.androidndk import AndroidNDK\nfrom pythonforandroid.archs import ArchARM, ArchARMv7_a, ArchAarch_64, Archx86, Archx86_64\nfrom pythonforandroid.logger import (info, warning, info_notify, info_main, shprint)\nfrom pythonforandroid.pythonpackage import get_package_name\nfrom pythonforandroid.recipe import CythonRecipe, Recipe\nfrom pythonforandroid.recommendations import (\n    check_ndk_version, check_target_api, check_ndk_api,\n    RECOMMENDED_NDK_API, RECOMMENDED_TARGET_API)\nfrom pythonforandroid.util import (\n    current_directory, ensure_dir,\n    BuildInterruptingException, rmdir\n)\n\n\ndef get_targets(sdk_dir):\n    if exists(join(sdk_dir, 'cmdline-tools', 'latest', 'bin', 'avdmanager')):\n        avdmanager = sh.Command(join(sdk_dir, 'cmdline-tools', 'latest', 'bin', 'avdmanager'))\n        targets = avdmanager('list', 'target').stdout.decode('utf-8').split('\\n')\n\n    elif exists(join(sdk_dir, 'tools', 'bin', 'avdmanager')):\n        avdmanager = sh.Command(join(sdk_dir, 'tools', 'bin', 'avdmanager'))\n        targets = avdmanager('list', 'target').stdout.decode('utf-8').split('\\n')\n    elif exists(join(sdk_dir, 'tools', 'android')):\n        android = sh.Command(join(sdk_dir, 'tools', 'android'))\n        targets = android('list').stdout.decode('utf-8').split('\\n')\n    else:\n        raise BuildInterruptingException(\n            'Could not find `android` or `sdkmanager` binaries in Android SDK',\n            instructions='Make sure the path to the Android SDK is correct')\n    return targets\n\n\ndef get_available_apis(sdk_dir):\n    targets = get_targets(sdk_dir)\n    apis = [s for s in targets if re.match(r'^ *API level: ', s)]\n    apis = [re.findall(r'[0-9]+', s) for s in apis]\n    apis = [int(s[0]) for s in apis if s]\n    return apis\n\n\nclass Context:\n    '''A build context. If anything will be built, an instance this class\n    will be instantiated and used to hold all the build state.'''\n\n    # Whether to make a debug or release build\n    build_as_debuggable = False\n\n    # Whether to strip debug symbols in `.so` files\n    with_debug_symbols = False\n\n    env = environ.copy()\n    # the filepath of toolchain.py\n    root_dir = None\n    # the root dir where builds and dists will be stored\n    storage_dir = None\n\n    # in which bootstraps are copied for building\n    # and recipes are built\n    build_dir = None\n\n    distribution = None\n    \"\"\"The Distribution object representing the current build target location.\"\"\"\n\n    # the Android project folder where everything ends up\n    dist_dir = None\n\n    # Whether setup.py or similar should be used if present:\n    use_setup_py = False\n\n    ccache = None  # whether to use ccache\n\n    ndk = None\n\n    bootstrap = None\n    bootstrap_build_dir = None\n\n    recipe_build_order = None  # Will hold the list of all built recipes\n\n    symlink_bootstrap_files = False  # If True, will symlink instead of copying during build\n\n    java_build_tool = 'auto'\n\n    @property\n    def packages_path(self):\n        '''Where packages are downloaded before being unpacked'''\n        return join(self.storage_dir, 'packages')\n\n    @property\n    def templates_dir(self):\n        return join(self.root_dir, 'templates')\n\n    @property\n    def libs_dir(self):\n        \"\"\"\n        where Android libs are cached after build\n        but before being placed in dists\n        \"\"\"\n        # Was previously hardcoded as self.build_dir/libs\n        directory = join(self.build_dir, 'libs_collections',\n                         self.bootstrap.distribution.name)\n        ensure_dir(directory)\n        return directory\n\n    @property\n    def javaclass_dir(self):\n        # Was previously hardcoded as self.build_dir/java\n        directory = join(self.build_dir, 'javaclasses',\n                         self.bootstrap.distribution.name)\n        ensure_dir(directory)\n        return directory\n\n    @property\n    def aars_dir(self):\n        directory = join(self.build_dir, 'aars', self.bootstrap.distribution.name)\n        ensure_dir(directory)\n        return directory\n\n    @property\n    def python_installs_dir(self):\n        directory = join(self.build_dir, 'python-installs')\n        ensure_dir(directory)\n        return directory\n\n    def get_python_install_dir(self, arch):\n        return join(self.python_installs_dir, self.bootstrap.distribution.name, arch)\n\n\n\n# The code to be completed is:\n    def setup_dirs(self, storage_dir):\n\n        \"\"\"\n        This function sets up the storage, build, and distribution directories for the Context instance. It calculates the paths for these directories based on the given storage directory and ensures that the directories exist.\n        Input-Output Arguments\n        :param self: Context. An instance of the Context class.\n        :param storage_dir: String. The path to the storage directory.\n        :return: No return values.\n        \"\"\""}
{"namespace": "diffprivlib.tools.quantiles.percentile", "prompt": "# Please complete the percentile function based on the contexts above the function.\n\n# The contexts above the function are:\n# MIT License\n#\n# Copyright (C) IBM Corporation 2020\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the\n# rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit\n# persons to whom the Software is furnished to do so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the\n# Software.\n#\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE\n# WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,\n# TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\"\"\"\nQuantile functions with differential privacy\n\"\"\"\nimport warnings\n\nimport numpy as np\n\n\nfrom diffprivlib.mechanisms import Exponential\nfrom diffprivlib.utils import warn_unused_args\n\n\n\n\ndef quantile(array, quant, epsilon=1.0, bounds=None, axis=None, keepdims=False, random_state=None, accountant=None,\n             **unused_args):\n    r\"\"\"\n    Compute the differentially private quantile of the array.\n\n    Returns the specified quantile with differential privacy.  The quantile is calculated over the flattened array.\n    Differential privacy is achieved with the :class:`.Exponential` mechanism, using the method first proposed by\n    Smith, 2011.\n\n    Paper link: https://dl.acm.org/doi/pdf/10.1145/1993636.1993743\n\n    Parameters\n    ----------\n    array : array_like\n        Array containing numbers whose quantile is sought.  If `array` is not an array, a conversion is attempted.\n\n    quant : float or array-like\n        Quantile or array of quantiles.  Each quantile must be in the unit interval [0, 1].  If quant is array-like,\n        quantiles are returned over the flattened array.\n\n    epsilon : float, default: 1.0\n        Privacy parameter :math:`\\epsilon`.  Differential privacy is achieved over the entire output, with epsilon split\n        evenly between each output value.\n\n    bounds : tuple, optional\n        Bounds of the values of the array, of the form (min, max).\n\n    axis : None or int or tuple of ints, optional\n        Axis or axes along which a sum is performed.  The default, axis=None, will sum all of the elements of the input\n        array.  If axis is negative it counts from the last to the first axis.\n\n        If axis is a tuple of ints, a sum is performed on all of the axes specified in the tuple instead of a single\n        axis or all the axes as before.\n\n    keepdims : bool, default: False\n        If this is set to True, the axes which are reduced are left in the result as dimensions with size one.  With\n        this option, the result will broadcast correctly against the input array.\n\n        If the default value is passed, then `keepdims` will not be passed through to the `mean` method of sub-classes\n        of `ndarray`, however any non-default value will be.  If the sub-class' method does not implement `keepdims` any\n        exceptions will be raised.\n\n    random_state : int or RandomState, optional\n        Controls the randomness of the algorithm.  To obtain a deterministic behaviour during randomisation,\n        ``random_state`` has to be fixed to an integer.\n\n    accountant : BudgetAccountant, optional\n        Accountant to keep track of privacy budget.\n\n    Returns\n    -------\n    m : ndarray\n        Returns a new array containing the quantile values.\n\n    See Also\n    --------\n    numpy.quantile : Equivalent non-private method.\n\n    percentile, median\n\n    \"\"\"\n    from diffprivlib.utils import check_random_state\n    from diffprivlib.utils import PrivacyLeakWarning\n    from diffprivlib.accountant import BudgetAccountant\n    from diffprivlib.validation import clip_to_bounds\n    from diffprivlib.tools.utils import _wrap_axis\n    from diffprivlib.validation import check_bounds\n    warn_unused_args(unused_args)\n\n    random_state = check_random_state(random_state)\n\n    if bounds is None:\n        warnings.warn(\"Bounds have not been specified and will be calculated on the data provided. This will \"\n                      \"result in additional privacy leakage. To ensure differential privacy and no additional \"\n                      \"privacy leakage, specify bounds for each dimension.\", PrivacyLeakWarning)\n        bounds = (np.min(array), np.max(array))\n\n    quant = np.ravel(quant)\n\n    if np.any(quant < 0) or np.any(quant > 1):\n        raise ValueError(\"Quantiles must be in the unit interval [0, 1].\")\n\n    if len(quant) > 1:\n        return np.array([quantile(array, q_i, epsilon=epsilon / len(quant), bounds=bounds, axis=axis, keepdims=keepdims,\n                                  accountant=accountant, random_state=random_state) for q_i in quant])\n\n    # Dealing with a single quant from now on\n    quant = quant.item()\n\n    if axis is not None or keepdims:\n        return _wrap_axis(quantile, array, quant=quant, epsilon=epsilon, bounds=bounds, axis=axis, keepdims=keepdims,\n                          random_state=random_state, accountant=accountant)\n\n    # Dealing with a scalar output from now on\n    bounds = check_bounds(bounds, shape=0, min_separation=1e-5)\n\n    accountant = BudgetAccountant.load_default(accountant)\n    accountant.check(epsilon, 0)\n\n    # Let's ravel array to be single-dimensional\n    array = clip_to_bounds(np.ravel(array), bounds)\n\n    k = array.size\n    array = np.append(array, list(bounds))\n    array.sort()\n\n    interval_sizes = np.diff(array)\n\n    # Todo: Need to find a way to do this in a differentially private way, see GH 80\n    if np.isnan(interval_sizes).any():\n        return np.nan\n\n    mech = Exponential(epsilon=epsilon, sensitivity=1, utility=list(-np.abs(np.arange(0, k + 1) - quant * k)),\n                       measure=list(interval_sizes), random_state=random_state)\n    idx = mech.randomise()\n    output = random_state.random() * (array[idx+1] - array[idx]) + array[idx]\n\n    accountant.spend(epsilon, 0)\n\n    return output\n\n\n\n\n# The code to be completed is:\ndef percentile(array, percent, epsilon=1.0, bounds=None, axis=None, keepdims=False, random_state=None, accountant=None,\n               **unused_args):\n\n    \"\"\"\n    This function computes the differentially private percentile of an array. It calls the quantile with the percentile value calculated as percent / 100 and validate the value, ensuring that the calculated percentile values fall within the acceptable range.\n    Input-Output Arguments\n    :param array: array_like. An array containing numbers whose percentile is sought.\n    :param percent: float or array-like. The percentile or list of percentiles sought. Each percentile must be in the range [0, 100]. If percent is array-like, percentiles are returned over the flattened array.\n    :param epsilon: float, default: 1.0. The privacy parameter epsilon. Differential privacy is achieved over the entire output, with epsilon split evenly between each output value.\n    :param bounds: tuple, optional. The bounds of the values of the array, in the form (min, max).\n    :param axis: None or int or tuple of ints, optional. The axis or axes along which the sum is performed. The default, axis=None, sums all the elements of the input array. If axis is negative, it counts from the last to the first axis. If axis is a tuple of ints, a sum is performed on all the specified axes.\n    :param keepdims: bool, default: False. If True, the axes which are reduced are left in the result as dimensions with size one. With this option, the result will broadcast correctly against the input array.\n    :param random_state: int or RandomState, optional. Controls the randomness of the algorithm. To obtain deterministic behavior during randomization, random_state has to be fixed to an integer.\n    :param accountant: BudgetAccountant, optional. An accountant to keep track of the privacy budget.\n    :param **unused_args: Should warn the user if any other parameters are passed.\n    :return: ndarray. Returns a new array containing the percentile values.\n    \"\"\""}
{"namespace": "chatette.parsing.ChoiceBuilder.create_concrete", "prompt": "# Please complete the create_concrete function based on the contexts above the function.\n\n# The contexts above the function are:\n# coding: utf-8\n\"\"\"\nModule `chatette.parsing`\nContains everything that is related to the management and parsing\nof the template file(s).\nThe most important classes defined in this module are:\n- Parser, which runs the whole parsing of template files.\n- Lexer, in charge of \"lexing\" the information present in those files.\n- All the lexing rules the lexer will use.\n- InputFileManager, which manages the opening, closing and read of those files.\n- ItemBuilders that are used by the parser to create concrete items.\n\"\"\"\n# TODO Add LineCountFileWrapper in here\n\nfrom abc import ABCMeta, abstractmethod\nfrom future.utils import with_metaclass\n\n\n\n\n\n\n\nfrom chatette.modifiers.representation import \\\n    ModifiersRepresentation, RandgenRepresentation\n\nfrom chatette.units.ast import AST\nfrom chatette.utils import UnitType\n\n\nclass ItemBuilder(with_metaclass(ABCMeta, object)):\n    \"\"\"\n    An intermediate representation of generating items that are used by the\n    parser. It is able to construct the corresponding item once it has\n    all the required information.\n    NOTE: This does not correspond to the *Builder* design pattern.\n    \"\"\"\n    def __init__(self):\n        self.leading_space = False\n        self.casegen = False\n        self.randgen = False\n        self.randgen_name = None\n        self.randgen_opposite = False\n        self.randgen_percent = 50\n\n    def _check_information(self):\n        if not self.randgen and self.randgen_name is not None:  # Should never happen\n            raise ValueError(\n                \"There was a problem with some modifiers: detected \" + \\\n                \"a random generation modifier name but no \" + \\\n                \"random generation modifier.\"\n            )\n\n    def _build_modifiers_repr(self):\n        \"\"\"\n        Returns an instance of `ModifiersRepresentation` that corresponds\n        to the modifiers set in `self`.\n        \"\"\"\n        modifiers = ModifiersRepresentation()\n        modifiers.casegen = self.casegen\n\n        randgen = RandgenRepresentation()\n        randgen._present = self.randgen\n        randgen.name = self.randgen_name\n        randgen.opposite = self.randgen_opposite\n        randgen.percentage = self.randgen_percent\n        modifiers.randgen = randgen\n\n        return modifiers\n\n    @abstractmethod\n    def create_concrete(self):\n        raise NotImplementedError()\n\nclass ChoiceBuilder(ItemBuilder):\n    def __init__(self):\n        super(ChoiceBuilder, self).__init__()\n        self.rules = []\n\n\n\n# The code to be completed is:\n    def create_concrete(self):\n\n        \"\"\"\n        The function creates a concrete Choice instance based on the current state of the ChoiceBuilder object. It first checks if all the necessary information is provided, and then constructs a Choice object using the leading space, modifiers representation, and rules of the ChoiceBuilder object.\n        Input-Output Arguments\n        :param self: ChoiceBuilder. An instance of the ChoiceBuilder class.\n        :return: Choice. The created Choice instance.\n        \"\"\""}
